{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load LiqourSales data and do sales prediction.\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.models import load_model, Sequential \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('LiquorSales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>date</th>\n",
       "      <th>month</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hour  date  month  quantity\n",
       "0     0     1     12         0\n",
       "1     1     1     12         0\n",
       "2     2     1     12         0\n",
       "3     3     1     12         0\n",
       "4     4     1     12         0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max and min values help in feature normalization.\n",
    "max_quantity = df['quantity'].max()\n",
    "min_quantity = df['quantity'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform One-hot encoding on categorical columns and normalize continuous column.\n",
    "df = pd.get_dummies(data=df, columns = ['hour', 'date','month'])\n",
    "df['quantity'] = (df['quantity']-min_quantity)/(max_quantity-min_quantity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quantity</th>\n",
       "      <th>hour_0</th>\n",
       "      <th>hour_1</th>\n",
       "      <th>hour_2</th>\n",
       "      <th>hour_3</th>\n",
       "      <th>hour_4</th>\n",
       "      <th>hour_5</th>\n",
       "      <th>hour_6</th>\n",
       "      <th>hour_7</th>\n",
       "      <th>hour_8</th>\n",
       "      <th>...</th>\n",
       "      <th>month_3</th>\n",
       "      <th>month_4</th>\n",
       "      <th>month_5</th>\n",
       "      <th>month_6</th>\n",
       "      <th>month_7</th>\n",
       "      <th>month_8</th>\n",
       "      <th>month_9</th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   quantity  hour_0  hour_1  hour_2  hour_3  hour_4  hour_5  hour_6  hour_7  \\\n",
       "0       0.0       1       0       0       0       0       0       0       0   \n",
       "1       0.0       0       1       0       0       0       0       0       0   \n",
       "2       0.0       0       0       1       0       0       0       0       0   \n",
       "3       0.0       0       0       0       1       0       0       0       0   \n",
       "4       0.0       0       0       0       0       1       0       0       0   \n",
       "\n",
       "   hour_8  ...  month_3  month_4  month_5  month_6  month_7  month_8  month_9  \\\n",
       "0       0  ...        0        0        0        0        0        0        0   \n",
       "1       0  ...        0        0        0        0        0        0        0   \n",
       "2       0  ...        0        0        0        0        0        0        0   \n",
       "3       0  ...        0        0        0        0        0        0        0   \n",
       "4       0  ...        0        0        0        0        0        0        0   \n",
       "\n",
       "   month_10  month_11  month_12  \n",
       "0         0         0         1  \n",
       "1         0         0         1  \n",
       "2         0         0         1  \n",
       "3         0         0         1  \n",
       "4         0         0         1  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quan = pd.DataFrame(columns=['quantity'])\n",
    "df_quan.quantity = df.quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quantity\n",
       "0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0.0\n",
       "4       0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_quan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = df.to_numpy()\n",
    "df_y = df_quan.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate into train validation and testing data.\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2, shuffle= False)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34213, 68) (11404, 68) (11404, 68) (34213, 1) (11404, 1) (11404, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train[:-1]\n",
    "x_val = x_val[:-1]\n",
    "x_test = x_test[:-1]\n",
    "\n",
    "y_train = y_train[1:]\n",
    "y_val = y_val[1:]\n",
    "y_test = y_test[1:]\n",
    "\n",
    "print(x_train.shape,\n",
    "     x_val.shape,\n",
    "     x_test.shape,\n",
    "     y_train.shape,\n",
    "     y_val.shape,\n",
    "     y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = x_train.reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
    "test_x = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
    "val_x = x_val.reshape((x_val.shape[0], 1, x_val.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34213, 1, 68) (11404, 1, 68) (11404, 1, 68) (34213, 1) (11404, 1) (11404, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape,\n",
    "     val_x.shape,\n",
    "     test_x.shape,\n",
    "     y_train.shape,\n",
    "     y_val.shape,\n",
    "     y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "Train on 34213 samples, validate on 11404 samples\n",
      "Epoch 1/1500\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f6a0027c488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f6a0027c488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00269, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 3s - loss: 0.0031 - val_loss: 0.0027\n",
      "Epoch 2/1500\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00269 to 0.00253, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 3/1500\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00253 to 0.00243, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 4/1500\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00243 to 0.00238, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 5/1500\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00238 to 0.00233, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 6/1500\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00233 to 0.00230, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 7/1500\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00230 to 0.00224, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 8/1500\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00224 to 0.00219, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 9/1500\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00219 to 0.00215, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 0.0020 - val_loss: 0.0022\n",
      "Epoch 10/1500\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00215 to 0.00213, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 11/1500\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00213 to 0.00209, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 12/1500\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00209 to 0.00204, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 13/1500\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00204 to 0.00202, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 14/1500\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00202 to 0.00201, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 15/1500\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00201 to 0.00195, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 16/1500\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00195 to 0.00192, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 17/1500\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00192 to 0.00185, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 18/1500\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00185 to 0.00182, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 19/1500\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00182 to 0.00177, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 20/1500\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00177\n",
      "34213/34213 - 0s - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 21/1500\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00177 to 0.00172, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 22/1500\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00172 to 0.00166, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 23/1500\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00166 to 0.00162, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 24/1500\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00162 to 0.00155, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 25/1500\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00155 to 0.00150, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 26/1500\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00150 to 0.00145, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 27/1500\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00145 to 0.00142, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 28/1500\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00142 to 0.00135, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 29/1500\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00135 to 0.00130, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 30/1500\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00130 to 0.00123, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 31/1500\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00123 to 0.00119, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 32/1500\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00119 to 0.00113, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 33/1500\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00113 to 0.00108, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 34/1500\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00108 to 0.00103, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 35/1500\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00103 to 0.00102, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 9.9419e-04 - val_loss: 0.0010\n",
      "Epoch 36/1500\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00102 to 0.00095, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 9.5531e-04 - val_loss: 9.4715e-04\n",
      "Epoch 37/1500\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00095 to 0.00091, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 9.1153e-04 - val_loss: 9.0801e-04\n",
      "Epoch 38/1500\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00091 to 0.00086, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 8.7731e-04 - val_loss: 8.5587e-04\n",
      "Epoch 39/1500\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00086 to 0.00082, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 8.3980e-04 - val_loss: 8.2364e-04\n",
      "Epoch 40/1500\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00082 to 0.00080, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 8.0658e-04 - val_loss: 8.0096e-04\n",
      "Epoch 41/1500\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00080 to 0.00077, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 7.8491e-04 - val_loss: 7.6934e-04\n",
      "Epoch 42/1500\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00077 to 0.00074, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 7.6271e-04 - val_loss: 7.4160e-04\n",
      "Epoch 43/1500\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00074 to 0.00070, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 7.2547e-04 - val_loss: 7.0134e-04\n",
      "Epoch 44/1500\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00070 to 0.00067, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 6.9662e-04 - val_loss: 6.6871e-04\n",
      "Epoch 45/1500\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00067 to 0.00065, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 6.7448e-04 - val_loss: 6.5225e-04\n",
      "Epoch 46/1500\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00065 to 0.00062, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 6.5284e-04 - val_loss: 6.2151e-04\n",
      "Epoch 47/1500\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00062 to 0.00060, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 6.3151e-04 - val_loss: 5.9755e-04\n",
      "Epoch 48/1500\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00060 to 0.00058, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 6.0742e-04 - val_loss: 5.7926e-04\n",
      "Epoch 49/1500\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00058 to 0.00057, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 5.9499e-04 - val_loss: 5.6625e-04\n",
      "Epoch 50/1500\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00057 to 0.00054, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 5.7649e-04 - val_loss: 5.4349e-04\n",
      "Epoch 51/1500\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00054 to 0.00053, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 5.5476e-04 - val_loss: 5.2588e-04\n",
      "Epoch 52/1500\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00053\n",
      "34213/34213 - 0s - loss: 5.3891e-04 - val_loss: 5.3096e-04\n",
      "Epoch 53/1500\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00053 to 0.00050, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 5.2003e-04 - val_loss: 5.0166e-04\n",
      "Epoch 54/1500\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00050 to 0.00049, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 5.0684e-04 - val_loss: 4.9480e-04\n",
      "Epoch 55/1500\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00049 to 0.00047, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 4.8793e-04 - val_loss: 4.6817e-04\n",
      "Epoch 56/1500\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00047 to 0.00045, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 4.7568e-04 - val_loss: 4.4838e-04\n",
      "Epoch 57/1500\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00045 to 0.00044, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 4.6360e-04 - val_loss: 4.3791e-04\n",
      "Epoch 58/1500\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00044 to 0.00044, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 4.4938e-04 - val_loss: 4.3539e-04\n",
      "Epoch 59/1500\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00044 to 0.00041, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 4.3536e-04 - val_loss: 4.0685e-04\n",
      "Epoch 60/1500\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00041\n",
      "34213/34213 - 0s - loss: 4.2074e-04 - val_loss: 4.0857e-04\n",
      "Epoch 61/1500\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00041 to 0.00039, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 4.1092e-04 - val_loss: 3.9024e-04\n",
      "Epoch 62/1500\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00039\n",
      "34213/34213 - 0s - loss: 4.0715e-04 - val_loss: 3.9542e-04\n",
      "Epoch 63/1500\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00039 to 0.00038, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 3.9746e-04 - val_loss: 3.8417e-04\n",
      "Epoch 64/1500\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00038 to 0.00036, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 3.8522e-04 - val_loss: 3.5846e-04\n",
      "Epoch 65/1500\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00036 to 0.00035, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 3.7773e-04 - val_loss: 3.5233e-04\n",
      "Epoch 66/1500\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00035 to 0.00034, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 3.6142e-04 - val_loss: 3.3822e-04\n",
      "Epoch 67/1500\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00034 to 0.00033, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 3.5780e-04 - val_loss: 3.3076e-04\n",
      "Epoch 68/1500\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00033\n",
      "34213/34213 - 0s - loss: 3.4619e-04 - val_loss: 3.4925e-04\n",
      "Epoch 69/1500\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00033 to 0.00031, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 3.4047e-04 - val_loss: 3.1031e-04\n",
      "Epoch 70/1500\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00031\n",
      "34213/34213 - 0s - loss: 3.2739e-04 - val_loss: 3.1930e-04\n",
      "Epoch 71/1500\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00031 to 0.00030, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 3.2130e-04 - val_loss: 3.0106e-04\n",
      "Epoch 72/1500\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00030 to 0.00030, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 3.1641e-04 - val_loss: 2.9693e-04\n",
      "Epoch 73/1500\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00030 to 0.00029, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 3.1179e-04 - val_loss: 2.8877e-04\n",
      "Epoch 74/1500\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00029 to 0.00027, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 3.0211e-04 - val_loss: 2.7301e-04\n",
      "Epoch 75/1500\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00027 to 0.00027, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.9016e-04 - val_loss: 2.7043e-04\n",
      "Epoch 76/1500\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00027 to 0.00027, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.8546e-04 - val_loss: 2.6580e-04\n",
      "Epoch 77/1500\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00027 to 0.00026, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 2.8106e-04 - val_loss: 2.6391e-04\n",
      "Epoch 78/1500\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00026 to 0.00025, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.7120e-04 - val_loss: 2.5083e-04\n",
      "Epoch 79/1500\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00025\n",
      "34213/34213 - 1s - loss: 2.6805e-04 - val_loss: 2.6405e-04\n",
      "Epoch 80/1500\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00025 to 0.00025, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.6278e-04 - val_loss: 2.5056e-04\n",
      "Epoch 81/1500\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00025 to 0.00024, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.5485e-04 - val_loss: 2.3538e-04\n",
      "Epoch 82/1500\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00024\n",
      "34213/34213 - 0s - loss: 2.5111e-04 - val_loss: 2.3943e-04\n",
      "Epoch 83/1500\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00024 to 0.00023, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.4680e-04 - val_loss: 2.2633e-04\n",
      "Epoch 84/1500\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00023\n",
      "34213/34213 - 0s - loss: 2.3651e-04 - val_loss: 2.2928e-04\n",
      "Epoch 85/1500\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00023 to 0.00022, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.3213e-04 - val_loss: 2.2077e-04\n",
      "Epoch 86/1500\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00022 to 0.00022, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 2.3316e-04 - val_loss: 2.1733e-04\n",
      "Epoch 87/1500\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00022 to 0.00022, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.3038e-04 - val_loss: 2.1630e-04\n",
      "Epoch 88/1500\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00022 to 0.00021, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.2256e-04 - val_loss: 2.0612e-04\n",
      "Epoch 89/1500\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00021\n",
      "34213/34213 - 0s - loss: 2.1458e-04 - val_loss: 2.1139e-04\n",
      "Epoch 90/1500\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00021 to 0.00020, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.1292e-04 - val_loss: 1.9502e-04\n",
      "Epoch 91/1500\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00020 to 0.00019, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 2.0732e-04 - val_loss: 1.8725e-04\n",
      "Epoch 92/1500\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00019\n",
      "34213/34213 - 0s - loss: 2.0664e-04 - val_loss: 1.9315e-04\n",
      "Epoch 93/1500\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00019\n",
      "34213/34213 - 0s - loss: 2.0325e-04 - val_loss: 1.8912e-04\n",
      "Epoch 94/1500\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00019\n",
      "34213/34213 - 0s - loss: 1.9953e-04 - val_loss: 1.9522e-04\n",
      "Epoch 95/1500\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00019 to 0.00018, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.9856e-04 - val_loss: 1.7731e-04\n",
      "Epoch 96/1500\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00018\n",
      "34213/34213 - 0s - loss: 1.9554e-04 - val_loss: 1.8190e-04\n",
      "Epoch 97/1500\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00018 to 0.00018, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.9378e-04 - val_loss: 1.7729e-04\n",
      "Epoch 98/1500\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00018\n",
      "34213/34213 - 0s - loss: 1.8637e-04 - val_loss: 1.7755e-04\n",
      "Epoch 99/1500\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00018 to 0.00017, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 1.8532e-04 - val_loss: 1.6812e-04\n",
      "Epoch 100/1500\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00017 to 0.00017, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 1.8372e-04 - val_loss: 1.6535e-04\n",
      "Epoch 101/1500\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00017 to 0.00016, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.7737e-04 - val_loss: 1.5912e-04\n",
      "Epoch 102/1500\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00016\n",
      "34213/34213 - 0s - loss: 1.7662e-04 - val_loss: 1.6477e-04\n",
      "Epoch 103/1500\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00016 to 0.00016, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.7530e-04 - val_loss: 1.5720e-04\n",
      "Epoch 104/1500\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00016\n",
      "34213/34213 - 0s - loss: 1.6954e-04 - val_loss: 1.6135e-04\n",
      "Epoch 105/1500\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00016\n",
      "34213/34213 - 0s - loss: 1.6843e-04 - val_loss: 1.6236e-04\n",
      "Epoch 106/1500\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.00016 to 0.00015, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.6936e-04 - val_loss: 1.5208e-04\n",
      "Epoch 107/1500\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00015 to 0.00015, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 1.6601e-04 - val_loss: 1.5065e-04\n",
      "Epoch 108/1500\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00015 to 0.00015, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.6281e-04 - val_loss: 1.4852e-04\n",
      "Epoch 109/1500\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00015\n",
      "34213/34213 - 0s - loss: 1.6028e-04 - val_loss: 1.5327e-04\n",
      "Epoch 110/1500\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00015 to 0.00014, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 1.5549e-04 - val_loss: 1.4104e-04\n",
      "Epoch 111/1500\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00014\n",
      "34213/34213 - 0s - loss: 1.5243e-04 - val_loss: 1.4262e-04\n",
      "Epoch 112/1500\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00014 to 0.00014, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.5476e-04 - val_loss: 1.4025e-04\n",
      "Epoch 113/1500\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00014 to 0.00014, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 1.4850e-04 - val_loss: 1.3963e-04\n",
      "Epoch 114/1500\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00014 to 0.00014, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.4789e-04 - val_loss: 1.3659e-04\n",
      "Epoch 115/1500\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00014 to 0.00014, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.4633e-04 - val_loss: 1.3601e-04\n",
      "Epoch 116/1500\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00014 to 0.00013, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 1.4369e-04 - val_loss: 1.2877e-04\n",
      "Epoch 117/1500\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00013\n",
      "34213/34213 - 0s - loss: 1.4244e-04 - val_loss: 1.3409e-04\n",
      "Epoch 118/1500\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00013\n",
      "34213/34213 - 0s - loss: 1.4126e-04 - val_loss: 1.2980e-04\n",
      "Epoch 119/1500\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00013\n",
      "34213/34213 - 0s - loss: 1.3933e-04 - val_loss: 1.3018e-04\n",
      "Epoch 120/1500\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00013 to 0.00012, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.3673e-04 - val_loss: 1.2489e-04\n",
      "Epoch 121/1500\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00012 to 0.00012, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 1.3797e-04 - val_loss: 1.2329e-04\n",
      "Epoch 122/1500\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00012\n",
      "34213/34213 - 0s - loss: 1.3644e-04 - val_loss: 1.2357e-04\n",
      "Epoch 123/1500\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00012\n",
      "34213/34213 - 0s - loss: 1.3218e-04 - val_loss: 1.2520e-04\n",
      "Epoch 124/1500\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00012 to 0.00012, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.3221e-04 - val_loss: 1.2181e-04\n",
      "Epoch 125/1500\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00012\n",
      "34213/34213 - 0s - loss: 1.3057e-04 - val_loss: 1.2192e-04\n",
      "Epoch 126/1500\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00012 to 0.00012, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 1.2756e-04 - val_loss: 1.1757e-04\n",
      "Epoch 127/1500\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00012 to 0.00011, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 1.2511e-04 - val_loss: 1.1285e-04\n",
      "Epoch 128/1500\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00011\n",
      "34213/34213 - 0s - loss: 1.2477e-04 - val_loss: 1.1968e-04\n",
      "Epoch 129/1500\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00011\n",
      "34213/34213 - 0s - loss: 1.2592e-04 - val_loss: 1.1893e-04\n",
      "Epoch 130/1500\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.00011\n",
      "34213/34213 - 0s - loss: 1.2431e-04 - val_loss: 1.2658e-04\n",
      "Epoch 131/1500\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00011\n",
      "34213/34213 - 0s - loss: 1.2200e-04 - val_loss: 1.2073e-04\n",
      "Epoch 132/1500\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00011 to 0.00011, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.2202e-04 - val_loss: 1.0871e-04\n",
      "Epoch 133/1500\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00011\n",
      "34213/34213 - 0s - loss: 1.1980e-04 - val_loss: 1.1084e-04\n",
      "Epoch 134/1500\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00011 to 0.00011, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.1503e-04 - val_loss: 1.0788e-04\n",
      "Epoch 135/1500\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00011 to 0.00010, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 1.1588e-04 - val_loss: 1.0482e-04\n",
      "Epoch 136/1500\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00010\n",
      "34213/34213 - 0s - loss: 1.1759e-04 - val_loss: 1.0690e-04\n",
      "Epoch 137/1500\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00010\n",
      "34213/34213 - 0s - loss: 1.1585e-04 - val_loss: 1.1118e-04\n",
      "Epoch 138/1500\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00010\n",
      "34213/34213 - 0s - loss: 1.1344e-04 - val_loss: 1.1129e-04\n",
      "Epoch 139/1500\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00010 to 0.00010, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.1092e-04 - val_loss: 1.0357e-04\n",
      "Epoch 140/1500\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00010\n",
      "34213/34213 - 0s - loss: 1.1231e-04 - val_loss: 1.0432e-04\n",
      "Epoch 141/1500\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00010 to 0.00010, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 1.0963e-04 - val_loss: 1.0096e-04\n",
      "Epoch 142/1500\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00010 to 0.00010, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.0896e-04 - val_loss: 9.5693e-05\n",
      "Epoch 143/1500\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00010 to 0.00009, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.0647e-04 - val_loss: 9.4721e-05\n",
      "Epoch 144/1500\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00009\n",
      "34213/34213 - 0s - loss: 1.0522e-04 - val_loss: 1.0066e-04\n",
      "Epoch 145/1500\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00009\n",
      "34213/34213 - 0s - loss: 1.0556e-04 - val_loss: 9.5045e-05\n",
      "Epoch 146/1500\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00009\n",
      "34213/34213 - 0s - loss: 1.0364e-04 - val_loss: 9.6517e-05\n",
      "Epoch 147/1500\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00009 to 0.00009, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.0251e-04 - val_loss: 9.4490e-05\n",
      "Epoch 148/1500\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00009 to 0.00009, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.0223e-04 - val_loss: 9.2640e-05\n",
      "Epoch 149/1500\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.00009 to 0.00009, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 1.0051e-04 - val_loss: 9.1930e-05\n",
      "Epoch 150/1500\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00009\n",
      "34213/34213 - 0s - loss: 9.9430e-05 - val_loss: 9.5079e-05\n",
      "Epoch 151/1500\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00009 to 0.00009, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.0038e-04 - val_loss: 9.0970e-05\n",
      "Epoch 152/1500\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.00009\n",
      "34213/34213 - 0s - loss: 9.6984e-05 - val_loss: 9.1722e-05\n",
      "Epoch 153/1500\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.00009\n",
      "34213/34213 - 0s - loss: 9.6272e-05 - val_loss: 9.8004e-05\n",
      "Epoch 154/1500\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.00009\n",
      "34213/34213 - 0s - loss: 9.7180e-05 - val_loss: 9.2284e-05\n",
      "Epoch 155/1500\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.00009 to 0.00009, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 9.7339e-05 - val_loss: 8.9558e-05\n",
      "Epoch 156/1500\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.00009\n",
      "34213/34213 - 0s - loss: 9.6990e-05 - val_loss: 9.0979e-05\n",
      "Epoch 157/1500\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.00009 to 0.00008, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 9.5250e-05 - val_loss: 8.4104e-05\n",
      "Epoch 158/1500\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.00008\n",
      "34213/34213 - 0s - loss: 9.4352e-05 - val_loss: 8.7824e-05\n",
      "Epoch 159/1500\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.00008 to 0.00008, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 9.1500e-05 - val_loss: 8.2766e-05\n",
      "Epoch 160/1500\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.00008\n",
      "34213/34213 - 0s - loss: 9.1087e-05 - val_loss: 8.3636e-05\n",
      "Epoch 161/1500\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.00008\n",
      "34213/34213 - 0s - loss: 8.9228e-05 - val_loss: 9.1270e-05\n",
      "Epoch 162/1500\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.00008 to 0.00008, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 8.7595e-05 - val_loss: 7.6713e-05\n",
      "Epoch 163/1500\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.00008\n",
      "34213/34213 - 0s - loss: 8.6563e-05 - val_loss: 8.1184e-05\n",
      "Epoch 164/1500\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.00008\n",
      "34213/34213 - 0s - loss: 8.8167e-05 - val_loss: 7.9612e-05\n",
      "Epoch 165/1500\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.00008\n",
      "34213/34213 - 0s - loss: 8.7927e-05 - val_loss: 8.3245e-05\n",
      "Epoch 166/1500\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.00008\n",
      "34213/34213 - 0s - loss: 8.6832e-05 - val_loss: 8.1108e-05\n",
      "Epoch 167/1500\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.00008\n",
      "34213/34213 - 0s - loss: 8.4809e-05 - val_loss: 8.3084e-05\n",
      "Epoch 168/1500\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.00008\n",
      "34213/34213 - 0s - loss: 8.6493e-05 - val_loss: 7.7193e-05\n",
      "Epoch 169/1500\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.00008\n",
      "34213/34213 - 0s - loss: 8.3814e-05 - val_loss: 7.8030e-05\n",
      "Epoch 170/1500\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.00008 to 0.00007, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 8.3468e-05 - val_loss: 7.4617e-05\n",
      "Epoch 171/1500\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.00007\n",
      "34213/34213 - 0s - loss: 8.2258e-05 - val_loss: 7.5477e-05\n",
      "Epoch 172/1500\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.00007\n",
      "34213/34213 - 0s - loss: 8.2772e-05 - val_loss: 8.8324e-05\n",
      "Epoch 173/1500\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.00007\n",
      "34213/34213 - 0s - loss: 8.5486e-05 - val_loss: 8.0121e-05\n",
      "Epoch 174/1500\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.00007\n",
      "34213/34213 - 0s - loss: 8.0639e-05 - val_loss: 8.1884e-05\n",
      "Epoch 175/1500\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.00007 to 0.00007, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 8.0260e-05 - val_loss: 7.3125e-05\n",
      "Epoch 176/1500\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.00007 to 0.00007, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 7.8956e-05 - val_loss: 7.2889e-05\n",
      "Epoch 177/1500\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.00007 to 0.00007, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 7.6552e-05 - val_loss: 7.0534e-05\n",
      "Epoch 178/1500\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.00007\n",
      "34213/34213 - 0s - loss: 7.5673e-05 - val_loss: 7.2462e-05\n",
      "Epoch 179/1500\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.00007\n",
      "34213/34213 - 0s - loss: 7.8176e-05 - val_loss: 8.1149e-05\n",
      "Epoch 180/1500\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.00007\n",
      "34213/34213 - 0s - loss: 7.7421e-05 - val_loss: 8.0287e-05\n",
      "Epoch 181/1500\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.00007\n",
      "34213/34213 - 0s - loss: 7.8095e-05 - val_loss: 7.3437e-05\n",
      "Epoch 182/1500\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.00007\n",
      "34213/34213 - 0s - loss: 7.7522e-05 - val_loss: 7.1211e-05\n",
      "Epoch 183/1500\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.00007\n",
      "34213/34213 - 0s - loss: 7.4926e-05 - val_loss: 8.0817e-05\n",
      "Epoch 184/1500\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.00007\n",
      "34213/34213 - 0s - loss: 7.4979e-05 - val_loss: 7.3541e-05\n",
      "Epoch 185/1500\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.00007 to 0.00006, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 7.3470e-05 - val_loss: 6.4942e-05\n",
      "Epoch 186/1500\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 7.1712e-05 - val_loss: 6.6062e-05\n",
      "Epoch 187/1500\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 7.1014e-05 - val_loss: 6.6081e-05\n",
      "Epoch 188/1500\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 7.3319e-05 - val_loss: 6.9436e-05\n",
      "Epoch 189/1500\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 7.1076e-05 - val_loss: 7.1781e-05\n",
      "Epoch 190/1500\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 7.1025e-05 - val_loss: 7.3086e-05\n",
      "Epoch 191/1500\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 7.0792e-05 - val_loss: 6.8411e-05\n",
      "Epoch 192/1500\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 6.9988e-05 - val_loss: 7.3246e-05\n",
      "Epoch 193/1500\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 6.9636e-05 - val_loss: 6.6683e-05\n",
      "Epoch 194/1500\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 6.9795e-05 - val_loss: 6.8441e-05\n",
      "Epoch 195/1500\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.00006 to 0.00006, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 6.9688e-05 - val_loss: 6.3831e-05\n",
      "Epoch 196/1500\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 6.8004e-05 - val_loss: 6.7197e-05\n",
      "Epoch 197/1500\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 6.8762e-05 - val_loss: 8.8220e-05\n",
      "Epoch 198/1500\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.00006 to 0.00006, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 6.8130e-05 - val_loss: 6.3194e-05\n",
      "Epoch 199/1500\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.00006 to 0.00006, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 6.5641e-05 - val_loss: 6.0448e-05\n",
      "Epoch 200/1500\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 6.8607e-05 - val_loss: 6.7605e-05\n",
      "Epoch 201/1500\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 6.7547e-05 - val_loss: 6.4610e-05\n",
      "Epoch 202/1500\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 6.6287e-05 - val_loss: 6.3533e-05\n",
      "Epoch 203/1500\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.00006 to 0.00006, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 6.4016e-05 - val_loss: 5.9097e-05\n",
      "Epoch 204/1500\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 6.3594e-05 - val_loss: 5.9753e-05\n",
      "Epoch 205/1500\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 6.4248e-05 - val_loss: 6.3320e-05\n",
      "Epoch 206/1500\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 6.4222e-05 - val_loss: 5.9152e-05\n",
      "Epoch 207/1500\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 6.4009e-05 - val_loss: 6.4952e-05\n",
      "Epoch 208/1500\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 6.3068e-05 - val_loss: 6.2001e-05\n",
      "Epoch 209/1500\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.00006 to 0.00006, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 6.3708e-05 - val_loss: 5.8742e-05\n",
      "Epoch 210/1500\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.00006 to 0.00006, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 6.1713e-05 - val_loss: 5.7255e-05\n",
      "Epoch 211/1500\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 6.1749e-05 - val_loss: 5.8498e-05\n",
      "Epoch 212/1500\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 6.1101e-05 - val_loss: 5.8444e-05\n",
      "Epoch 213/1500\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 6.1397e-05 - val_loss: 5.7809e-05\n",
      "Epoch 214/1500\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.00006 to 0.00006, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 5.8973e-05 - val_loss: 5.5028e-05\n",
      "Epoch 215/1500\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.00006\n",
      "34213/34213 - 0s - loss: 6.1511e-05 - val_loss: 6.5183e-05\n",
      "Epoch 216/1500\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.00006 to 0.00005, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 6.0745e-05 - val_loss: 5.2738e-05\n",
      "Epoch 217/1500\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.9645e-05 - val_loss: 5.5624e-05\n",
      "Epoch 218/1500\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 6.0005e-05 - val_loss: 5.7988e-05\n",
      "Epoch 219/1500\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.9066e-05 - val_loss: 5.8426e-05\n",
      "Epoch 220/1500\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.8725e-05 - val_loss: 5.5154e-05\n",
      "Epoch 221/1500\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.00005 to 0.00005, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 5.8307e-05 - val_loss: 5.2039e-05\n",
      "Epoch 222/1500\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.8892e-05 - val_loss: 5.5878e-05\n",
      "Epoch 223/1500\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.00005 to 0.00005, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 5.5331e-05 - val_loss: 5.0743e-05\n",
      "Epoch 224/1500\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.5521e-05 - val_loss: 5.5202e-05\n",
      "Epoch 225/1500\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.8544e-05 - val_loss: 5.6689e-05\n",
      "Epoch 226/1500\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.5860e-05 - val_loss: 5.4175e-05\n",
      "Epoch 227/1500\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.4713e-05 - val_loss: 5.6207e-05\n",
      "Epoch 228/1500\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.00005 to 0.00005, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 5.3722e-05 - val_loss: 5.0348e-05\n",
      "Epoch 229/1500\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.00005 to 0.00005, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 5.5037e-05 - val_loss: 4.9710e-05\n",
      "Epoch 230/1500\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.4546e-05 - val_loss: 5.8211e-05\n",
      "Epoch 231/1500\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.7620e-05 - val_loss: 5.6585e-05\n",
      "Epoch 232/1500\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.7398e-05 - val_loss: 5.3042e-05\n",
      "Epoch 233/1500\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.5678e-05 - val_loss: 4.9948e-05\n",
      "Epoch 234/1500\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.00005 to 0.00005, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 5.3514e-05 - val_loss: 4.6939e-05\n",
      "Epoch 235/1500\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.0916e-05 - val_loss: 5.0910e-05\n",
      "Epoch 236/1500\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.3221e-05 - val_loss: 4.7187e-05\n",
      "Epoch 237/1500\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.3072e-05 - val_loss: 5.2423e-05\n",
      "Epoch 238/1500\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.00005 to 0.00005, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 5.0294e-05 - val_loss: 4.6192e-05\n",
      "Epoch 239/1500\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.1940e-05 - val_loss: 4.6956e-05\n",
      "Epoch 240/1500\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.3742e-05 - val_loss: 4.9075e-05\n",
      "Epoch 241/1500\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.1738e-05 - val_loss: 5.1114e-05\n",
      "Epoch 242/1500\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.2335e-05 - val_loss: 4.8704e-05\n",
      "Epoch 243/1500\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.1265e-05 - val_loss: 4.7073e-05\n",
      "Epoch 244/1500\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 4.9188e-05 - val_loss: 4.7643e-05\n",
      "Epoch 245/1500\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.0418e-05 - val_loss: 4.8441e-05\n",
      "Epoch 246/1500\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.1392e-05 - val_loss: 5.0774e-05\n",
      "Epoch 247/1500\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.00005 to 0.00005, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 5.0410e-05 - val_loss: 4.6185e-05\n",
      "Epoch 248/1500\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 4.8533e-05 - val_loss: 4.7337e-05\n",
      "Epoch 249/1500\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 4.7154e-05 - val_loss: 4.9159e-05\n",
      "Epoch 250/1500\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 5.0374e-05 - val_loss: 5.0556e-05\n",
      "Epoch 251/1500\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 4.7811e-05 - val_loss: 4.8057e-05\n",
      "Epoch 252/1500\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.00005 to 0.00005, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 4.8309e-05 - val_loss: 4.5223e-05\n",
      "Epoch 253/1500\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.00005\n",
      "34213/34213 - 0s - loss: 4.7657e-05 - val_loss: 4.5677e-05\n",
      "Epoch 254/1500\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.00005 to 0.00004, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 4.7022e-05 - val_loss: 4.2454e-05\n",
      "Epoch 255/1500\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.6251e-05 - val_loss: 4.6694e-05\n",
      "Epoch 256/1500\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.9946e-05 - val_loss: 4.8689e-05\n",
      "Epoch 257/1500\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 5.0842e-05 - val_loss: 4.7947e-05\n",
      "Epoch 258/1500\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.7409e-05 - val_loss: 4.7363e-05\n",
      "Epoch 259/1500\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.5449e-05 - val_loss: 4.3178e-05\n",
      "Epoch 260/1500\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.5277e-05 - val_loss: 5.2150e-05\n",
      "Epoch 261/1500\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.8504e-05 - val_loss: 4.2756e-05\n",
      "Epoch 262/1500\n",
      "\n",
      "Epoch 00262: val_loss improved from 0.00004 to 0.00004, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 4.4264e-05 - val_loss: 3.9481e-05\n",
      "Epoch 263/1500\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.3393e-05 - val_loss: 4.3662e-05\n",
      "Epoch 264/1500\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.4419e-05 - val_loss: 5.0461e-05\n",
      "Epoch 265/1500\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.5145e-05 - val_loss: 4.4193e-05\n",
      "Epoch 266/1500\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.4096e-05 - val_loss: 4.2678e-05\n",
      "Epoch 267/1500\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.5172e-05 - val_loss: 4.2365e-05\n",
      "Epoch 268/1500\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.3277e-05 - val_loss: 4.6232e-05\n",
      "Epoch 269/1500\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.5412e-05 - val_loss: 4.1027e-05\n",
      "Epoch 270/1500\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.3395e-05 - val_loss: 4.3659e-05\n",
      "Epoch 271/1500\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.3843e-05 - val_loss: 4.1167e-05\n",
      "Epoch 272/1500\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.3270e-05 - val_loss: 4.0283e-05\n",
      "Epoch 273/1500\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.1893e-05 - val_loss: 4.3937e-05\n",
      "Epoch 274/1500\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.3798e-05 - val_loss: 4.4418e-05\n",
      "Epoch 275/1500\n",
      "\n",
      "Epoch 00275: val_loss improved from 0.00004 to 0.00004, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 4.2823e-05 - val_loss: 3.8956e-05\n",
      "Epoch 276/1500\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.2487e-05 - val_loss: 4.2250e-05\n",
      "Epoch 277/1500\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.2996e-05 - val_loss: 4.0052e-05\n",
      "Epoch 278/1500\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.00004 to 0.00004, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 4.3190e-05 - val_loss: 3.8009e-05\n",
      "Epoch 279/1500\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 3.9457e-05 - val_loss: 3.9538e-05\n",
      "Epoch 280/1500\n",
      "\n",
      "Epoch 00280: val_loss improved from 0.00004 to 0.00004, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 4.2787e-05 - val_loss: 3.7523e-05\n",
      "Epoch 281/1500\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.1367e-05 - val_loss: 4.0391e-05\n",
      "Epoch 282/1500\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.1237e-05 - val_loss: 4.3987e-05\n",
      "Epoch 283/1500\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.2685e-05 - val_loss: 4.0610e-05\n",
      "Epoch 284/1500\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.1381e-05 - val_loss: 3.8507e-05\n",
      "Epoch 285/1500\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.2275e-05 - val_loss: 3.9049e-05\n",
      "Epoch 286/1500\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.1007e-05 - val_loss: 4.5091e-05\n",
      "Epoch 287/1500\n",
      "\n",
      "Epoch 00287: val_loss improved from 0.00004 to 0.00004, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 4.0530e-05 - val_loss: 3.7050e-05\n",
      "Epoch 288/1500\n",
      "\n",
      "Epoch 00288: val_loss improved from 0.00004 to 0.00004, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 3.9451e-05 - val_loss: 3.6145e-05\n",
      "Epoch 289/1500\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 3.8315e-05 - val_loss: 3.6985e-05\n",
      "Epoch 290/1500\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.3680e-05 - val_loss: 3.7739e-05\n",
      "Epoch 291/1500\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.0489e-05 - val_loss: 3.8537e-05\n",
      "Epoch 292/1500\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.4207e-05 - val_loss: 4.2842e-05\n",
      "Epoch 293/1500\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.0423e-05 - val_loss: 3.7153e-05\n",
      "Epoch 294/1500\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 3.7152e-05 - val_loss: 3.6759e-05\n",
      "Epoch 295/1500\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 4.0671e-05 - val_loss: 3.7225e-05\n",
      "Epoch 296/1500\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 3.7747e-05 - val_loss: 3.7791e-05\n",
      "Epoch 297/1500\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.00004\n",
      "34213/34213 - 0s - loss: 3.6341e-05 - val_loss: 3.6176e-05\n",
      "Epoch 298/1500\n",
      "\n",
      "Epoch 00298: val_loss improved from 0.00004 to 0.00003, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 3.8867e-05 - val_loss: 3.4638e-05\n",
      "Epoch 299/1500\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.6986e-05 - val_loss: 3.6633e-05\n",
      "Epoch 300/1500\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.8386e-05 - val_loss: 3.8113e-05\n",
      "Epoch 301/1500\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.9288e-05 - val_loss: 4.1040e-05\n",
      "Epoch 302/1500\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 4.0289e-05 - val_loss: 3.9397e-05\n",
      "Epoch 303/1500\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.8336e-05 - val_loss: 3.8590e-05\n",
      "Epoch 304/1500\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.8529e-05 - val_loss: 3.6761e-05\n",
      "Epoch 305/1500\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 4.0657e-05 - val_loss: 3.7824e-05\n",
      "Epoch 306/1500\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.8398e-05 - val_loss: 3.4864e-05\n",
      "Epoch 307/1500\n",
      "\n",
      "Epoch 00307: val_loss improved from 0.00003 to 0.00003, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 3.5504e-05 - val_loss: 3.1664e-05\n",
      "Epoch 308/1500\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.7669e-05 - val_loss: 3.6806e-05\n",
      "Epoch 309/1500\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.6008e-05 - val_loss: 3.9025e-05\n",
      "Epoch 310/1500\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.5520e-05 - val_loss: 3.4600e-05\n",
      "Epoch 311/1500\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.7636e-05 - val_loss: 3.7341e-05\n",
      "Epoch 312/1500\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.7952e-05 - val_loss: 3.6137e-05\n",
      "Epoch 313/1500\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.6608e-05 - val_loss: 3.2482e-05\n",
      "Epoch 314/1500\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.6220e-05 - val_loss: 3.6019e-05\n",
      "Epoch 315/1500\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.8931e-05 - val_loss: 3.4548e-05\n",
      "Epoch 316/1500\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.6191e-05 - val_loss: 3.7232e-05\n",
      "Epoch 317/1500\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.5844e-05 - val_loss: 3.2724e-05\n",
      "Epoch 318/1500\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.5317e-05 - val_loss: 3.4058e-05\n",
      "Epoch 319/1500\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.9228e-05 - val_loss: 3.5338e-05\n",
      "Epoch 320/1500\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.4882e-05 - val_loss: 3.5517e-05\n",
      "Epoch 321/1500\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.6606e-05 - val_loss: 3.4857e-05\n",
      "Epoch 322/1500\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.3641e-05 - val_loss: 3.3480e-05\n",
      "Epoch 323/1500\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.6712e-05 - val_loss: 3.3410e-05\n",
      "Epoch 324/1500\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.6467e-05 - val_loss: 3.5576e-05\n",
      "Epoch 325/1500\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.6137e-05 - val_loss: 3.3138e-05\n",
      "Epoch 326/1500\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.5805e-05 - val_loss: 3.3593e-05\n",
      "Epoch 327/1500\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.4183e-05 - val_loss: 3.1871e-05\n",
      "Epoch 328/1500\n",
      "\n",
      "Epoch 00328: val_loss improved from 0.00003 to 0.00003, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 3.4691e-05 - val_loss: 3.0383e-05\n",
      "Epoch 329/1500\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.4443e-05 - val_loss: 3.2653e-05\n",
      "Epoch 330/1500\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.6078e-05 - val_loss: 3.5332e-05\n",
      "Epoch 331/1500\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.5724e-05 - val_loss: 3.3990e-05\n",
      "Epoch 332/1500\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.6538e-05 - val_loss: 3.9930e-05\n",
      "Epoch 333/1500\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.7560e-05 - val_loss: 3.4704e-05\n",
      "Epoch 334/1500\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.3417e-05 - val_loss: 3.0400e-05\n",
      "Epoch 335/1500\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.3234e-05 - val_loss: 3.2983e-05\n",
      "Epoch 336/1500\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.5485e-05 - val_loss: 3.7477e-05\n",
      "Epoch 337/1500\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.4183e-05 - val_loss: 3.4831e-05\n",
      "Epoch 338/1500\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.4861e-05 - val_loss: 3.6851e-05\n",
      "Epoch 339/1500\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.3895e-05 - val_loss: 3.2522e-05\n",
      "Epoch 340/1500\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.4591e-05 - val_loss: 3.5923e-05\n",
      "Epoch 341/1500\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.4101e-05 - val_loss: 3.3558e-05\n",
      "Epoch 342/1500\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.2362e-05 - val_loss: 3.1913e-05\n",
      "Epoch 343/1500\n",
      "\n",
      "Epoch 00343: val_loss improved from 0.00003 to 0.00003, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 3.2524e-05 - val_loss: 2.9026e-05\n",
      "Epoch 344/1500\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.2750e-05 - val_loss: 3.1635e-05\n",
      "Epoch 345/1500\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.2918e-05 - val_loss: 3.3611e-05\n",
      "Epoch 346/1500\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.4740e-05 - val_loss: 3.6133e-05\n",
      "Epoch 347/1500\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.4949e-05 - val_loss: 3.3705e-05\n",
      "Epoch 348/1500\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.2336e-05 - val_loss: 2.9455e-05\n",
      "Epoch 349/1500\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.2140e-05 - val_loss: 3.4071e-05\n",
      "Epoch 350/1500\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.7148e-05 - val_loss: 3.3795e-05\n",
      "Epoch 351/1500\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.4178e-05 - val_loss: 2.9764e-05\n",
      "Epoch 352/1500\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.0770e-05 - val_loss: 2.9256e-05\n",
      "Epoch 353/1500\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.0473e-05 - val_loss: 3.1816e-05\n",
      "Epoch 354/1500\n",
      "\n",
      "Epoch 00354: val_loss improved from 0.00003 to 0.00003, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 3.0961e-05 - val_loss: 2.8791e-05\n",
      "Epoch 355/1500\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.2819e-05 - val_loss: 3.3870e-05\n",
      "Epoch 356/1500\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.5012e-05 - val_loss: 3.1801e-05\n",
      "Epoch 357/1500\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.4508e-05 - val_loss: 3.6352e-05\n",
      "Epoch 358/1500\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.4881e-05 - val_loss: 3.4047e-05\n",
      "Epoch 359/1500\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.3894e-05 - val_loss: 3.4836e-05\n",
      "Epoch 360/1500\n",
      "\n",
      "Epoch 00360: val_loss improved from 0.00003 to 0.00003, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 3.2834e-05 - val_loss: 2.8433e-05\n",
      "Epoch 361/1500\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.1493e-05 - val_loss: 2.9169e-05\n",
      "Epoch 362/1500\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.2111e-05 - val_loss: 2.9505e-05\n",
      "Epoch 363/1500\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.0727e-05 - val_loss: 3.4049e-05\n",
      "Epoch 364/1500\n",
      "\n",
      "Epoch 00364: val_loss improved from 0.00003 to 0.00003, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 3.1854e-05 - val_loss: 2.7521e-05\n",
      "Epoch 365/1500\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.1643e-05 - val_loss: 2.8683e-05\n",
      "Epoch 366/1500\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.1222e-05 - val_loss: 3.0794e-05\n",
      "Epoch 367/1500\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.1028e-05 - val_loss: 2.7746e-05\n",
      "Epoch 368/1500\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.0821e-05 - val_loss: 3.2702e-05\n",
      "Epoch 369/1500\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.2163e-05 - val_loss: 3.2474e-05\n",
      "Epoch 370/1500\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.1780e-05 - val_loss: 3.2806e-05\n",
      "Epoch 371/1500\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.1544e-05 - val_loss: 2.8076e-05\n",
      "Epoch 372/1500\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.2863e-05 - val_loss: 2.8798e-05\n",
      "Epoch 373/1500\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.0352e-05 - val_loss: 2.9104e-05\n",
      "Epoch 374/1500\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.0934e-05 - val_loss: 2.9352e-05\n",
      "Epoch 375/1500\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.0176e-05 - val_loss: 3.0547e-05\n",
      "Epoch 376/1500\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.1809e-05 - val_loss: 3.3667e-05\n",
      "Epoch 377/1500\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.3561e-05 - val_loss: 2.9479e-05\n",
      "Epoch 378/1500\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.0925e-05 - val_loss: 3.1296e-05\n",
      "Epoch 379/1500\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.0710e-05 - val_loss: 2.8222e-05\n",
      "Epoch 380/1500\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 2.8648e-05 - val_loss: 2.7668e-05\n",
      "Epoch 381/1500\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.2111e-05 - val_loss: 2.8661e-05\n",
      "Epoch 382/1500\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.0411e-05 - val_loss: 2.7621e-05\n",
      "Epoch 383/1500\n",
      "\n",
      "Epoch 00383: val_loss improved from 0.00003 to 0.00003, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.7924e-05 - val_loss: 2.6046e-05\n",
      "Epoch 384/1500\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 2.9305e-05 - val_loss: 2.6164e-05\n",
      "Epoch 385/1500\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 2.9054e-05 - val_loss: 3.1141e-05\n",
      "Epoch 386/1500\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.1201e-05 - val_loss: 2.7440e-05\n",
      "Epoch 387/1500\n",
      "\n",
      "Epoch 00387: val_loss improved from 0.00003 to 0.00003, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.8963e-05 - val_loss: 2.5216e-05\n",
      "Epoch 388/1500\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 2.9474e-05 - val_loss: 2.8205e-05\n",
      "Epoch 389/1500\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.0725e-05 - val_loss: 3.1151e-05\n",
      "Epoch 390/1500\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 2.9867e-05 - val_loss: 2.9578e-05\n",
      "Epoch 391/1500\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.2422e-05 - val_loss: 3.0033e-05\n",
      "Epoch 392/1500\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.3146e-05 - val_loss: 3.1268e-05\n",
      "Epoch 393/1500\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 2.8680e-05 - val_loss: 2.6545e-05\n",
      "Epoch 394/1500\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 2.8031e-05 - val_loss: 2.7354e-05\n",
      "Epoch 395/1500\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 2.9251e-05 - val_loss: 2.9443e-05\n",
      "Epoch 396/1500\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.0635e-05 - val_loss: 3.2475e-05\n",
      "Epoch 397/1500\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.2555e-05 - val_loss: 3.0968e-05\n",
      "Epoch 398/1500\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 2.9212e-05 - val_loss: 2.9282e-05\n",
      "Epoch 399/1500\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 2.8560e-05 - val_loss: 2.6473e-05\n",
      "Epoch 400/1500\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 2.8824e-05 - val_loss: 2.7644e-05\n",
      "Epoch 401/1500\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 2.7830e-05 - val_loss: 3.3068e-05\n",
      "Epoch 402/1500\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.1853e-05 - val_loss: 3.4327e-05\n",
      "Epoch 403/1500\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 3.1406e-05 - val_loss: 2.9247e-05\n",
      "Epoch 404/1500\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.00003\n",
      "34213/34213 - 0s - loss: 2.9345e-05 - val_loss: 2.6765e-05\n",
      "Epoch 405/1500\n",
      "\n",
      "Epoch 00405: val_loss improved from 0.00003 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.7929e-05 - val_loss: 2.4037e-05\n",
      "Epoch 406/1500\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7822e-05 - val_loss: 2.7239e-05\n",
      "Epoch 407/1500\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.8105e-05 - val_loss: 2.7741e-05\n",
      "Epoch 408/1500\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7244e-05 - val_loss: 2.6604e-05\n",
      "Epoch 409/1500\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.8464e-05 - val_loss: 2.8417e-05\n",
      "Epoch 410/1500\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 3.0793e-05 - val_loss: 2.9254e-05\n",
      "Epoch 411/1500\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.9631e-05 - val_loss: 2.6830e-05\n",
      "Epoch 412/1500\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.8202e-05 - val_loss: 2.9179e-05\n",
      "Epoch 413/1500\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.8244e-05 - val_loss: 2.9831e-05\n",
      "Epoch 414/1500\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.9722e-05 - val_loss: 3.0563e-05\n",
      "Epoch 415/1500\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.9924e-05 - val_loss: 2.8105e-05\n",
      "Epoch 416/1500\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.9801e-05 - val_loss: 3.2414e-05\n",
      "Epoch 417/1500\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.9004e-05 - val_loss: 2.8567e-05\n",
      "Epoch 418/1500\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6427e-05 - val_loss: 2.8875e-05\n",
      "Epoch 419/1500\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7055e-05 - val_loss: 2.5500e-05\n",
      "Epoch 420/1500\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7329e-05 - val_loss: 2.5864e-05\n",
      "Epoch 421/1500\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.8284e-05 - val_loss: 2.6186e-05\n",
      "Epoch 422/1500\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 3.0025e-05 - val_loss: 3.2817e-05\n",
      "Epoch 423/1500\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7955e-05 - val_loss: 3.0215e-05\n",
      "Epoch 424/1500\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.8843e-05 - val_loss: 3.0175e-05\n",
      "Epoch 425/1500\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.9674e-05 - val_loss: 3.0201e-05\n",
      "Epoch 426/1500\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.9156e-05 - val_loss: 2.8878e-05\n",
      "Epoch 427/1500\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.8610e-05 - val_loss: 2.7513e-05\n",
      "Epoch 428/1500\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7480e-05 - val_loss: 2.7151e-05\n",
      "Epoch 429/1500\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7446e-05 - val_loss: 2.6416e-05\n",
      "Epoch 430/1500\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6952e-05 - val_loss: 2.6034e-05\n",
      "Epoch 431/1500\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7198e-05 - val_loss: 2.7788e-05\n",
      "Epoch 432/1500\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.8660e-05 - val_loss: 2.8994e-05\n",
      "Epoch 433/1500\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.9273e-05 - val_loss: 2.6640e-05\n",
      "Epoch 434/1500\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.8762e-05 - val_loss: 2.7948e-05\n",
      "Epoch 435/1500\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7405e-05 - val_loss: 2.4146e-05\n",
      "Epoch 436/1500\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7963e-05 - val_loss: 2.7161e-05\n",
      "Epoch 437/1500\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6984e-05 - val_loss: 2.8252e-05\n",
      "Epoch 438/1500\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6739e-05 - val_loss: 2.7367e-05\n",
      "Epoch 439/1500\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6430e-05 - val_loss: 2.7691e-05\n",
      "Epoch 440/1500\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7149e-05 - val_loss: 2.4703e-05\n",
      "Epoch 441/1500\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6648e-05 - val_loss: 2.5051e-05\n",
      "Epoch 442/1500\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6800e-05 - val_loss: 2.7396e-05\n",
      "Epoch 443/1500\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7029e-05 - val_loss: 2.7539e-05\n",
      "Epoch 444/1500\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7687e-05 - val_loss: 2.8526e-05\n",
      "Epoch 445/1500\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7350e-05 - val_loss: 2.6695e-05\n",
      "Epoch 446/1500\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.8608e-05 - val_loss: 2.5910e-05\n",
      "Epoch 447/1500\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.8030e-05 - val_loss: 2.9274e-05\n",
      "Epoch 448/1500\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7605e-05 - val_loss: 2.6699e-05\n",
      "Epoch 449/1500\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6478e-05 - val_loss: 2.8169e-05\n",
      "Epoch 450/1500\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.8185e-05 - val_loss: 2.6387e-05\n",
      "Epoch 451/1500\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7151e-05 - val_loss: 2.9880e-05\n",
      "Epoch 452/1500\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7513e-05 - val_loss: 2.6462e-05\n",
      "Epoch 453/1500\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7090e-05 - val_loss: 2.5642e-05\n",
      "Epoch 454/1500\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6960e-05 - val_loss: 2.7793e-05\n",
      "Epoch 455/1500\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6487e-05 - val_loss: 2.5746e-05\n",
      "Epoch 456/1500\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.5250e-05 - val_loss: 2.4669e-05\n",
      "Epoch 457/1500\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.5008e-05 - val_loss: 2.4908e-05\n",
      "Epoch 458/1500\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6068e-05 - val_loss: 2.7725e-05\n",
      "Epoch 459/1500\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6959e-05 - val_loss: 2.7886e-05\n",
      "Epoch 460/1500\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6927e-05 - val_loss: 2.6530e-05\n",
      "Epoch 461/1500\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.5483e-05 - val_loss: 2.7139e-05\n",
      "Epoch 462/1500\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7762e-05 - val_loss: 2.9325e-05\n",
      "Epoch 463/1500\n",
      "\n",
      "Epoch 00463: val_loss improved from 0.00002 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.5458e-05 - val_loss: 2.2453e-05\n",
      "Epoch 464/1500\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7505e-05 - val_loss: 3.0861e-05\n",
      "Epoch 465/1500\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.8543e-05 - val_loss: 2.5752e-05\n",
      "Epoch 466/1500\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6091e-05 - val_loss: 2.5509e-05\n",
      "Epoch 467/1500\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6106e-05 - val_loss: 2.4732e-05\n",
      "Epoch 468/1500\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4815e-05 - val_loss: 2.6234e-05\n",
      "Epoch 469/1500\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6963e-05 - val_loss: 2.6329e-05\n",
      "Epoch 470/1500\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6686e-05 - val_loss: 2.5423e-05\n",
      "Epoch 471/1500\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6092e-05 - val_loss: 3.0372e-05\n",
      "Epoch 472/1500\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7454e-05 - val_loss: 2.6782e-05\n",
      "Epoch 473/1500\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.5945e-05 - val_loss: 2.3069e-05\n",
      "Epoch 474/1500\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4288e-05 - val_loss: 2.2632e-05\n",
      "Epoch 475/1500\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.5587e-05 - val_loss: 2.8231e-05\n",
      "Epoch 476/1500\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.5962e-05 - val_loss: 2.9845e-05\n",
      "Epoch 477/1500\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6885e-05 - val_loss: 2.4527e-05\n",
      "Epoch 478/1500\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.5524e-05 - val_loss: 2.4816e-05\n",
      "Epoch 479/1500\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6799e-05 - val_loss: 2.9192e-05\n",
      "Epoch 480/1500\n",
      "\n",
      "Epoch 00480: val_loss improved from 0.00002 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.5376e-05 - val_loss: 2.2137e-05\n",
      "Epoch 481/1500\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6407e-05 - val_loss: 2.6219e-05\n",
      "Epoch 482/1500\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.5300e-05 - val_loss: 2.3944e-05\n",
      "Epoch 483/1500\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4367e-05 - val_loss: 2.4527e-05\n",
      "Epoch 484/1500\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6133e-05 - val_loss: 2.8524e-05\n",
      "Epoch 485/1500\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6480e-05 - val_loss: 2.5807e-05\n",
      "Epoch 486/1500\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7296e-05 - val_loss: 2.6486e-05\n",
      "Epoch 487/1500\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7871e-05 - val_loss: 2.4611e-05\n",
      "Epoch 488/1500\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4183e-05 - val_loss: 2.2481e-05\n",
      "Epoch 489/1500\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4911e-05 - val_loss: 2.2165e-05\n",
      "Epoch 490/1500\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3872e-05 - val_loss: 2.2324e-05\n",
      "Epoch 491/1500\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3959e-05 - val_loss: 2.4227e-05\n",
      "Epoch 492/1500\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.5678e-05 - val_loss: 2.9396e-05\n",
      "Epoch 493/1500\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6130e-05 - val_loss: 2.5456e-05\n",
      "Epoch 494/1500\n",
      "\n",
      "Epoch 00494: val_loss improved from 0.00002 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.5714e-05 - val_loss: 2.1733e-05\n",
      "Epoch 495/1500\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3960e-05 - val_loss: 2.4655e-05\n",
      "Epoch 496/1500\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7723e-05 - val_loss: 2.6515e-05\n",
      "Epoch 497/1500\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6794e-05 - val_loss: 2.5909e-05\n",
      "Epoch 498/1500\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4506e-05 - val_loss: 2.1836e-05\n",
      "Epoch 499/1500\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4270e-05 - val_loss: 2.4625e-05\n",
      "Epoch 500/1500\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4833e-05 - val_loss: 2.3065e-05\n",
      "Epoch 501/1500\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4745e-05 - val_loss: 2.4160e-05\n",
      "Epoch 502/1500\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4245e-05 - val_loss: 2.1820e-05\n",
      "Epoch 503/1500\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4365e-05 - val_loss: 2.5157e-05\n",
      "Epoch 504/1500\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.5462e-05 - val_loss: 2.4091e-05\n",
      "Epoch 505/1500\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.5024e-05 - val_loss: 2.9318e-05\n",
      "Epoch 506/1500\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6812e-05 - val_loss: 2.2712e-05\n",
      "Epoch 507/1500\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3594e-05 - val_loss: 2.2392e-05\n",
      "Epoch 508/1500\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.5800e-05 - val_loss: 2.7863e-05\n",
      "Epoch 509/1500\n",
      "\n",
      "Epoch 00509: val_loss improved from 0.00002 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 2.4779e-05 - val_loss: 2.1218e-05\n",
      "Epoch 510/1500\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3058e-05 - val_loss: 2.1616e-05\n",
      "Epoch 511/1500\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4433e-05 - val_loss: 2.7050e-05\n",
      "Epoch 512/1500\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6381e-05 - val_loss: 2.5084e-05\n",
      "Epoch 513/1500\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4038e-05 - val_loss: 2.4011e-05\n",
      "Epoch 514/1500\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.5224e-05 - val_loss: 2.2911e-05\n",
      "Epoch 515/1500\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.5342e-05 - val_loss: 2.5605e-05\n",
      "Epoch 516/1500\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.5121e-05 - val_loss: 2.7627e-05\n",
      "Epoch 517/1500\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3589e-05 - val_loss: 2.2747e-05\n",
      "Epoch 518/1500\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3403e-05 - val_loss: 2.2145e-05\n",
      "Epoch 519/1500\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4240e-05 - val_loss: 2.5414e-05\n",
      "Epoch 520/1500\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4988e-05 - val_loss: 2.8869e-05\n",
      "Epoch 521/1500\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6096e-05 - val_loss: 2.4091e-05\n",
      "Epoch 522/1500\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4749e-05 - val_loss: 2.2064e-05\n",
      "Epoch 523/1500\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3045e-05 - val_loss: 2.3425e-05\n",
      "Epoch 524/1500\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3904e-05 - val_loss: 2.5158e-05\n",
      "Epoch 525/1500\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4827e-05 - val_loss: 2.4097e-05\n",
      "Epoch 526/1500\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.5193e-05 - val_loss: 2.3031e-05\n",
      "Epoch 527/1500\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2514e-05 - val_loss: 2.2123e-05\n",
      "Epoch 528/1500\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3403e-05 - val_loss: 2.1761e-05\n",
      "Epoch 529/1500\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6543e-05 - val_loss: 3.1197e-05\n",
      "Epoch 530/1500\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.7658e-05 - val_loss: 2.3537e-05\n",
      "Epoch 531/1500\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3912e-05 - val_loss: 2.3887e-05\n",
      "Epoch 532/1500\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4242e-05 - val_loss: 2.2785e-05\n",
      "Epoch 533/1500\n",
      "\n",
      "Epoch 00533: val_loss improved from 0.00002 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.3764e-05 - val_loss: 2.0792e-05\n",
      "Epoch 534/1500\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3233e-05 - val_loss: 2.2479e-05\n",
      "Epoch 535/1500\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4737e-05 - val_loss: 2.3428e-05\n",
      "Epoch 536/1500\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3055e-05 - val_loss: 2.3200e-05\n",
      "Epoch 537/1500\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2750e-05 - val_loss: 2.2344e-05\n",
      "Epoch 538/1500\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3567e-05 - val_loss: 2.5780e-05\n",
      "Epoch 539/1500\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3592e-05 - val_loss: 2.1682e-05\n",
      "Epoch 540/1500\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4959e-05 - val_loss: 2.2024e-05\n",
      "Epoch 541/1500\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4582e-05 - val_loss: 2.4559e-05\n",
      "Epoch 542/1500\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3165e-05 - val_loss: 2.2234e-05\n",
      "Epoch 543/1500\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3037e-05 - val_loss: 2.1677e-05\n",
      "Epoch 544/1500\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2714e-05 - val_loss: 2.1524e-05\n",
      "Epoch 545/1500\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3007e-05 - val_loss: 2.1323e-05\n",
      "Epoch 546/1500\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3699e-05 - val_loss: 2.1041e-05\n",
      "Epoch 547/1500\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2684e-05 - val_loss: 2.6680e-05\n",
      "Epoch 548/1500\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6481e-05 - val_loss: 2.8172e-05\n",
      "Epoch 549/1500\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4866e-05 - val_loss: 2.2393e-05\n",
      "Epoch 550/1500\n",
      "\n",
      "Epoch 00550: val_loss improved from 0.00002 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.1819e-05 - val_loss: 2.0659e-05\n",
      "Epoch 551/1500\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2864e-05 - val_loss: 2.5144e-05\n",
      "Epoch 552/1500\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.5542e-05 - val_loss: 2.3672e-05\n",
      "Epoch 553/1500\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6668e-05 - val_loss: 2.7416e-05\n",
      "Epoch 554/1500\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4544e-05 - val_loss: 3.1287e-05\n",
      "Epoch 555/1500\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.5098e-05 - val_loss: 2.1466e-05\n",
      "Epoch 556/1500\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2276e-05 - val_loss: 2.2445e-05\n",
      "Epoch 557/1500\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1858e-05 - val_loss: 2.0939e-05\n",
      "Epoch 558/1500\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2581e-05 - val_loss: 2.5428e-05\n",
      "Epoch 559/1500\n",
      "\n",
      "Epoch 00559: val_loss improved from 0.00002 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.1700e-05 - val_loss: 2.0581e-05\n",
      "Epoch 560/1500\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3127e-05 - val_loss: 2.5497e-05\n",
      "Epoch 561/1500\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3600e-05 - val_loss: 2.1021e-05\n",
      "Epoch 562/1500\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1934e-05 - val_loss: 2.0711e-05\n",
      "Epoch 563/1500\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4406e-05 - val_loss: 2.2845e-05\n",
      "Epoch 564/1500\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3785e-05 - val_loss: 2.6384e-05\n",
      "Epoch 565/1500\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.5073e-05 - val_loss: 2.3837e-05\n",
      "Epoch 566/1500\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2800e-05 - val_loss: 2.1985e-05\n",
      "Epoch 567/1500\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2352e-05 - val_loss: 2.1679e-05\n",
      "Epoch 568/1500\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1679e-05 - val_loss: 2.1028e-05\n",
      "Epoch 569/1500\n",
      "\n",
      "Epoch 00569: val_loss improved from 0.00002 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.1693e-05 - val_loss: 1.9755e-05\n",
      "Epoch 570/1500\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2351e-05 - val_loss: 2.3540e-05\n",
      "Epoch 571/1500\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3388e-05 - val_loss: 2.3810e-05\n",
      "Epoch 572/1500\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.5238e-05 - val_loss: 2.9695e-05\n",
      "Epoch 573/1500\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.5500e-05 - val_loss: 2.2492e-05\n",
      "Epoch 574/1500\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2694e-05 - val_loss: 2.2658e-05\n",
      "Epoch 575/1500\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4000e-05 - val_loss: 2.1911e-05\n",
      "Epoch 576/1500\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4594e-05 - val_loss: 2.5933e-05\n",
      "Epoch 577/1500\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2662e-05 - val_loss: 2.2132e-05\n",
      "Epoch 578/1500\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1828e-05 - val_loss: 2.0011e-05\n",
      "Epoch 579/1500\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1138e-05 - val_loss: 1.9809e-05\n",
      "Epoch 580/1500\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2762e-05 - val_loss: 2.7596e-05\n",
      "Epoch 581/1500\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4023e-05 - val_loss: 2.3735e-05\n",
      "Epoch 582/1500\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3152e-05 - val_loss: 2.0507e-05\n",
      "Epoch 583/1500\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2771e-05 - val_loss: 2.3046e-05\n",
      "Epoch 584/1500\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3830e-05 - val_loss: 2.1356e-05\n",
      "Epoch 585/1500\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1857e-05 - val_loss: 2.0091e-05\n",
      "Epoch 586/1500\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2833e-05 - val_loss: 2.2199e-05\n",
      "Epoch 587/1500\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2931e-05 - val_loss: 2.4010e-05\n",
      "Epoch 588/1500\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.5888e-05 - val_loss: 2.5510e-05\n",
      "Epoch 589/1500\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3649e-05 - val_loss: 2.2314e-05\n",
      "Epoch 590/1500\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1463e-05 - val_loss: 2.0454e-05\n",
      "Epoch 591/1500\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1873e-05 - val_loss: 2.0731e-05\n",
      "Epoch 592/1500\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2593e-05 - val_loss: 2.2159e-05\n",
      "Epoch 593/1500\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3420e-05 - val_loss: 2.2831e-05\n",
      "Epoch 594/1500\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2511e-05 - val_loss: 2.2438e-05\n",
      "Epoch 595/1500\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2108e-05 - val_loss: 2.4834e-05\n",
      "Epoch 596/1500\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2219e-05 - val_loss: 2.3592e-05\n",
      "Epoch 597/1500\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2580e-05 - val_loss: 2.0042e-05\n",
      "Epoch 598/1500\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3425e-05 - val_loss: 2.0819e-05\n",
      "Epoch 599/1500\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1118e-05 - val_loss: 2.2211e-05\n",
      "Epoch 600/1500\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6464e-05 - val_loss: 2.2641e-05\n",
      "Epoch 601/1500\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1699e-05 - val_loss: 2.0763e-05\n",
      "Epoch 602/1500\n",
      "\n",
      "Epoch 00602: val_loss improved from 0.00002 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.1220e-05 - val_loss: 1.9587e-05\n",
      "Epoch 603/1500\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0938e-05 - val_loss: 2.0434e-05\n",
      "Epoch 604/1500\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2175e-05 - val_loss: 2.3591e-05\n",
      "Epoch 605/1500\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3070e-05 - val_loss: 2.1215e-05\n",
      "Epoch 606/1500\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0658e-05 - val_loss: 2.0397e-05\n",
      "Epoch 607/1500\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2459e-05 - val_loss: 2.2171e-05\n",
      "Epoch 608/1500\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3022e-05 - val_loss: 2.3684e-05\n",
      "Epoch 609/1500\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.4444e-05 - val_loss: 2.8409e-05\n",
      "Epoch 610/1500\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2937e-05 - val_loss: 2.2061e-05\n",
      "Epoch 611/1500\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2820e-05 - val_loss: 2.5404e-05\n",
      "Epoch 612/1500\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.6865e-05 - val_loss: 2.7325e-05\n",
      "Epoch 613/1500\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0968e-05 - val_loss: 2.0903e-05\n",
      "Epoch 614/1500\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0535e-05 - val_loss: 2.2003e-05\n",
      "Epoch 615/1500\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2247e-05 - val_loss: 2.3638e-05\n",
      "Epoch 616/1500\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2109e-05 - val_loss: 2.4160e-05\n",
      "Epoch 617/1500\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2895e-05 - val_loss: 2.1125e-05\n",
      "Epoch 618/1500\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1807e-05 - val_loss: 2.0291e-05\n",
      "Epoch 619/1500\n",
      "\n",
      "Epoch 00619: val_loss improved from 0.00002 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.0820e-05 - val_loss: 1.9067e-05\n",
      "Epoch 620/1500\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0050e-05 - val_loss: 2.0361e-05\n",
      "Epoch 621/1500\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1764e-05 - val_loss: 2.4186e-05\n",
      "Epoch 622/1500\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3500e-05 - val_loss: 2.5099e-05\n",
      "Epoch 623/1500\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1879e-05 - val_loss: 2.0062e-05\n",
      "Epoch 624/1500\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1695e-05 - val_loss: 1.9919e-05\n",
      "Epoch 625/1500\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9934e-05 - val_loss: 2.2082e-05\n",
      "Epoch 626/1500\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0682e-05 - val_loss: 2.1333e-05\n",
      "Epoch 627/1500\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1972e-05 - val_loss: 2.2205e-05\n",
      "Epoch 628/1500\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2786e-05 - val_loss: 2.0321e-05\n",
      "Epoch 629/1500\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3101e-05 - val_loss: 2.3416e-05\n",
      "Epoch 630/1500\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3131e-05 - val_loss: 2.1521e-05\n",
      "Epoch 631/1500\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1758e-05 - val_loss: 2.2096e-05\n",
      "Epoch 632/1500\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2182e-05 - val_loss: 2.1179e-05\n",
      "Epoch 633/1500\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2730e-05 - val_loss: 2.0629e-05\n",
      "Epoch 634/1500\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3206e-05 - val_loss: 2.3171e-05\n",
      "Epoch 635/1500\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2660e-05 - val_loss: 2.1735e-05\n",
      "Epoch 636/1500\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0458e-05 - val_loss: 1.9276e-05\n",
      "Epoch 637/1500\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1072e-05 - val_loss: 2.0692e-05\n",
      "Epoch 638/1500\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1086e-05 - val_loss: 2.0567e-05\n",
      "Epoch 639/1500\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2858e-05 - val_loss: 2.2039e-05\n",
      "Epoch 640/1500\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3196e-05 - val_loss: 2.2764e-05\n",
      "Epoch 641/1500\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1906e-05 - val_loss: 2.0808e-05\n",
      "Epoch 642/1500\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9817e-05 - val_loss: 2.0514e-05\n",
      "Epoch 643/1500\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2032e-05 - val_loss: 2.6143e-05\n",
      "Epoch 644/1500\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1357e-05 - val_loss: 2.0404e-05\n",
      "Epoch 645/1500\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0664e-05 - val_loss: 2.1194e-05\n",
      "Epoch 646/1500\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0666e-05 - val_loss: 2.1416e-05\n",
      "Epoch 647/1500\n",
      "\n",
      "Epoch 00647: val_loss improved from 0.00002 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.1176e-05 - val_loss: 1.8822e-05\n",
      "Epoch 648/1500\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1488e-05 - val_loss: 2.1029e-05\n",
      "Epoch 649/1500\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1202e-05 - val_loss: 2.5464e-05\n",
      "Epoch 650/1500\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2773e-05 - val_loss: 2.1390e-05\n",
      "Epoch 651/1500\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3336e-05 - val_loss: 2.2956e-05\n",
      "Epoch 652/1500\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2436e-05 - val_loss: 1.9539e-05\n",
      "Epoch 653/1500\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9976e-05 - val_loss: 2.0422e-05\n",
      "Epoch 654/1500\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0928e-05 - val_loss: 2.0921e-05\n",
      "Epoch 655/1500\n",
      "\n",
      "Epoch 00655: val_loss improved from 0.00002 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.0443e-05 - val_loss: 1.8790e-05\n",
      "Epoch 656/1500\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9674e-05 - val_loss: 2.0367e-05\n",
      "Epoch 657/1500\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1628e-05 - val_loss: 2.0698e-05\n",
      "Epoch 658/1500\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2481e-05 - val_loss: 2.3191e-05\n",
      "Epoch 659/1500\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1756e-05 - val_loss: 1.9066e-05\n",
      "Epoch 660/1500\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2521e-05 - val_loss: 3.0312e-05\n",
      "Epoch 661/1500\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2530e-05 - val_loss: 2.0581e-05\n",
      "Epoch 662/1500\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0248e-05 - val_loss: 2.1253e-05\n",
      "Epoch 663/1500\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3156e-05 - val_loss: 2.3528e-05\n",
      "Epoch 664/1500\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2197e-05 - val_loss: 2.1325e-05\n",
      "Epoch 665/1500\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1073e-05 - val_loss: 2.0209e-05\n",
      "Epoch 666/1500\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0712e-05 - val_loss: 1.9578e-05\n",
      "Epoch 667/1500\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9867e-05 - val_loss: 2.3563e-05\n",
      "Epoch 668/1500\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0376e-05 - val_loss: 2.0023e-05\n",
      "Epoch 669/1500\n",
      "\n",
      "Epoch 00669: val_loss improved from 0.00002 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.9860e-05 - val_loss: 1.8066e-05\n",
      "Epoch 670/1500\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0798e-05 - val_loss: 1.9926e-05\n",
      "Epoch 671/1500\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1920e-05 - val_loss: 2.1873e-05\n",
      "Epoch 672/1500\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0454e-05 - val_loss: 1.8356e-05\n",
      "Epoch 673/1500\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2183e-05 - val_loss: 1.9871e-05\n",
      "Epoch 674/1500\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9911e-05 - val_loss: 1.9706e-05\n",
      "Epoch 675/1500\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0370e-05 - val_loss: 1.8358e-05\n",
      "Epoch 676/1500\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0511e-05 - val_loss: 2.4866e-05\n",
      "Epoch 677/1500\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1006e-05 - val_loss: 2.0426e-05\n",
      "Epoch 678/1500\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0920e-05 - val_loss: 2.2903e-05\n",
      "Epoch 679/1500\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2707e-05 - val_loss: 2.3441e-05\n",
      "Epoch 680/1500\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1390e-05 - val_loss: 2.1604e-05\n",
      "Epoch 681/1500\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1960e-05 - val_loss: 1.8447e-05\n",
      "Epoch 682/1500\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8880e-05 - val_loss: 1.9137e-05\n",
      "Epoch 683/1500\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9697e-05 - val_loss: 2.1929e-05\n",
      "Epoch 684/1500\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2156e-05 - val_loss: 2.3751e-05\n",
      "Epoch 685/1500\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2651e-05 - val_loss: 2.0103e-05\n",
      "Epoch 686/1500\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0075e-05 - val_loss: 1.9462e-05\n",
      "Epoch 687/1500\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0265e-05 - val_loss: 2.1381e-05\n",
      "Epoch 688/1500\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1903e-05 - val_loss: 2.2393e-05\n",
      "Epoch 689/1500\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2385e-05 - val_loss: 2.1044e-05\n",
      "Epoch 690/1500\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9895e-05 - val_loss: 2.0629e-05\n",
      "Epoch 691/1500\n",
      "\n",
      "Epoch 00691: val_loss improved from 0.00002 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 1s - loss: 1.8931e-05 - val_loss: 1.7521e-05\n",
      "Epoch 692/1500\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9486e-05 - val_loss: 2.0243e-05\n",
      "Epoch 693/1500\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0646e-05 - val_loss: 1.9237e-05\n",
      "Epoch 694/1500\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2093e-05 - val_loss: 2.4382e-05\n",
      "Epoch 695/1500\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1724e-05 - val_loss: 2.2769e-05\n",
      "Epoch 696/1500\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2400e-05 - val_loss: 1.9451e-05\n",
      "Epoch 697/1500\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0381e-05 - val_loss: 1.9750e-05\n",
      "Epoch 698/1500\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0090e-05 - val_loss: 2.0533e-05\n",
      "Epoch 699/1500\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8877e-05 - val_loss: 1.8736e-05\n",
      "Epoch 700/1500\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8585e-05 - val_loss: 1.8655e-05\n",
      "Epoch 701/1500\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9199e-05 - val_loss: 2.0088e-05\n",
      "Epoch 702/1500\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2550e-05 - val_loss: 2.4192e-05\n",
      "Epoch 703/1500\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2259e-05 - val_loss: 2.4683e-05\n",
      "Epoch 704/1500\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1147e-05 - val_loss: 2.1481e-05\n",
      "Epoch 705/1500\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9807e-05 - val_loss: 1.7836e-05\n",
      "Epoch 706/1500\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0031e-05 - val_loss: 2.0462e-05\n",
      "Epoch 707/1500\n",
      "\n",
      "Epoch 00707: val_loss improved from 0.00002 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 2.0154e-05 - val_loss: 1.7117e-05\n",
      "Epoch 708/1500\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9399e-05 - val_loss: 3.0307e-05\n",
      "Epoch 709/1500\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.3272e-05 - val_loss: 1.7729e-05\n",
      "Epoch 710/1500\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9392e-05 - val_loss: 1.9231e-05\n",
      "Epoch 711/1500\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1674e-05 - val_loss: 2.2045e-05\n",
      "Epoch 712/1500\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1129e-05 - val_loss: 2.0322e-05\n",
      "Epoch 713/1500\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8790e-05 - val_loss: 1.8644e-05\n",
      "Epoch 714/1500\n",
      "\n",
      "Epoch 00714: val_loss improved from 0.00002 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.8173e-05 - val_loss: 1.6782e-05\n",
      "Epoch 715/1500\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9048e-05 - val_loss: 2.0682e-05\n",
      "Epoch 716/1500\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1623e-05 - val_loss: 2.2314e-05\n",
      "Epoch 717/1500\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1451e-05 - val_loss: 1.9522e-05\n",
      "Epoch 718/1500\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1094e-05 - val_loss: 2.3005e-05\n",
      "Epoch 719/1500\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1374e-05 - val_loss: 1.8710e-05\n",
      "Epoch 720/1500\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0111e-05 - val_loss: 1.8293e-05\n",
      "Epoch 721/1500\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1139e-05 - val_loss: 2.3746e-05\n",
      "Epoch 722/1500\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2863e-05 - val_loss: 2.3011e-05\n",
      "Epoch 723/1500\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9170e-05 - val_loss: 1.7555e-05\n",
      "Epoch 724/1500\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8409e-05 - val_loss: 1.8026e-05\n",
      "Epoch 725/1500\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9069e-05 - val_loss: 1.8872e-05\n",
      "Epoch 726/1500\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0150e-05 - val_loss: 2.3383e-05\n",
      "Epoch 727/1500\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9812e-05 - val_loss: 2.0807e-05\n",
      "Epoch 728/1500\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0854e-05 - val_loss: 1.9306e-05\n",
      "Epoch 729/1500\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9208e-05 - val_loss: 1.9392e-05\n",
      "Epoch 730/1500\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9038e-05 - val_loss: 2.1625e-05\n",
      "Epoch 731/1500\n",
      "\n",
      "Epoch 00731: val_loss improved from 0.00002 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.9975e-05 - val_loss: 1.6534e-05\n",
      "Epoch 732/1500\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8160e-05 - val_loss: 1.8751e-05\n",
      "Epoch 733/1500\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9239e-05 - val_loss: 1.8440e-05\n",
      "Epoch 734/1500\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1172e-05 - val_loss: 2.2750e-05\n",
      "Epoch 735/1500\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0214e-05 - val_loss: 1.9504e-05\n",
      "Epoch 736/1500\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9675e-05 - val_loss: 1.8912e-05\n",
      "Epoch 737/1500\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8836e-05 - val_loss: 1.9522e-05\n",
      "Epoch 738/1500\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9916e-05 - val_loss: 1.9832e-05\n",
      "Epoch 739/1500\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8684e-05 - val_loss: 1.7794e-05\n",
      "Epoch 740/1500\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8749e-05 - val_loss: 1.9524e-05\n",
      "Epoch 741/1500\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0644e-05 - val_loss: 1.9744e-05\n",
      "Epoch 742/1500\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9783e-05 - val_loss: 1.7877e-05\n",
      "Epoch 743/1500\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0381e-05 - val_loss: 2.1764e-05\n",
      "Epoch 744/1500\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1069e-05 - val_loss: 2.2214e-05\n",
      "Epoch 745/1500\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9939e-05 - val_loss: 1.9124e-05\n",
      "Epoch 746/1500\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9377e-05 - val_loss: 1.9378e-05\n",
      "Epoch 747/1500\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8712e-05 - val_loss: 1.8281e-05\n",
      "Epoch 748/1500\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9064e-05 - val_loss: 1.8724e-05\n",
      "Epoch 749/1500\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9429e-05 - val_loss: 1.7968e-05\n",
      "Epoch 750/1500\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8612e-05 - val_loss: 1.8614e-05\n",
      "Epoch 751/1500\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0903e-05 - val_loss: 1.8900e-05\n",
      "Epoch 752/1500\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8127e-05 - val_loss: 1.9302e-05\n",
      "Epoch 753/1500\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9603e-05 - val_loss: 1.9897e-05\n",
      "Epoch 754/1500\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2307e-05 - val_loss: 1.9477e-05\n",
      "Epoch 755/1500\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1034e-05 - val_loss: 2.1761e-05\n",
      "Epoch 756/1500\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0107e-05 - val_loss: 1.8135e-05\n",
      "Epoch 757/1500\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7959e-05 - val_loss: 1.7678e-05\n",
      "Epoch 758/1500\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7724e-05 - val_loss: 1.8630e-05\n",
      "Epoch 759/1500\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8269e-05 - val_loss: 1.6807e-05\n",
      "Epoch 760/1500\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8785e-05 - val_loss: 1.8808e-05\n",
      "Epoch 761/1500\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9814e-05 - val_loss: 2.0753e-05\n",
      "Epoch 762/1500\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2430e-05 - val_loss: 2.7190e-05\n",
      "Epoch 763/1500\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1480e-05 - val_loss: 1.9165e-05\n",
      "Epoch 764/1500\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9188e-05 - val_loss: 1.9674e-05\n",
      "Epoch 765/1500\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9161e-05 - val_loss: 1.8307e-05\n",
      "Epoch 766/1500\n",
      "\n",
      "Epoch 00766: val_loss improved from 0.00002 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.8265e-05 - val_loss: 1.6265e-05\n",
      "Epoch 767/1500\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7623e-05 - val_loss: 1.9935e-05\n",
      "Epoch 768/1500\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9238e-05 - val_loss: 1.8959e-05\n",
      "Epoch 769/1500\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9149e-05 - val_loss: 1.8104e-05\n",
      "Epoch 770/1500\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0529e-05 - val_loss: 2.2271e-05\n",
      "Epoch 771/1500\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.1232e-05 - val_loss: 1.9592e-05\n",
      "Epoch 772/1500\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9615e-05 - val_loss: 1.9519e-05\n",
      "Epoch 773/1500\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8844e-05 - val_loss: 1.8032e-05\n",
      "Epoch 774/1500\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8375e-05 - val_loss: 1.7955e-05\n",
      "Epoch 775/1500\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0544e-05 - val_loss: 2.0391e-05\n",
      "Epoch 776/1500\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9341e-05 - val_loss: 2.0042e-05\n",
      "Epoch 777/1500\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0915e-05 - val_loss: 2.2425e-05\n",
      "Epoch 778/1500\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0004e-05 - val_loss: 1.9297e-05\n",
      "Epoch 779/1500\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8408e-05 - val_loss: 1.8142e-05\n",
      "Epoch 780/1500\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9572e-05 - val_loss: 1.7916e-05\n",
      "Epoch 781/1500\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8091e-05 - val_loss: 2.0933e-05\n",
      "Epoch 782/1500\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8546e-05 - val_loss: 1.7508e-05\n",
      "Epoch 783/1500\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8724e-05 - val_loss: 1.7572e-05\n",
      "Epoch 784/1500\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7935e-05 - val_loss: 1.6877e-05\n",
      "Epoch 785/1500\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8359e-05 - val_loss: 1.8540e-05\n",
      "Epoch 786/1500\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8631e-05 - val_loss: 1.9803e-05\n",
      "Epoch 787/1500\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9469e-05 - val_loss: 1.9520e-05\n",
      "Epoch 788/1500\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0845e-05 - val_loss: 1.8856e-05\n",
      "Epoch 789/1500\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8472e-05 - val_loss: 1.8385e-05\n",
      "Epoch 790/1500\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8879e-05 - val_loss: 2.0091e-05\n",
      "Epoch 791/1500\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9110e-05 - val_loss: 1.8312e-05\n",
      "Epoch 792/1500\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9088e-05 - val_loss: 1.8254e-05\n",
      "Epoch 793/1500\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0794e-05 - val_loss: 2.0607e-05\n",
      "Epoch 794/1500\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9232e-05 - val_loss: 1.7919e-05\n",
      "Epoch 795/1500\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8959e-05 - val_loss: 1.9666e-05\n",
      "Epoch 796/1500\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9899e-05 - val_loss: 1.7335e-05\n",
      "Epoch 797/1500\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8769e-05 - val_loss: 1.8494e-05\n",
      "Epoch 798/1500\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7862e-05 - val_loss: 1.8743e-05\n",
      "Epoch 799/1500\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8506e-05 - val_loss: 1.7166e-05\n",
      "Epoch 800/1500\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8122e-05 - val_loss: 1.7906e-05\n",
      "Epoch 801/1500\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7403e-05 - val_loss: 1.6738e-05\n",
      "Epoch 802/1500\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8401e-05 - val_loss: 1.7433e-05\n",
      "Epoch 803/1500\n",
      "\n",
      "Epoch 00803: val_loss improved from 0.00002 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.7836e-05 - val_loss: 1.5802e-05\n",
      "Epoch 804/1500\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9138e-05 - val_loss: 1.8681e-05\n",
      "Epoch 805/1500\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9568e-05 - val_loss: 1.9294e-05\n",
      "Epoch 806/1500\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9819e-05 - val_loss: 1.7632e-05\n",
      "Epoch 807/1500\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9881e-05 - val_loss: 1.9345e-05\n",
      "Epoch 808/1500\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8769e-05 - val_loss: 1.8752e-05\n",
      "Epoch 809/1500\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0259e-05 - val_loss: 1.8669e-05\n",
      "Epoch 810/1500\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8298e-05 - val_loss: 1.9289e-05\n",
      "Epoch 811/1500\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9318e-05 - val_loss: 1.8574e-05\n",
      "Epoch 812/1500\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7430e-05 - val_loss: 1.6077e-05\n",
      "Epoch 813/1500\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7939e-05 - val_loss: 1.9122e-05\n",
      "Epoch 814/1500\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8927e-05 - val_loss: 2.0238e-05\n",
      "Epoch 815/1500\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0433e-05 - val_loss: 2.0915e-05\n",
      "Epoch 816/1500\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0268e-05 - val_loss: 1.8718e-05\n",
      "Epoch 817/1500\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8187e-05 - val_loss: 1.7440e-05\n",
      "Epoch 818/1500\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7217e-05 - val_loss: 1.6427e-05\n",
      "Epoch 819/1500\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7196e-05 - val_loss: 1.6794e-05\n",
      "Epoch 820/1500\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7339e-05 - val_loss: 1.7285e-05\n",
      "Epoch 821/1500\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9786e-05 - val_loss: 1.9218e-05\n",
      "Epoch 822/1500\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9577e-05 - val_loss: 1.7155e-05\n",
      "Epoch 823/1500\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8351e-05 - val_loss: 1.9918e-05\n",
      "Epoch 824/1500\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9744e-05 - val_loss: 1.9650e-05\n",
      "Epoch 825/1500\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8997e-05 - val_loss: 1.8857e-05\n",
      "Epoch 826/1500\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7824e-05 - val_loss: 1.7862e-05\n",
      "Epoch 827/1500\n",
      "\n",
      "Epoch 00827: val_loss improved from 0.00002 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.7272e-05 - val_loss: 1.5702e-05\n",
      "Epoch 828/1500\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8308e-05 - val_loss: 1.7245e-05\n",
      "Epoch 829/1500\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7659e-05 - val_loss: 1.7583e-05\n",
      "Epoch 830/1500\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9835e-05 - val_loss: 2.1855e-05\n",
      "Epoch 831/1500\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9955e-05 - val_loss: 1.9324e-05\n",
      "Epoch 832/1500\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7760e-05 - val_loss: 1.8512e-05\n",
      "Epoch 833/1500\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8183e-05 - val_loss: 1.8074e-05\n",
      "Epoch 834/1500\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9472e-05 - val_loss: 1.9436e-05\n",
      "Epoch 835/1500\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8037e-05 - val_loss: 1.8569e-05\n",
      "Epoch 836/1500\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7876e-05 - val_loss: 1.8116e-05\n",
      "Epoch 837/1500\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8921e-05 - val_loss: 1.9565e-05\n",
      "Epoch 838/1500\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9445e-05 - val_loss: 1.7273e-05\n",
      "Epoch 839/1500\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.6496e-05 - val_loss: 1.8660e-05\n",
      "Epoch 840/1500\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8578e-05 - val_loss: 1.7670e-05\n",
      "Epoch 841/1500\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7689e-05 - val_loss: 1.7742e-05\n",
      "Epoch 842/1500\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8282e-05 - val_loss: 2.0483e-05\n",
      "Epoch 843/1500\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8915e-05 - val_loss: 1.8120e-05\n",
      "Epoch 844/1500\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7053e-05 - val_loss: 1.7559e-05\n",
      "Epoch 845/1500\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7933e-05 - val_loss: 1.8600e-05\n",
      "Epoch 846/1500\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8754e-05 - val_loss: 2.0037e-05\n",
      "Epoch 847/1500\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0083e-05 - val_loss: 2.2569e-05\n",
      "Epoch 848/1500\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7824e-05 - val_loss: 1.6323e-05\n",
      "Epoch 849/1500\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7329e-05 - val_loss: 1.9283e-05\n",
      "Epoch 850/1500\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9281e-05 - val_loss: 1.9083e-05\n",
      "Epoch 851/1500\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7674e-05 - val_loss: 1.8364e-05\n",
      "Epoch 852/1500\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8664e-05 - val_loss: 2.0123e-05\n",
      "Epoch 853/1500\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.2494e-05 - val_loss: 2.5156e-05\n",
      "Epoch 854/1500\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 2.0666e-05 - val_loss: 1.8030e-05\n",
      "Epoch 855/1500\n",
      "\n",
      "Epoch 00855: val_loss improved from 0.00002 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.7443e-05 - val_loss: 1.5601e-05\n",
      "Epoch 856/1500\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.6564e-05 - val_loss: 1.7920e-05\n",
      "Epoch 857/1500\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7907e-05 - val_loss: 1.8203e-05\n",
      "Epoch 858/1500\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7794e-05 - val_loss: 1.5860e-05\n",
      "Epoch 859/1500\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7350e-05 - val_loss: 2.0399e-05\n",
      "Epoch 860/1500\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9343e-05 - val_loss: 1.7237e-05\n",
      "Epoch 861/1500\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7560e-05 - val_loss: 1.6990e-05\n",
      "Epoch 862/1500\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.8370e-05 - val_loss: 2.1869e-05\n",
      "Epoch 863/1500\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9074e-05 - val_loss: 1.9291e-05\n",
      "Epoch 864/1500\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.9197e-05 - val_loss: 1.6289e-05\n",
      "Epoch 865/1500\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7134e-05 - val_loss: 1.6280e-05\n",
      "Epoch 866/1500\n",
      "\n",
      "Epoch 00866: val_loss improved from 0.00002 to 0.00002, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.8143e-05 - val_loss: 1.5579e-05\n",
      "Epoch 867/1500\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 0.00002\n",
      "34213/34213 - 0s - loss: 1.7162e-05 - val_loss: 1.6346e-05\n",
      "Epoch 868/1500\n",
      "\n",
      "Epoch 00868: val_loss improved from 0.00002 to 0.00001, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.6112e-05 - val_loss: 1.4944e-05\n",
      "Epoch 869/1500\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6760e-05 - val_loss: 1.5698e-05\n",
      "Epoch 870/1500\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7041e-05 - val_loss: 1.9760e-05\n",
      "Epoch 871/1500\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8598e-05 - val_loss: 1.9102e-05\n",
      "Epoch 872/1500\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7978e-05 - val_loss: 1.7786e-05\n",
      "Epoch 873/1500\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8791e-05 - val_loss: 2.0477e-05\n",
      "Epoch 874/1500\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.9863e-05 - val_loss: 1.8958e-05\n",
      "Epoch 875/1500\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7904e-05 - val_loss: 1.8238e-05\n",
      "Epoch 876/1500\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7585e-05 - val_loss: 1.7974e-05\n",
      "Epoch 877/1500\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.9341e-05 - val_loss: 1.8340e-05\n",
      "Epoch 878/1500\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6923e-05 - val_loss: 1.6921e-05\n",
      "Epoch 879/1500\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6287e-05 - val_loss: 1.6802e-05\n",
      "Epoch 880/1500\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6642e-05 - val_loss: 1.9265e-05\n",
      "Epoch 881/1500\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8864e-05 - val_loss: 1.7595e-05\n",
      "Epoch 882/1500\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7930e-05 - val_loss: 1.8287e-05\n",
      "Epoch 883/1500\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.9246e-05 - val_loss: 1.8105e-05\n",
      "Epoch 884/1500\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7603e-05 - val_loss: 1.5957e-05\n",
      "Epoch 885/1500\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8392e-05 - val_loss: 1.5646e-05\n",
      "Epoch 886/1500\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7953e-05 - val_loss: 1.7722e-05\n",
      "Epoch 887/1500\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7342e-05 - val_loss: 1.7414e-05\n",
      "Epoch 888/1500\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6987e-05 - val_loss: 1.7362e-05\n",
      "Epoch 889/1500\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8463e-05 - val_loss: 1.9244e-05\n",
      "Epoch 890/1500\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 2.0158e-05 - val_loss: 1.7422e-05\n",
      "Epoch 891/1500\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7607e-05 - val_loss: 1.7664e-05\n",
      "Epoch 892/1500\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6740e-05 - val_loss: 1.5645e-05\n",
      "Epoch 893/1500\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7040e-05 - val_loss: 1.5225e-05\n",
      "Epoch 894/1500\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6449e-05 - val_loss: 1.7347e-05\n",
      "Epoch 895/1500\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7433e-05 - val_loss: 1.8123e-05\n",
      "Epoch 896/1500\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8088e-05 - val_loss: 1.9747e-05\n",
      "Epoch 897/1500\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7864e-05 - val_loss: 2.0954e-05\n",
      "Epoch 898/1500\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.9955e-05 - val_loss: 2.2229e-05\n",
      "Epoch 899/1500\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7088e-05 - val_loss: 1.8028e-05\n",
      "Epoch 900/1500\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8077e-05 - val_loss: 1.8588e-05\n",
      "Epoch 901/1500\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6803e-05 - val_loss: 1.5110e-05\n",
      "Epoch 902/1500\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6447e-05 - val_loss: 1.6739e-05\n",
      "Epoch 903/1500\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8595e-05 - val_loss: 1.8312e-05\n",
      "Epoch 904/1500\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7858e-05 - val_loss: 1.6657e-05\n",
      "Epoch 905/1500\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 2.0407e-05 - val_loss: 1.9826e-05\n",
      "Epoch 906/1500\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.9604e-05 - val_loss: 1.7580e-05\n",
      "Epoch 907/1500\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6586e-05 - val_loss: 1.5580e-05\n",
      "Epoch 908/1500\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4952e-05 - val_loss: 1.5580e-05\n",
      "Epoch 909/1500\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7367e-05 - val_loss: 1.7941e-05\n",
      "Epoch 910/1500\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6595e-05 - val_loss: 1.6885e-05\n",
      "Epoch 911/1500\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.9934e-05 - val_loss: 2.0302e-05\n",
      "Epoch 912/1500\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7609e-05 - val_loss: 1.6346e-05\n",
      "Epoch 913/1500\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7358e-05 - val_loss: 1.6903e-05\n",
      "Epoch 914/1500\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6598e-05 - val_loss: 1.5503e-05\n",
      "Epoch 915/1500\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.9040e-05 - val_loss: 2.1062e-05\n",
      "Epoch 916/1500\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.9261e-05 - val_loss: 1.8711e-05\n",
      "Epoch 917/1500\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8062e-05 - val_loss: 1.7483e-05\n",
      "Epoch 918/1500\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6591e-05 - val_loss: 1.7641e-05\n",
      "Epoch 919/1500\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7958e-05 - val_loss: 1.6191e-05\n",
      "Epoch 920/1500\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6715e-05 - val_loss: 1.7589e-05\n",
      "Epoch 921/1500\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5812e-05 - val_loss: 1.6357e-05\n",
      "Epoch 922/1500\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8252e-05 - val_loss: 2.1067e-05\n",
      "Epoch 923/1500\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 2.1215e-05 - val_loss: 1.6813e-05\n",
      "Epoch 924/1500\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7746e-05 - val_loss: 1.5290e-05\n",
      "Epoch 925/1500\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7442e-05 - val_loss: 1.9265e-05\n",
      "Epoch 926/1500\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6458e-05 - val_loss: 1.6105e-05\n",
      "Epoch 927/1500\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6336e-05 - val_loss: 1.6514e-05\n",
      "Epoch 928/1500\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6386e-05 - val_loss: 1.8562e-05\n",
      "Epoch 929/1500\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6515e-05 - val_loss: 1.6471e-05\n",
      "Epoch 930/1500\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7829e-05 - val_loss: 2.3177e-05\n",
      "Epoch 931/1500\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.9608e-05 - val_loss: 1.8085e-05\n",
      "Epoch 932/1500\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6818e-05 - val_loss: 1.6061e-05\n",
      "Epoch 933/1500\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6178e-05 - val_loss: 1.5002e-05\n",
      "Epoch 934/1500\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6975e-05 - val_loss: 1.8051e-05\n",
      "Epoch 935/1500\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7572e-05 - val_loss: 1.8440e-05\n",
      "Epoch 936/1500\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7550e-05 - val_loss: 1.6511e-05\n",
      "Epoch 937/1500\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7040e-05 - val_loss: 1.7592e-05\n",
      "Epoch 938/1500\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7225e-05 - val_loss: 1.6963e-05\n",
      "Epoch 939/1500\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7619e-05 - val_loss: 1.7948e-05\n",
      "Epoch 940/1500\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7626e-05 - val_loss: 1.9057e-05\n",
      "Epoch 941/1500\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7015e-05 - val_loss: 1.8038e-05\n",
      "Epoch 942/1500\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7106e-05 - val_loss: 1.8566e-05\n",
      "Epoch 943/1500\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8413e-05 - val_loss: 1.5206e-05\n",
      "Epoch 944/1500\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6398e-05 - val_loss: 1.5336e-05\n",
      "Epoch 945/1500\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6263e-05 - val_loss: 1.6405e-05\n",
      "Epoch 946/1500\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8393e-05 - val_loss: 1.7918e-05\n",
      "Epoch 947/1500\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7935e-05 - val_loss: 2.5482e-05\n",
      "Epoch 948/1500\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.9494e-05 - val_loss: 1.8118e-05\n",
      "Epoch 949/1500\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6987e-05 - val_loss: 1.5529e-05\n",
      "Epoch 950/1500\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5974e-05 - val_loss: 1.5137e-05\n",
      "Epoch 951/1500\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5427e-05 - val_loss: 1.7411e-05\n",
      "Epoch 952/1500\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7185e-05 - val_loss: 1.6809e-05\n",
      "Epoch 953/1500\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7138e-05 - val_loss: 1.8319e-05\n",
      "Epoch 954/1500\n",
      "\n",
      "Epoch 00954: val_loss improved from 0.00001 to 0.00001, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.6679e-05 - val_loss: 1.4297e-05\n",
      "Epoch 955/1500\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6385e-05 - val_loss: 1.6979e-05\n",
      "Epoch 956/1500\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6626e-05 - val_loss: 1.6628e-05\n",
      "Epoch 957/1500\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8977e-05 - val_loss: 2.2048e-05\n",
      "Epoch 958/1500\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 2.1791e-05 - val_loss: 1.8539e-05\n",
      "Epoch 959/1500\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7661e-05 - val_loss: 1.5335e-05\n",
      "Epoch 960/1500\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5576e-05 - val_loss: 1.4652e-05\n",
      "Epoch 961/1500\n",
      "\n",
      "Epoch 00961: val_loss improved from 0.00001 to 0.00001, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.4696e-05 - val_loss: 1.3934e-05\n",
      "Epoch 962/1500\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5648e-05 - val_loss: 1.6830e-05\n",
      "Epoch 963/1500\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6892e-05 - val_loss: 1.7272e-05\n",
      "Epoch 964/1500\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6826e-05 - val_loss: 1.6132e-05\n",
      "Epoch 965/1500\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7018e-05 - val_loss: 1.8090e-05\n",
      "Epoch 966/1500\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8492e-05 - val_loss: 1.8338e-05\n",
      "Epoch 967/1500\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7919e-05 - val_loss: 1.7209e-05\n",
      "Epoch 968/1500\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7412e-05 - val_loss: 1.8599e-05\n",
      "Epoch 969/1500\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8183e-05 - val_loss: 1.9021e-05\n",
      "Epoch 970/1500\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6222e-05 - val_loss: 1.5546e-05\n",
      "Epoch 971/1500\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6208e-05 - val_loss: 1.6105e-05\n",
      "Epoch 972/1500\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.9103e-05 - val_loss: 1.9470e-05\n",
      "Epoch 973/1500\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7390e-05 - val_loss: 1.5987e-05\n",
      "Epoch 974/1500\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5836e-05 - val_loss: 1.4058e-05\n",
      "Epoch 975/1500\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4876e-05 - val_loss: 1.5018e-05\n",
      "Epoch 976/1500\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5794e-05 - val_loss: 1.5369e-05\n",
      "Epoch 977/1500\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6500e-05 - val_loss: 1.5836e-05\n",
      "Epoch 978/1500\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6135e-05 - val_loss: 1.7496e-05\n",
      "Epoch 979/1500\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7773e-05 - val_loss: 2.0391e-05\n",
      "Epoch 980/1500\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8747e-05 - val_loss: 1.8366e-05\n",
      "Epoch 981/1500\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6610e-05 - val_loss: 1.6021e-05\n",
      "Epoch 982/1500\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7091e-05 - val_loss: 1.6578e-05\n",
      "Epoch 983/1500\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7124e-05 - val_loss: 1.8898e-05\n",
      "Epoch 984/1500\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8511e-05 - val_loss: 1.9168e-05\n",
      "Epoch 985/1500\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7269e-05 - val_loss: 1.6643e-05\n",
      "Epoch 986/1500\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7835e-05 - val_loss: 2.0046e-05\n",
      "Epoch 987/1500\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.9736e-05 - val_loss: 1.9818e-05\n",
      "Epoch 988/1500\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7508e-05 - val_loss: 1.5288e-05\n",
      "Epoch 989/1500\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6325e-05 - val_loss: 1.6238e-05\n",
      "Epoch 990/1500\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6426e-05 - val_loss: 1.7688e-05\n",
      "Epoch 991/1500\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6810e-05 - val_loss: 1.4789e-05\n",
      "Epoch 992/1500\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5633e-05 - val_loss: 1.7852e-05\n",
      "Epoch 993/1500\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5916e-05 - val_loss: 1.6690e-05\n",
      "Epoch 994/1500\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6089e-05 - val_loss: 1.8281e-05\n",
      "Epoch 995/1500\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5751e-05 - val_loss: 1.5355e-05\n",
      "Epoch 996/1500\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6414e-05 - val_loss: 1.5861e-05\n",
      "Epoch 997/1500\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7900e-05 - val_loss: 2.1090e-05\n",
      "Epoch 998/1500\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 2.0102e-05 - val_loss: 1.8302e-05\n",
      "Epoch 999/1500\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6344e-05 - val_loss: 1.6070e-05\n",
      "Epoch 1000/1500\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6555e-05 - val_loss: 1.7966e-05\n",
      "Epoch 1001/1500\n",
      "\n",
      "Epoch 01001: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5810e-05 - val_loss: 1.7830e-05\n",
      "Epoch 1002/1500\n",
      "\n",
      "Epoch 01002: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8421e-05 - val_loss: 1.4020e-05\n",
      "Epoch 1003/1500\n",
      "\n",
      "Epoch 01003: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5601e-05 - val_loss: 1.4416e-05\n",
      "Epoch 1004/1500\n",
      "\n",
      "Epoch 01004: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5146e-05 - val_loss: 1.6371e-05\n",
      "Epoch 1005/1500\n",
      "\n",
      "Epoch 01005: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6031e-05 - val_loss: 1.5069e-05\n",
      "Epoch 1006/1500\n",
      "\n",
      "Epoch 01006: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6070e-05 - val_loss: 1.6530e-05\n",
      "Epoch 1007/1500\n",
      "\n",
      "Epoch 01007: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6195e-05 - val_loss: 1.8141e-05\n",
      "Epoch 1008/1500\n",
      "\n",
      "Epoch 01008: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6538e-05 - val_loss: 1.7621e-05\n",
      "Epoch 1009/1500\n",
      "\n",
      "Epoch 01009: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6407e-05 - val_loss: 1.7670e-05\n",
      "Epoch 1010/1500\n",
      "\n",
      "Epoch 01010: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6372e-05 - val_loss: 1.7686e-05\n",
      "Epoch 1011/1500\n",
      "\n",
      "Epoch 01011: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6344e-05 - val_loss: 1.7339e-05\n",
      "Epoch 1012/1500\n",
      "\n",
      "Epoch 01012: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5719e-05 - val_loss: 1.6697e-05\n",
      "Epoch 1013/1500\n",
      "\n",
      "Epoch 01013: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8928e-05 - val_loss: 2.2005e-05\n",
      "Epoch 1014/1500\n",
      "\n",
      "Epoch 01014: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7070e-05 - val_loss: 1.5848e-05\n",
      "Epoch 1015/1500\n",
      "\n",
      "Epoch 01015: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7740e-05 - val_loss: 1.7949e-05\n",
      "Epoch 1016/1500\n",
      "\n",
      "Epoch 01016: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6905e-05 - val_loss: 1.4313e-05\n",
      "Epoch 1017/1500\n",
      "\n",
      "Epoch 01017: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5812e-05 - val_loss: 1.5224e-05\n",
      "Epoch 1018/1500\n",
      "\n",
      "Epoch 01018: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5914e-05 - val_loss: 1.6059e-05\n",
      "Epoch 1019/1500\n",
      "\n",
      "Epoch 01019: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.9854e-05 - val_loss: 1.6287e-05\n",
      "Epoch 1020/1500\n",
      "\n",
      "Epoch 01020: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6836e-05 - val_loss: 1.6795e-05\n",
      "Epoch 1021/1500\n",
      "\n",
      "Epoch 01021: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5164e-05 - val_loss: 1.6721e-05\n",
      "Epoch 1022/1500\n",
      "\n",
      "Epoch 01022: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5825e-05 - val_loss: 1.5427e-05\n",
      "Epoch 1023/1500\n",
      "\n",
      "Epoch 01023: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6238e-05 - val_loss: 1.8662e-05\n",
      "Epoch 1024/1500\n",
      "\n",
      "Epoch 01024: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7703e-05 - val_loss: 1.9036e-05\n",
      "Epoch 1025/1500\n",
      "\n",
      "Epoch 01025: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8264e-05 - val_loss: 1.6192e-05\n",
      "Epoch 1026/1500\n",
      "\n",
      "Epoch 01026: val_loss improved from 0.00001 to 0.00001, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.4641e-05 - val_loss: 1.2809e-05\n",
      "Epoch 1027/1500\n",
      "\n",
      "Epoch 01027: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5329e-05 - val_loss: 1.9705e-05\n",
      "Epoch 1028/1500\n",
      "\n",
      "Epoch 01028: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8713e-05 - val_loss: 1.7357e-05\n",
      "Epoch 1029/1500\n",
      "\n",
      "Epoch 01029: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6775e-05 - val_loss: 1.5122e-05\n",
      "Epoch 1030/1500\n",
      "\n",
      "Epoch 01030: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5448e-05 - val_loss: 1.5247e-05\n",
      "Epoch 1031/1500\n",
      "\n",
      "Epoch 01031: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4790e-05 - val_loss: 1.6284e-05\n",
      "Epoch 1032/1500\n",
      "\n",
      "Epoch 01032: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7723e-05 - val_loss: 1.7148e-05\n",
      "Epoch 1033/1500\n",
      "\n",
      "Epoch 01033: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6618e-05 - val_loss: 1.5175e-05\n",
      "Epoch 1034/1500\n",
      "\n",
      "Epoch 01034: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5350e-05 - val_loss: 1.5495e-05\n",
      "Epoch 1035/1500\n",
      "\n",
      "Epoch 01035: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5593e-05 - val_loss: 1.5550e-05\n",
      "Epoch 1036/1500\n",
      "\n",
      "Epoch 01036: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5806e-05 - val_loss: 1.5285e-05\n",
      "Epoch 1037/1500\n",
      "\n",
      "Epoch 01037: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6128e-05 - val_loss: 1.6631e-05\n",
      "Epoch 1038/1500\n",
      "\n",
      "Epoch 01038: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5876e-05 - val_loss: 1.5846e-05\n",
      "Epoch 1039/1500\n",
      "\n",
      "Epoch 01039: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6691e-05 - val_loss: 1.5578e-05\n",
      "Epoch 1040/1500\n",
      "\n",
      "Epoch 01040: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5884e-05 - val_loss: 1.5795e-05\n",
      "Epoch 1041/1500\n",
      "\n",
      "Epoch 01041: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6308e-05 - val_loss: 2.0363e-05\n",
      "Epoch 1042/1500\n",
      "\n",
      "Epoch 01042: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8900e-05 - val_loss: 1.6965e-05\n",
      "Epoch 1043/1500\n",
      "\n",
      "Epoch 01043: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7425e-05 - val_loss: 1.8906e-05\n",
      "Epoch 1044/1500\n",
      "\n",
      "Epoch 01044: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6613e-05 - val_loss: 1.7404e-05\n",
      "Epoch 1045/1500\n",
      "\n",
      "Epoch 01045: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5947e-05 - val_loss: 1.3632e-05\n",
      "Epoch 1046/1500\n",
      "\n",
      "Epoch 01046: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4772e-05 - val_loss: 1.3693e-05\n",
      "Epoch 1047/1500\n",
      "\n",
      "Epoch 01047: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4771e-05 - val_loss: 1.6110e-05\n",
      "Epoch 1048/1500\n",
      "\n",
      "Epoch 01048: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7966e-05 - val_loss: 1.8321e-05\n",
      "Epoch 1049/1500\n",
      "\n",
      "Epoch 01049: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7530e-05 - val_loss: 1.5785e-05\n",
      "Epoch 1050/1500\n",
      "\n",
      "Epoch 01050: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5534e-05 - val_loss: 1.5255e-05\n",
      "Epoch 1051/1500\n",
      "\n",
      "Epoch 01051: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4686e-05 - val_loss: 1.6929e-05\n",
      "Epoch 1052/1500\n",
      "\n",
      "Epoch 01052: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8034e-05 - val_loss: 1.6890e-05\n",
      "Epoch 1053/1500\n",
      "\n",
      "Epoch 01053: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6363e-05 - val_loss: 1.5251e-05\n",
      "Epoch 1054/1500\n",
      "\n",
      "Epoch 01054: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4885e-05 - val_loss: 1.4169e-05\n",
      "Epoch 1055/1500\n",
      "\n",
      "Epoch 01055: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4480e-05 - val_loss: 1.4823e-05\n",
      "Epoch 1056/1500\n",
      "\n",
      "Epoch 01056: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6419e-05 - val_loss: 1.8161e-05\n",
      "Epoch 1057/1500\n",
      "\n",
      "Epoch 01057: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6410e-05 - val_loss: 1.9054e-05\n",
      "Epoch 1058/1500\n",
      "\n",
      "Epoch 01058: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8276e-05 - val_loss: 1.7721e-05\n",
      "Epoch 1059/1500\n",
      "\n",
      "Epoch 01059: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.9093e-05 - val_loss: 1.4657e-05\n",
      "Epoch 1060/1500\n",
      "\n",
      "Epoch 01060: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4466e-05 - val_loss: 1.4623e-05\n",
      "Epoch 1061/1500\n",
      "\n",
      "Epoch 01061: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4545e-05 - val_loss: 1.6540e-05\n",
      "Epoch 1062/1500\n",
      "\n",
      "Epoch 01062: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5184e-05 - val_loss: 1.5165e-05\n",
      "Epoch 1063/1500\n",
      "\n",
      "Epoch 01063: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6251e-05 - val_loss: 1.8060e-05\n",
      "Epoch 1064/1500\n",
      "\n",
      "Epoch 01064: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6578e-05 - val_loss: 1.4997e-05\n",
      "Epoch 1065/1500\n",
      "\n",
      "Epoch 01065: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6018e-05 - val_loss: 1.5565e-05\n",
      "Epoch 1066/1500\n",
      "\n",
      "Epoch 01066: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5674e-05 - val_loss: 1.4697e-05\n",
      "Epoch 1067/1500\n",
      "\n",
      "Epoch 01067: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6331e-05 - val_loss: 1.9305e-05\n",
      "Epoch 1068/1500\n",
      "\n",
      "Epoch 01068: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5938e-05 - val_loss: 1.5862e-05\n",
      "Epoch 1069/1500\n",
      "\n",
      "Epoch 01069: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5818e-05 - val_loss: 1.5391e-05\n",
      "Epoch 1070/1500\n",
      "\n",
      "Epoch 01070: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5549e-05 - val_loss: 1.6728e-05\n",
      "Epoch 1071/1500\n",
      "\n",
      "Epoch 01071: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7357e-05 - val_loss: 1.6268e-05\n",
      "Epoch 1072/1500\n",
      "\n",
      "Epoch 01072: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5404e-05 - val_loss: 1.7676e-05\n",
      "Epoch 1073/1500\n",
      "\n",
      "Epoch 01073: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5707e-05 - val_loss: 1.6689e-05\n",
      "Epoch 1074/1500\n",
      "\n",
      "Epoch 01074: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7485e-05 - val_loss: 1.5611e-05\n",
      "Epoch 1075/1500\n",
      "\n",
      "Epoch 01075: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6497e-05 - val_loss: 1.6079e-05\n",
      "Epoch 1076/1500\n",
      "\n",
      "Epoch 01076: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5718e-05 - val_loss: 1.5206e-05\n",
      "Epoch 1077/1500\n",
      "\n",
      "Epoch 01077: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6642e-05 - val_loss: 1.5640e-05\n",
      "Epoch 1078/1500\n",
      "\n",
      "Epoch 01078: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5586e-05 - val_loss: 1.5610e-05\n",
      "Epoch 1079/1500\n",
      "\n",
      "Epoch 01079: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4762e-05 - val_loss: 1.5222e-05\n",
      "Epoch 1080/1500\n",
      "\n",
      "Epoch 01080: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5148e-05 - val_loss: 1.5792e-05\n",
      "Epoch 1081/1500\n",
      "\n",
      "Epoch 01081: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6762e-05 - val_loss: 1.5294e-05\n",
      "Epoch 1082/1500\n",
      "\n",
      "Epoch 01082: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4964e-05 - val_loss: 1.5892e-05\n",
      "Epoch 1083/1500\n",
      "\n",
      "Epoch 01083: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5246e-05 - val_loss: 1.6069e-05\n",
      "Epoch 1084/1500\n",
      "\n",
      "Epoch 01084: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6324e-05 - val_loss: 1.4673e-05\n",
      "Epoch 1085/1500\n",
      "\n",
      "Epoch 01085: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5312e-05 - val_loss: 1.9957e-05\n",
      "Epoch 1086/1500\n",
      "\n",
      "Epoch 01086: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5883e-05 - val_loss: 1.5415e-05\n",
      "Epoch 1087/1500\n",
      "\n",
      "Epoch 01087: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5811e-05 - val_loss: 1.8197e-05\n",
      "Epoch 1088/1500\n",
      "\n",
      "Epoch 01088: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6475e-05 - val_loss: 1.3738e-05\n",
      "Epoch 1089/1500\n",
      "\n",
      "Epoch 01089: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4999e-05 - val_loss: 1.5378e-05\n",
      "Epoch 1090/1500\n",
      "\n",
      "Epoch 01090: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5390e-05 - val_loss: 1.5805e-05\n",
      "Epoch 1091/1500\n",
      "\n",
      "Epoch 01091: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5608e-05 - val_loss: 1.6649e-05\n",
      "Epoch 1092/1500\n",
      "\n",
      "Epoch 01092: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6080e-05 - val_loss: 1.8985e-05\n",
      "Epoch 1093/1500\n",
      "\n",
      "Epoch 01093: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7668e-05 - val_loss: 1.5946e-05\n",
      "Epoch 1094/1500\n",
      "\n",
      "Epoch 01094: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5366e-05 - val_loss: 1.6957e-05\n",
      "Epoch 1095/1500\n",
      "\n",
      "Epoch 01095: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5282e-05 - val_loss: 1.5207e-05\n",
      "Epoch 1096/1500\n",
      "\n",
      "Epoch 01096: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5011e-05 - val_loss: 1.4819e-05\n",
      "Epoch 1097/1500\n",
      "\n",
      "Epoch 01097: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7708e-05 - val_loss: 1.8473e-05\n",
      "Epoch 1098/1500\n",
      "\n",
      "Epoch 01098: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6839e-05 - val_loss: 1.7479e-05\n",
      "Epoch 1099/1500\n",
      "\n",
      "Epoch 01099: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5906e-05 - val_loss: 1.4897e-05\n",
      "Epoch 1100/1500\n",
      "\n",
      "Epoch 01100: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5428e-05 - val_loss: 1.4100e-05\n",
      "Epoch 1101/1500\n",
      "\n",
      "Epoch 01101: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4653e-05 - val_loss: 1.6973e-05\n",
      "Epoch 1102/1500\n",
      "\n",
      "Epoch 01102: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4870e-05 - val_loss: 1.4732e-05\n",
      "Epoch 1103/1500\n",
      "\n",
      "Epoch 01103: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5045e-05 - val_loss: 1.6018e-05\n",
      "Epoch 1104/1500\n",
      "\n",
      "Epoch 01104: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5108e-05 - val_loss: 1.6747e-05\n",
      "Epoch 1105/1500\n",
      "\n",
      "Epoch 01105: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7742e-05 - val_loss: 1.7073e-05\n",
      "Epoch 1106/1500\n",
      "\n",
      "Epoch 01106: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5782e-05 - val_loss: 1.6873e-05\n",
      "Epoch 1107/1500\n",
      "\n",
      "Epoch 01107: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5535e-05 - val_loss: 1.6445e-05\n",
      "Epoch 1108/1500\n",
      "\n",
      "Epoch 01108: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6389e-05 - val_loss: 1.3705e-05\n",
      "Epoch 1109/1500\n",
      "\n",
      "Epoch 01109: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3983e-05 - val_loss: 1.5400e-05\n",
      "Epoch 1110/1500\n",
      "\n",
      "Epoch 01110: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5452e-05 - val_loss: 1.6830e-05\n",
      "Epoch 1111/1500\n",
      "\n",
      "Epoch 01111: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8241e-05 - val_loss: 1.7010e-05\n",
      "Epoch 1112/1500\n",
      "\n",
      "Epoch 01112: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6492e-05 - val_loss: 1.5171e-05\n",
      "Epoch 1113/1500\n",
      "\n",
      "Epoch 01113: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6151e-05 - val_loss: 1.5153e-05\n",
      "Epoch 1114/1500\n",
      "\n",
      "Epoch 01114: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6227e-05 - val_loss: 1.6068e-05\n",
      "Epoch 1115/1500\n",
      "\n",
      "Epoch 01115: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4840e-05 - val_loss: 1.5125e-05\n",
      "Epoch 1116/1500\n",
      "\n",
      "Epoch 01116: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4052e-05 - val_loss: 1.3582e-05\n",
      "Epoch 1117/1500\n",
      "\n",
      "Epoch 01117: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4147e-05 - val_loss: 1.4951e-05\n",
      "Epoch 1118/1500\n",
      "\n",
      "Epoch 01118: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5674e-05 - val_loss: 1.7638e-05\n",
      "Epoch 1119/1500\n",
      "\n",
      "Epoch 01119: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7688e-05 - val_loss: 1.8085e-05\n",
      "Epoch 1120/1500\n",
      "\n",
      "Epoch 01120: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5788e-05 - val_loss: 1.5036e-05\n",
      "Epoch 1121/1500\n",
      "\n",
      "Epoch 01121: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6597e-05 - val_loss: 1.6982e-05\n",
      "Epoch 1122/1500\n",
      "\n",
      "Epoch 01122: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6360e-05 - val_loss: 1.3891e-05\n",
      "Epoch 1123/1500\n",
      "\n",
      "Epoch 01123: val_loss improved from 0.00001 to 0.00001, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.3336e-05 - val_loss: 1.2791e-05\n",
      "Epoch 1124/1500\n",
      "\n",
      "Epoch 01124: val_loss improved from 0.00001 to 0.00001, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.3346e-05 - val_loss: 1.2742e-05\n",
      "Epoch 1125/1500\n",
      "\n",
      "Epoch 01125: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3402e-05 - val_loss: 1.4289e-05\n",
      "Epoch 1126/1500\n",
      "\n",
      "Epoch 01126: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5509e-05 - val_loss: 1.4957e-05\n",
      "Epoch 1127/1500\n",
      "\n",
      "Epoch 01127: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5879e-05 - val_loss: 1.7117e-05\n",
      "Epoch 1128/1500\n",
      "\n",
      "Epoch 01128: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6807e-05 - val_loss: 1.6448e-05\n",
      "Epoch 1129/1500\n",
      "\n",
      "Epoch 01129: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5644e-05 - val_loss: 1.5976e-05\n",
      "Epoch 1130/1500\n",
      "\n",
      "Epoch 01130: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5092e-05 - val_loss: 1.4712e-05\n",
      "Epoch 1131/1500\n",
      "\n",
      "Epoch 01131: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4681e-05 - val_loss: 1.6868e-05\n",
      "Epoch 1132/1500\n",
      "\n",
      "Epoch 01132: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7432e-05 - val_loss: 1.4661e-05\n",
      "Epoch 1133/1500\n",
      "\n",
      "Epoch 01133: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4916e-05 - val_loss: 1.4531e-05\n",
      "Epoch 1134/1500\n",
      "\n",
      "Epoch 01134: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4605e-05 - val_loss: 1.4074e-05\n",
      "Epoch 1135/1500\n",
      "\n",
      "Epoch 01135: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5892e-05 - val_loss: 1.6668e-05\n",
      "Epoch 1136/1500\n",
      "\n",
      "Epoch 01136: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7005e-05 - val_loss: 1.6065e-05\n",
      "Epoch 1137/1500\n",
      "\n",
      "Epoch 01137: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4967e-05 - val_loss: 1.3692e-05\n",
      "Epoch 1138/1500\n",
      "\n",
      "Epoch 01138: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3671e-05 - val_loss: 1.6302e-05\n",
      "Epoch 1139/1500\n",
      "\n",
      "Epoch 01139: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4841e-05 - val_loss: 1.7638e-05\n",
      "Epoch 1140/1500\n",
      "\n",
      "Epoch 01140: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.9575e-05 - val_loss: 1.7846e-05\n",
      "Epoch 1141/1500\n",
      "\n",
      "Epoch 01141: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5404e-05 - val_loss: 1.3917e-05\n",
      "Epoch 1142/1500\n",
      "\n",
      "Epoch 01142: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4413e-05 - val_loss: 1.3704e-05\n",
      "Epoch 1143/1500\n",
      "\n",
      "Epoch 01143: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4446e-05 - val_loss: 1.4333e-05\n",
      "Epoch 1144/1500\n",
      "\n",
      "Epoch 01144: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5856e-05 - val_loss: 1.9039e-05\n",
      "Epoch 1145/1500\n",
      "\n",
      "Epoch 01145: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8858e-05 - val_loss: 1.8430e-05\n",
      "Epoch 1146/1500\n",
      "\n",
      "Epoch 01146: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4718e-05 - val_loss: 1.3588e-05\n",
      "Epoch 1147/1500\n",
      "\n",
      "Epoch 01147: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4116e-05 - val_loss: 1.5021e-05\n",
      "Epoch 1148/1500\n",
      "\n",
      "Epoch 01148: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4129e-05 - val_loss: 1.6452e-05\n",
      "Epoch 1149/1500\n",
      "\n",
      "Epoch 01149: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6182e-05 - val_loss: 1.5286e-05\n",
      "Epoch 1150/1500\n",
      "\n",
      "Epoch 01150: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5028e-05 - val_loss: 1.3020e-05\n",
      "Epoch 1151/1500\n",
      "\n",
      "Epoch 01151: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4125e-05 - val_loss: 1.5971e-05\n",
      "Epoch 1152/1500\n",
      "\n",
      "Epoch 01152: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4888e-05 - val_loss: 1.5878e-05\n",
      "Epoch 1153/1500\n",
      "\n",
      "Epoch 01153: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5042e-05 - val_loss: 1.3616e-05\n",
      "Epoch 1154/1500\n",
      "\n",
      "Epoch 01154: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5121e-05 - val_loss: 1.5715e-05\n",
      "Epoch 1155/1500\n",
      "\n",
      "Epoch 01155: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5233e-05 - val_loss: 1.4164e-05\n",
      "Epoch 1156/1500\n",
      "\n",
      "Epoch 01156: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5317e-05 - val_loss: 1.9351e-05\n",
      "Epoch 1157/1500\n",
      "\n",
      "Epoch 01157: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7823e-05 - val_loss: 1.8483e-05\n",
      "Epoch 1158/1500\n",
      "\n",
      "Epoch 01158: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6458e-05 - val_loss: 1.8509e-05\n",
      "Epoch 1159/1500\n",
      "\n",
      "Epoch 01159: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6098e-05 - val_loss: 1.6442e-05\n",
      "Epoch 1160/1500\n",
      "\n",
      "Epoch 01160: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5035e-05 - val_loss: 1.4534e-05\n",
      "Epoch 1161/1500\n",
      "\n",
      "Epoch 01161: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4391e-05 - val_loss: 1.3540e-05\n",
      "Epoch 1162/1500\n",
      "\n",
      "Epoch 01162: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5140e-05 - val_loss: 1.7725e-05\n",
      "Epoch 1163/1500\n",
      "\n",
      "Epoch 01163: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5729e-05 - val_loss: 1.5508e-05\n",
      "Epoch 1164/1500\n",
      "\n",
      "Epoch 01164: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6254e-05 - val_loss: 1.6179e-05\n",
      "Epoch 1165/1500\n",
      "\n",
      "Epoch 01165: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4362e-05 - val_loss: 1.4090e-05\n",
      "Epoch 1166/1500\n",
      "\n",
      "Epoch 01166: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4141e-05 - val_loss: 1.3333e-05\n",
      "Epoch 1167/1500\n",
      "\n",
      "Epoch 01167: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4029e-05 - val_loss: 1.4079e-05\n",
      "Epoch 1168/1500\n",
      "\n",
      "Epoch 01168: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5053e-05 - val_loss: 1.4469e-05\n",
      "Epoch 1169/1500\n",
      "\n",
      "Epoch 01169: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5904e-05 - val_loss: 1.7330e-05\n",
      "Epoch 1170/1500\n",
      "\n",
      "Epoch 01170: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6841e-05 - val_loss: 1.6771e-05\n",
      "Epoch 1171/1500\n",
      "\n",
      "Epoch 01171: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4800e-05 - val_loss: 1.6586e-05\n",
      "Epoch 1172/1500\n",
      "\n",
      "Epoch 01172: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5711e-05 - val_loss: 1.5774e-05\n",
      "Epoch 1173/1500\n",
      "\n",
      "Epoch 01173: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4807e-05 - val_loss: 1.2874e-05\n",
      "Epoch 1174/1500\n",
      "\n",
      "Epoch 01174: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3231e-05 - val_loss: 1.2850e-05\n",
      "Epoch 1175/1500\n",
      "\n",
      "Epoch 01175: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4717e-05 - val_loss: 1.5828e-05\n",
      "Epoch 1176/1500\n",
      "\n",
      "Epoch 01176: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5075e-05 - val_loss: 1.4122e-05\n",
      "Epoch 1177/1500\n",
      "\n",
      "Epoch 01177: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5120e-05 - val_loss: 1.5317e-05\n",
      "Epoch 1178/1500\n",
      "\n",
      "Epoch 01178: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6568e-05 - val_loss: 1.8487e-05\n",
      "Epoch 1179/1500\n",
      "\n",
      "Epoch 01179: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6886e-05 - val_loss: 1.5835e-05\n",
      "Epoch 1180/1500\n",
      "\n",
      "Epoch 01180: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4998e-05 - val_loss: 1.4703e-05\n",
      "Epoch 1181/1500\n",
      "\n",
      "Epoch 01181: val_loss improved from 0.00001 to 0.00001, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.3489e-05 - val_loss: 1.2316e-05\n",
      "Epoch 1182/1500\n",
      "\n",
      "Epoch 01182: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3626e-05 - val_loss: 1.6218e-05\n",
      "Epoch 1183/1500\n",
      "\n",
      "Epoch 01183: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6158e-05 - val_loss: 1.5925e-05\n",
      "Epoch 1184/1500\n",
      "\n",
      "Epoch 01184: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6356e-05 - val_loss: 1.5771e-05\n",
      "Epoch 1185/1500\n",
      "\n",
      "Epoch 01185: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4911e-05 - val_loss: 1.4642e-05\n",
      "Epoch 1186/1500\n",
      "\n",
      "Epoch 01186: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4420e-05 - val_loss: 1.5315e-05\n",
      "Epoch 1187/1500\n",
      "\n",
      "Epoch 01187: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5025e-05 - val_loss: 1.6219e-05\n",
      "Epoch 1188/1500\n",
      "\n",
      "Epoch 01188: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4679e-05 - val_loss: 1.3751e-05\n",
      "Epoch 1189/1500\n",
      "\n",
      "Epoch 01189: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4586e-05 - val_loss: 1.3692e-05\n",
      "Epoch 1190/1500\n",
      "\n",
      "Epoch 01190: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4923e-05 - val_loss: 1.5202e-05\n",
      "Epoch 1191/1500\n",
      "\n",
      "Epoch 01191: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7026e-05 - val_loss: 1.8235e-05\n",
      "Epoch 1192/1500\n",
      "\n",
      "Epoch 01192: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6162e-05 - val_loss: 1.4696e-05\n",
      "Epoch 1193/1500\n",
      "\n",
      "Epoch 01193: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5443e-05 - val_loss: 1.6582e-05\n",
      "Epoch 1194/1500\n",
      "\n",
      "Epoch 01194: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4974e-05 - val_loss: 1.4050e-05\n",
      "Epoch 1195/1500\n",
      "\n",
      "Epoch 01195: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5141e-05 - val_loss: 1.4649e-05\n",
      "Epoch 1196/1500\n",
      "\n",
      "Epoch 01196: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4819e-05 - val_loss: 1.6311e-05\n",
      "Epoch 1197/1500\n",
      "\n",
      "Epoch 01197: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5786e-05 - val_loss: 2.0721e-05\n",
      "Epoch 1198/1500\n",
      "\n",
      "Epoch 01198: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5853e-05 - val_loss: 1.5058e-05\n",
      "Epoch 1199/1500\n",
      "\n",
      "Epoch 01199: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4154e-05 - val_loss: 1.5959e-05\n",
      "Epoch 1200/1500\n",
      "\n",
      "Epoch 01200: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4391e-05 - val_loss: 1.2988e-05\n",
      "Epoch 1201/1500\n",
      "\n",
      "Epoch 01201: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5348e-05 - val_loss: 1.6219e-05\n",
      "Epoch 1202/1500\n",
      "\n",
      "Epoch 01202: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6642e-05 - val_loss: 1.5398e-05\n",
      "Epoch 1203/1500\n",
      "\n",
      "Epoch 01203: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5246e-05 - val_loss: 1.5145e-05\n",
      "Epoch 1204/1500\n",
      "\n",
      "Epoch 01204: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5181e-05 - val_loss: 1.7334e-05\n",
      "Epoch 1205/1500\n",
      "\n",
      "Epoch 01205: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.8773e-05 - val_loss: 1.8518e-05\n",
      "Epoch 1206/1500\n",
      "\n",
      "Epoch 01206: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5581e-05 - val_loss: 1.4533e-05\n",
      "Epoch 1207/1500\n",
      "\n",
      "Epoch 01207: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3934e-05 - val_loss: 1.3578e-05\n",
      "Epoch 1208/1500\n",
      "\n",
      "Epoch 01208: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3787e-05 - val_loss: 1.3473e-05\n",
      "Epoch 1209/1500\n",
      "\n",
      "Epoch 01209: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3694e-05 - val_loss: 1.3791e-05\n",
      "Epoch 1210/1500\n",
      "\n",
      "Epoch 01210: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4694e-05 - val_loss: 1.5530e-05\n",
      "Epoch 1211/1500\n",
      "\n",
      "Epoch 01211: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4334e-05 - val_loss: 1.3993e-05\n",
      "Epoch 1212/1500\n",
      "\n",
      "Epoch 01212: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4801e-05 - val_loss: 1.6481e-05\n",
      "Epoch 1213/1500\n",
      "\n",
      "Epoch 01213: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6839e-05 - val_loss: 1.8146e-05\n",
      "Epoch 1214/1500\n",
      "\n",
      "Epoch 01214: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4258e-05 - val_loss: 1.3019e-05\n",
      "Epoch 1215/1500\n",
      "\n",
      "Epoch 01215: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5191e-05 - val_loss: 1.8108e-05\n",
      "Epoch 1216/1500\n",
      "\n",
      "Epoch 01216: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6028e-05 - val_loss: 1.6531e-05\n",
      "Epoch 1217/1500\n",
      "\n",
      "Epoch 01217: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4940e-05 - val_loss: 1.3738e-05\n",
      "Epoch 1218/1500\n",
      "\n",
      "Epoch 01218: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.2769e-05 - val_loss: 1.4518e-05\n",
      "Epoch 1219/1500\n",
      "\n",
      "Epoch 01219: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4119e-05 - val_loss: 1.4513e-05\n",
      "Epoch 1220/1500\n",
      "\n",
      "Epoch 01220: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4691e-05 - val_loss: 1.7006e-05\n",
      "Epoch 1221/1500\n",
      "\n",
      "Epoch 01221: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6392e-05 - val_loss: 1.4742e-05\n",
      "Epoch 1222/1500\n",
      "\n",
      "Epoch 01222: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5038e-05 - val_loss: 1.4568e-05\n",
      "Epoch 1223/1500\n",
      "\n",
      "Epoch 01223: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4663e-05 - val_loss: 1.4022e-05\n",
      "Epoch 1224/1500\n",
      "\n",
      "Epoch 01224: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3989e-05 - val_loss: 1.3399e-05\n",
      "Epoch 1225/1500\n",
      "\n",
      "Epoch 01225: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4460e-05 - val_loss: 1.6709e-05\n",
      "Epoch 1226/1500\n",
      "\n",
      "Epoch 01226: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4905e-05 - val_loss: 1.4181e-05\n",
      "Epoch 1227/1500\n",
      "\n",
      "Epoch 01227: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4047e-05 - val_loss: 1.3871e-05\n",
      "Epoch 1228/1500\n",
      "\n",
      "Epoch 01228: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4669e-05 - val_loss: 1.4594e-05\n",
      "Epoch 1229/1500\n",
      "\n",
      "Epoch 01229: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6928e-05 - val_loss: 2.3664e-05\n",
      "Epoch 1230/1500\n",
      "\n",
      "Epoch 01230: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6160e-05 - val_loss: 1.3578e-05\n",
      "Epoch 1231/1500\n",
      "\n",
      "Epoch 01231: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3599e-05 - val_loss: 1.4592e-05\n",
      "Epoch 1232/1500\n",
      "\n",
      "Epoch 01232: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4886e-05 - val_loss: 1.2942e-05\n",
      "Epoch 1233/1500\n",
      "\n",
      "Epoch 01233: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3902e-05 - val_loss: 1.4854e-05\n",
      "Epoch 1234/1500\n",
      "\n",
      "Epoch 01234: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4250e-05 - val_loss: 1.6559e-05\n",
      "Epoch 1235/1500\n",
      "\n",
      "Epoch 01235: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5656e-05 - val_loss: 1.4991e-05\n",
      "Epoch 1236/1500\n",
      "\n",
      "Epoch 01236: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4013e-05 - val_loss: 1.5031e-05\n",
      "Epoch 1237/1500\n",
      "\n",
      "Epoch 01237: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5876e-05 - val_loss: 1.9429e-05\n",
      "Epoch 1238/1500\n",
      "\n",
      "Epoch 01238: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 2.1896e-05 - val_loss: 1.5798e-05\n",
      "Epoch 1239/1500\n",
      "\n",
      "Epoch 01239: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3169e-05 - val_loss: 1.2322e-05\n",
      "Epoch 1240/1500\n",
      "\n",
      "Epoch 01240: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.2774e-05 - val_loss: 1.3017e-05\n",
      "Epoch 1241/1500\n",
      "\n",
      "Epoch 01241: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.2782e-05 - val_loss: 1.2835e-05\n",
      "Epoch 1242/1500\n",
      "\n",
      "Epoch 01242: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4188e-05 - val_loss: 1.3904e-05\n",
      "Epoch 1243/1500\n",
      "\n",
      "Epoch 01243: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4468e-05 - val_loss: 1.8127e-05\n",
      "Epoch 1244/1500\n",
      "\n",
      "Epoch 01244: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5940e-05 - val_loss: 1.4675e-05\n",
      "Epoch 1245/1500\n",
      "\n",
      "Epoch 01245: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4143e-05 - val_loss: 1.4076e-05\n",
      "Epoch 1246/1500\n",
      "\n",
      "Epoch 01246: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3978e-05 - val_loss: 1.3347e-05\n",
      "Epoch 1247/1500\n",
      "\n",
      "Epoch 01247: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3878e-05 - val_loss: 1.3453e-05\n",
      "Epoch 1248/1500\n",
      "\n",
      "Epoch 01248: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3981e-05 - val_loss: 1.6568e-05\n",
      "Epoch 1249/1500\n",
      "\n",
      "Epoch 01249: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4987e-05 - val_loss: 1.5196e-05\n",
      "Epoch 1250/1500\n",
      "\n",
      "Epoch 01250: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4306e-05 - val_loss: 1.3890e-05\n",
      "Epoch 1251/1500\n",
      "\n",
      "Epoch 01251: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3977e-05 - val_loss: 1.4057e-05\n",
      "Epoch 1252/1500\n",
      "\n",
      "Epoch 01252: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5389e-05 - val_loss: 1.6560e-05\n",
      "Epoch 1253/1500\n",
      "\n",
      "Epoch 01253: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6698e-05 - val_loss: 1.6649e-05\n",
      "Epoch 1254/1500\n",
      "\n",
      "Epoch 01254: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5678e-05 - val_loss: 1.6343e-05\n",
      "Epoch 1255/1500\n",
      "\n",
      "Epoch 01255: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4968e-05 - val_loss: 1.5003e-05\n",
      "Epoch 1256/1500\n",
      "\n",
      "Epoch 01256: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5357e-05 - val_loss: 1.8239e-05\n",
      "Epoch 1257/1500\n",
      "\n",
      "Epoch 01257: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4864e-05 - val_loss: 1.3561e-05\n",
      "Epoch 1258/1500\n",
      "\n",
      "Epoch 01258: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4100e-05 - val_loss: 1.3902e-05\n",
      "Epoch 1259/1500\n",
      "\n",
      "Epoch 01259: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4646e-05 - val_loss: 1.3868e-05\n",
      "Epoch 1260/1500\n",
      "\n",
      "Epoch 01260: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4338e-05 - val_loss: 1.4176e-05\n",
      "Epoch 1261/1500\n",
      "\n",
      "Epoch 01261: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5498e-05 - val_loss: 1.7052e-05\n",
      "Epoch 1262/1500\n",
      "\n",
      "Epoch 01262: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5800e-05 - val_loss: 1.5547e-05\n",
      "Epoch 1263/1500\n",
      "\n",
      "Epoch 01263: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4342e-05 - val_loss: 1.3570e-05\n",
      "Epoch 1264/1500\n",
      "\n",
      "Epoch 01264: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4466e-05 - val_loss: 1.4636e-05\n",
      "Epoch 1265/1500\n",
      "\n",
      "Epoch 01265: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4273e-05 - val_loss: 1.5566e-05\n",
      "Epoch 1266/1500\n",
      "\n",
      "Epoch 01266: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4590e-05 - val_loss: 1.5195e-05\n",
      "Epoch 1267/1500\n",
      "\n",
      "Epoch 01267: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3589e-05 - val_loss: 1.4444e-05\n",
      "Epoch 1268/1500\n",
      "\n",
      "Epoch 01268: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3906e-05 - val_loss: 1.5017e-05\n",
      "Epoch 1269/1500\n",
      "\n",
      "Epoch 01269: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5682e-05 - val_loss: 1.4333e-05\n",
      "Epoch 1270/1500\n",
      "\n",
      "Epoch 01270: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5208e-05 - val_loss: 1.5944e-05\n",
      "Epoch 1271/1500\n",
      "\n",
      "Epoch 01271: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5316e-05 - val_loss: 1.6220e-05\n",
      "Epoch 1272/1500\n",
      "\n",
      "Epoch 01272: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3869e-05 - val_loss: 1.3262e-05\n",
      "Epoch 1273/1500\n",
      "\n",
      "Epoch 01273: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4782e-05 - val_loss: 1.5287e-05\n",
      "Epoch 1274/1500\n",
      "\n",
      "Epoch 01274: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4740e-05 - val_loss: 1.3541e-05\n",
      "Epoch 1275/1500\n",
      "\n",
      "Epoch 01275: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3277e-05 - val_loss: 1.2993e-05\n",
      "Epoch 1276/1500\n",
      "\n",
      "Epoch 01276: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4024e-05 - val_loss: 1.4286e-05\n",
      "Epoch 1277/1500\n",
      "\n",
      "Epoch 01277: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4531e-05 - val_loss: 1.5502e-05\n",
      "Epoch 1278/1500\n",
      "\n",
      "Epoch 01278: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5239e-05 - val_loss: 1.5458e-05\n",
      "Epoch 1279/1500\n",
      "\n",
      "Epoch 01279: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5069e-05 - val_loss: 1.6540e-05\n",
      "Epoch 1280/1500\n",
      "\n",
      "Epoch 01280: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4418e-05 - val_loss: 1.5631e-05\n",
      "Epoch 1281/1500\n",
      "\n",
      "Epoch 01281: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4609e-05 - val_loss: 1.5033e-05\n",
      "Epoch 1282/1500\n",
      "\n",
      "Epoch 01282: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4633e-05 - val_loss: 1.5656e-05\n",
      "Epoch 1283/1500\n",
      "\n",
      "Epoch 01283: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4479e-05 - val_loss: 1.5406e-05\n",
      "Epoch 1284/1500\n",
      "\n",
      "Epoch 01284: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4607e-05 - val_loss: 1.2900e-05\n",
      "Epoch 1285/1500\n",
      "\n",
      "Epoch 01285: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3594e-05 - val_loss: 1.5942e-05\n",
      "Epoch 1286/1500\n",
      "\n",
      "Epoch 01286: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4074e-05 - val_loss: 1.4252e-05\n",
      "Epoch 1287/1500\n",
      "\n",
      "Epoch 01287: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4118e-05 - val_loss: 1.6933e-05\n",
      "Epoch 1288/1500\n",
      "\n",
      "Epoch 01288: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7232e-05 - val_loss: 1.6629e-05\n",
      "Epoch 1289/1500\n",
      "\n",
      "Epoch 01289: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4619e-05 - val_loss: 1.3531e-05\n",
      "Epoch 1290/1500\n",
      "\n",
      "Epoch 01290: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3756e-05 - val_loss: 1.3820e-05\n",
      "Epoch 1291/1500\n",
      "\n",
      "Epoch 01291: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3709e-05 - val_loss: 1.4331e-05\n",
      "Epoch 1292/1500\n",
      "\n",
      "Epoch 01292: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3738e-05 - val_loss: 1.3452e-05\n",
      "Epoch 1293/1500\n",
      "\n",
      "Epoch 01293: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4120e-05 - val_loss: 1.4163e-05\n",
      "Epoch 1294/1500\n",
      "\n",
      "Epoch 01294: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3794e-05 - val_loss: 1.5424e-05\n",
      "Epoch 1295/1500\n",
      "\n",
      "Epoch 01295: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6609e-05 - val_loss: 1.9055e-05\n",
      "Epoch 1296/1500\n",
      "\n",
      "Epoch 01296: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6269e-05 - val_loss: 1.4440e-05\n",
      "Epoch 1297/1500\n",
      "\n",
      "Epoch 01297: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3605e-05 - val_loss: 1.4429e-05\n",
      "Epoch 1298/1500\n",
      "\n",
      "Epoch 01298: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3860e-05 - val_loss: 1.4612e-05\n",
      "Epoch 1299/1500\n",
      "\n",
      "Epoch 01299: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3864e-05 - val_loss: 1.5740e-05\n",
      "Epoch 1300/1500\n",
      "\n",
      "Epoch 01300: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5296e-05 - val_loss: 1.3935e-05\n",
      "Epoch 1301/1500\n",
      "\n",
      "Epoch 01301: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3493e-05 - val_loss: 1.2978e-05\n",
      "Epoch 1302/1500\n",
      "\n",
      "Epoch 01302: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5597e-05 - val_loss: 1.6952e-05\n",
      "Epoch 1303/1500\n",
      "\n",
      "Epoch 01303: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5458e-05 - val_loss: 1.3561e-05\n",
      "Epoch 1304/1500\n",
      "\n",
      "Epoch 01304: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4226e-05 - val_loss: 1.4131e-05\n",
      "Epoch 1305/1500\n",
      "\n",
      "Epoch 01305: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3827e-05 - val_loss: 1.5134e-05\n",
      "Epoch 1306/1500\n",
      "\n",
      "Epoch 01306: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4654e-05 - val_loss: 1.4547e-05\n",
      "Epoch 1307/1500\n",
      "\n",
      "Epoch 01307: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4150e-05 - val_loss: 1.4060e-05\n",
      "Epoch 1308/1500\n",
      "\n",
      "Epoch 01308: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4026e-05 - val_loss: 1.4557e-05\n",
      "Epoch 1309/1500\n",
      "\n",
      "Epoch 01309: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4471e-05 - val_loss: 1.4724e-05\n",
      "Epoch 1310/1500\n",
      "\n",
      "Epoch 01310: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5102e-05 - val_loss: 1.4437e-05\n",
      "Epoch 1311/1500\n",
      "\n",
      "Epoch 01311: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3369e-05 - val_loss: 1.2898e-05\n",
      "Epoch 1312/1500\n",
      "\n",
      "Epoch 01312: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4569e-05 - val_loss: 1.3970e-05\n",
      "Epoch 1313/1500\n",
      "\n",
      "Epoch 01313: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4701e-05 - val_loss: 1.4525e-05\n",
      "Epoch 1314/1500\n",
      "\n",
      "Epoch 01314: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6053e-05 - val_loss: 1.3434e-05\n",
      "Epoch 1315/1500\n",
      "\n",
      "Epoch 01315: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4466e-05 - val_loss: 1.4227e-05\n",
      "Epoch 1316/1500\n",
      "\n",
      "Epoch 01316: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5089e-05 - val_loss: 1.4922e-05\n",
      "Epoch 1317/1500\n",
      "\n",
      "Epoch 01317: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4807e-05 - val_loss: 1.4473e-05\n",
      "Epoch 1318/1500\n",
      "\n",
      "Epoch 01318: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5139e-05 - val_loss: 1.3785e-05\n",
      "Epoch 1319/1500\n",
      "\n",
      "Epoch 01319: val_loss improved from 0.00001 to 0.00001, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.3432e-05 - val_loss: 1.2128e-05\n",
      "Epoch 1320/1500\n",
      "\n",
      "Epoch 01320: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3118e-05 - val_loss: 1.3573e-05\n",
      "Epoch 1321/1500\n",
      "\n",
      "Epoch 01321: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4851e-05 - val_loss: 1.5590e-05\n",
      "Epoch 1322/1500\n",
      "\n",
      "Epoch 01322: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5796e-05 - val_loss: 1.9942e-05\n",
      "Epoch 1323/1500\n",
      "\n",
      "Epoch 01323: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7450e-05 - val_loss: 1.4726e-05\n",
      "Epoch 1324/1500\n",
      "\n",
      "Epoch 01324: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3188e-05 - val_loss: 1.2148e-05\n",
      "Epoch 1325/1500\n",
      "\n",
      "Epoch 01325: val_loss improved from 0.00001 to 0.00001, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.2547e-05 - val_loss: 1.1812e-05\n",
      "Epoch 1326/1500\n",
      "\n",
      "Epoch 01326: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4009e-05 - val_loss: 1.5404e-05\n",
      "Epoch 1327/1500\n",
      "\n",
      "Epoch 01327: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3556e-05 - val_loss: 1.3456e-05\n",
      "Epoch 1328/1500\n",
      "\n",
      "Epoch 01328: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4024e-05 - val_loss: 1.4940e-05\n",
      "Epoch 1329/1500\n",
      "\n",
      "Epoch 01329: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3746e-05 - val_loss: 1.3134e-05\n",
      "Epoch 1330/1500\n",
      "\n",
      "Epoch 01330: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4295e-05 - val_loss: 1.5860e-05\n",
      "Epoch 1331/1500\n",
      "\n",
      "Epoch 01331: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4103e-05 - val_loss: 1.2566e-05\n",
      "Epoch 1332/1500\n",
      "\n",
      "Epoch 01332: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3887e-05 - val_loss: 1.4286e-05\n",
      "Epoch 1333/1500\n",
      "\n",
      "Epoch 01333: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4608e-05 - val_loss: 1.3272e-05\n",
      "Epoch 1334/1500\n",
      "\n",
      "Epoch 01334: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4161e-05 - val_loss: 1.4054e-05\n",
      "Epoch 1335/1500\n",
      "\n",
      "Epoch 01335: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4459e-05 - val_loss: 1.3838e-05\n",
      "Epoch 1336/1500\n",
      "\n",
      "Epoch 01336: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3688e-05 - val_loss: 1.4324e-05\n",
      "Epoch 1337/1500\n",
      "\n",
      "Epoch 01337: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3366e-05 - val_loss: 1.1953e-05\n",
      "Epoch 1338/1500\n",
      "\n",
      "Epoch 01338: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4310e-05 - val_loss: 1.5918e-05\n",
      "Epoch 1339/1500\n",
      "\n",
      "Epoch 01339: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5877e-05 - val_loss: 1.5744e-05\n",
      "Epoch 1340/1500\n",
      "\n",
      "Epoch 01340: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4647e-05 - val_loss: 1.3416e-05\n",
      "Epoch 1341/1500\n",
      "\n",
      "Epoch 01341: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4048e-05 - val_loss: 1.6365e-05\n",
      "Epoch 1342/1500\n",
      "\n",
      "Epoch 01342: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3681e-05 - val_loss: 1.4252e-05\n",
      "Epoch 1343/1500\n",
      "\n",
      "Epoch 01343: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3224e-05 - val_loss: 1.3078e-05\n",
      "Epoch 1344/1500\n",
      "\n",
      "Epoch 01344: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4000e-05 - val_loss: 1.4818e-05\n",
      "Epoch 1345/1500\n",
      "\n",
      "Epoch 01345: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4657e-05 - val_loss: 1.3676e-05\n",
      "Epoch 1346/1500\n",
      "\n",
      "Epoch 01346: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4575e-05 - val_loss: 1.5810e-05\n",
      "Epoch 1347/1500\n",
      "\n",
      "Epoch 01347: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3839e-05 - val_loss: 1.6794e-05\n",
      "Epoch 1348/1500\n",
      "\n",
      "Epoch 01348: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6470e-05 - val_loss: 1.4341e-05\n",
      "Epoch 1349/1500\n",
      "\n",
      "Epoch 01349: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5454e-05 - val_loss: 1.4162e-05\n",
      "Epoch 1350/1500\n",
      "\n",
      "Epoch 01350: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4401e-05 - val_loss: 1.3538e-05\n",
      "Epoch 1351/1500\n",
      "\n",
      "Epoch 01351: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3258e-05 - val_loss: 1.2082e-05\n",
      "Epoch 1352/1500\n",
      "\n",
      "Epoch 01352: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3066e-05 - val_loss: 1.2782e-05\n",
      "Epoch 1353/1500\n",
      "\n",
      "Epoch 01353: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4971e-05 - val_loss: 1.7273e-05\n",
      "Epoch 1354/1500\n",
      "\n",
      "Epoch 01354: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5173e-05 - val_loss: 1.5470e-05\n",
      "Epoch 1355/1500\n",
      "\n",
      "Epoch 01355: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4047e-05 - val_loss: 1.3708e-05\n",
      "Epoch 1356/1500\n",
      "\n",
      "Epoch 01356: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5667e-05 - val_loss: 1.4487e-05\n",
      "Epoch 1357/1500\n",
      "\n",
      "Epoch 01357: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5078e-05 - val_loss: 1.4117e-05\n",
      "Epoch 1358/1500\n",
      "\n",
      "Epoch 01358: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3347e-05 - val_loss: 1.2853e-05\n",
      "Epoch 1359/1500\n",
      "\n",
      "Epoch 01359: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3757e-05 - val_loss: 1.3542e-05\n",
      "Epoch 1360/1500\n",
      "\n",
      "Epoch 01360: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4389e-05 - val_loss: 1.3476e-05\n",
      "Epoch 1361/1500\n",
      "\n",
      "Epoch 01361: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4540e-05 - val_loss: 1.5596e-05\n",
      "Epoch 1362/1500\n",
      "\n",
      "Epoch 01362: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4488e-05 - val_loss: 1.3642e-05\n",
      "Epoch 1363/1500\n",
      "\n",
      "Epoch 01363: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3505e-05 - val_loss: 1.3736e-05\n",
      "Epoch 1364/1500\n",
      "\n",
      "Epoch 01364: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3491e-05 - val_loss: 1.4686e-05\n",
      "Epoch 1365/1500\n",
      "\n",
      "Epoch 01365: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6275e-05 - val_loss: 2.2472e-05\n",
      "Epoch 1366/1500\n",
      "\n",
      "Epoch 01366: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6183e-05 - val_loss: 1.2949e-05\n",
      "Epoch 1367/1500\n",
      "\n",
      "Epoch 01367: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.2932e-05 - val_loss: 1.2394e-05\n",
      "Epoch 1368/1500\n",
      "\n",
      "Epoch 01368: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3448e-05 - val_loss: 1.4596e-05\n",
      "Epoch 1369/1500\n",
      "\n",
      "Epoch 01369: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4308e-05 - val_loss: 1.4390e-05\n",
      "Epoch 1370/1500\n",
      "\n",
      "Epoch 01370: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4471e-05 - val_loss: 1.6199e-05\n",
      "Epoch 1371/1500\n",
      "\n",
      "Epoch 01371: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6526e-05 - val_loss: 1.6661e-05\n",
      "Epoch 1372/1500\n",
      "\n",
      "Epoch 01372: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4044e-05 - val_loss: 1.3147e-05\n",
      "Epoch 1373/1500\n",
      "\n",
      "Epoch 01373: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.2494e-05 - val_loss: 1.4868e-05\n",
      "Epoch 1374/1500\n",
      "\n",
      "Epoch 01374: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4693e-05 - val_loss: 1.5108e-05\n",
      "Epoch 1375/1500\n",
      "\n",
      "Epoch 01375: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4212e-05 - val_loss: 1.5681e-05\n",
      "Epoch 1376/1500\n",
      "\n",
      "Epoch 01376: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3950e-05 - val_loss: 1.4248e-05\n",
      "Epoch 1377/1500\n",
      "\n",
      "Epoch 01377: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4365e-05 - val_loss: 1.5649e-05\n",
      "Epoch 1378/1500\n",
      "\n",
      "Epoch 01378: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4608e-05 - val_loss: 1.3209e-05\n",
      "Epoch 1379/1500\n",
      "\n",
      "Epoch 01379: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3355e-05 - val_loss: 1.3039e-05\n",
      "Epoch 1380/1500\n",
      "\n",
      "Epoch 01380: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3438e-05 - val_loss: 1.3097e-05\n",
      "Epoch 1381/1500\n",
      "\n",
      "Epoch 01381: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.2926e-05 - val_loss: 1.5122e-05\n",
      "Epoch 1382/1500\n",
      "\n",
      "Epoch 01382: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5397e-05 - val_loss: 1.3504e-05\n",
      "Epoch 1383/1500\n",
      "\n",
      "Epoch 01383: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4220e-05 - val_loss: 1.6873e-05\n",
      "Epoch 1384/1500\n",
      "\n",
      "Epoch 01384: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6105e-05 - val_loss: 1.6140e-05\n",
      "Epoch 1385/1500\n",
      "\n",
      "Epoch 01385: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5451e-05 - val_loss: 1.3346e-05\n",
      "Epoch 1386/1500\n",
      "\n",
      "Epoch 01386: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.2979e-05 - val_loss: 1.4857e-05\n",
      "Epoch 1387/1500\n",
      "\n",
      "Epoch 01387: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3513e-05 - val_loss: 1.2176e-05\n",
      "Epoch 1388/1500\n",
      "\n",
      "Epoch 01388: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3008e-05 - val_loss: 1.2377e-05\n",
      "Epoch 1389/1500\n",
      "\n",
      "Epoch 01389: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.2963e-05 - val_loss: 1.2708e-05\n",
      "Epoch 1390/1500\n",
      "\n",
      "Epoch 01390: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3775e-05 - val_loss: 1.5003e-05\n",
      "Epoch 1391/1500\n",
      "\n",
      "Epoch 01391: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7286e-05 - val_loss: 1.5656e-05\n",
      "Epoch 1392/1500\n",
      "\n",
      "Epoch 01392: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5527e-05 - val_loss: 1.3976e-05\n",
      "Epoch 1393/1500\n",
      "\n",
      "Epoch 01393: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3802e-05 - val_loss: 1.5761e-05\n",
      "Epoch 1394/1500\n",
      "\n",
      "Epoch 01394: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4589e-05 - val_loss: 1.4694e-05\n",
      "Epoch 1395/1500\n",
      "\n",
      "Epoch 01395: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3479e-05 - val_loss: 1.2282e-05\n",
      "Epoch 1396/1500\n",
      "\n",
      "Epoch 01396: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.2595e-05 - val_loss: 1.3044e-05\n",
      "Epoch 1397/1500\n",
      "\n",
      "Epoch 01397: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5054e-05 - val_loss: 1.4435e-05\n",
      "Epoch 1398/1500\n",
      "\n",
      "Epoch 01398: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3713e-05 - val_loss: 1.3659e-05\n",
      "Epoch 1399/1500\n",
      "\n",
      "Epoch 01399: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3331e-05 - val_loss: 1.2625e-05\n",
      "Epoch 1400/1500\n",
      "\n",
      "Epoch 01400: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3932e-05 - val_loss: 1.2699e-05\n",
      "Epoch 1401/1500\n",
      "\n",
      "Epoch 01401: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3180e-05 - val_loss: 1.3427e-05\n",
      "Epoch 1402/1500\n",
      "\n",
      "Epoch 01402: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3541e-05 - val_loss: 1.3300e-05\n",
      "Epoch 1403/1500\n",
      "\n",
      "Epoch 01403: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.7633e-05 - val_loss: 1.7604e-05\n",
      "Epoch 1404/1500\n",
      "\n",
      "Epoch 01404: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4994e-05 - val_loss: 1.5352e-05\n",
      "Epoch 1405/1500\n",
      "\n",
      "Epoch 01405: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3107e-05 - val_loss: 1.2277e-05\n",
      "Epoch 1406/1500\n",
      "\n",
      "Epoch 01406: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3683e-05 - val_loss: 1.4527e-05\n",
      "Epoch 1407/1500\n",
      "\n",
      "Epoch 01407: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4093e-05 - val_loss: 1.4890e-05\n",
      "Epoch 1408/1500\n",
      "\n",
      "Epoch 01408: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3524e-05 - val_loss: 1.5085e-05\n",
      "Epoch 1409/1500\n",
      "\n",
      "Epoch 01409: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5279e-05 - val_loss: 1.6921e-05\n",
      "Epoch 1410/1500\n",
      "\n",
      "Epoch 01410: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4446e-05 - val_loss: 1.4271e-05\n",
      "Epoch 1411/1500\n",
      "\n",
      "Epoch 01411: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3188e-05 - val_loss: 1.2572e-05\n",
      "Epoch 1412/1500\n",
      "\n",
      "Epoch 01412: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3848e-05 - val_loss: 1.4408e-05\n",
      "Epoch 1413/1500\n",
      "\n",
      "Epoch 01413: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4028e-05 - val_loss: 1.5218e-05\n",
      "Epoch 1414/1500\n",
      "\n",
      "Epoch 01414: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4506e-05 - val_loss: 1.2050e-05\n",
      "Epoch 1415/1500\n",
      "\n",
      "Epoch 01415: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3487e-05 - val_loss: 1.4752e-05\n",
      "Epoch 1416/1500\n",
      "\n",
      "Epoch 01416: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4827e-05 - val_loss: 1.5625e-05\n",
      "Epoch 1417/1500\n",
      "\n",
      "Epoch 01417: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4336e-05 - val_loss: 1.3819e-05\n",
      "Epoch 1418/1500\n",
      "\n",
      "Epoch 01418: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4178e-05 - val_loss: 1.3954e-05\n",
      "Epoch 1419/1500\n",
      "\n",
      "Epoch 01419: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4640e-05 - val_loss: 1.5490e-05\n",
      "Epoch 1420/1500\n",
      "\n",
      "Epoch 01420: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4617e-05 - val_loss: 1.3960e-05\n",
      "Epoch 1421/1500\n",
      "\n",
      "Epoch 01421: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4774e-05 - val_loss: 1.7847e-05\n",
      "Epoch 1422/1500\n",
      "\n",
      "Epoch 01422: val_loss improved from 0.00001 to 0.00001, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.4151e-05 - val_loss: 1.1743e-05\n",
      "Epoch 1423/1500\n",
      "\n",
      "Epoch 01423: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3305e-05 - val_loss: 1.5062e-05\n",
      "Epoch 1424/1500\n",
      "\n",
      "Epoch 01424: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3555e-05 - val_loss: 1.2900e-05\n",
      "Epoch 1425/1500\n",
      "\n",
      "Epoch 01425: val_loss improved from 0.00001 to 0.00001, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.2491e-05 - val_loss: 1.1647e-05\n",
      "Epoch 1426/1500\n",
      "\n",
      "Epoch 01426: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3278e-05 - val_loss: 1.3571e-05\n",
      "Epoch 1427/1500\n",
      "\n",
      "Epoch 01427: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4770e-05 - val_loss: 1.7579e-05\n",
      "Epoch 1428/1500\n",
      "\n",
      "Epoch 01428: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4499e-05 - val_loss: 1.2991e-05\n",
      "Epoch 1429/1500\n",
      "\n",
      "Epoch 01429: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4763e-05 - val_loss: 1.5588e-05\n",
      "Epoch 1430/1500\n",
      "\n",
      "Epoch 01430: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6723e-05 - val_loss: 1.8366e-05\n",
      "Epoch 1431/1500\n",
      "\n",
      "Epoch 01431: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4668e-05 - val_loss: 1.2844e-05\n",
      "Epoch 1432/1500\n",
      "\n",
      "Epoch 01432: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3287e-05 - val_loss: 1.2222e-05\n",
      "Epoch 1433/1500\n",
      "\n",
      "Epoch 01433: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.2911e-05 - val_loss: 1.2902e-05\n",
      "Epoch 1434/1500\n",
      "\n",
      "Epoch 01434: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3540e-05 - val_loss: 1.3802e-05\n",
      "Epoch 1435/1500\n",
      "\n",
      "Epoch 01435: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4871e-05 - val_loss: 1.6013e-05\n",
      "Epoch 1436/1500\n",
      "\n",
      "Epoch 01436: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6375e-05 - val_loss: 1.4637e-05\n",
      "Epoch 1437/1500\n",
      "\n",
      "Epoch 01437: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3673e-05 - val_loss: 1.2951e-05\n",
      "Epoch 1438/1500\n",
      "\n",
      "Epoch 01438: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3114e-05 - val_loss: 1.2196e-05\n",
      "Epoch 1439/1500\n",
      "\n",
      "Epoch 01439: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.2870e-05 - val_loss: 1.3112e-05\n",
      "Epoch 1440/1500\n",
      "\n",
      "Epoch 01440: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3928e-05 - val_loss: 1.7734e-05\n",
      "Epoch 1441/1500\n",
      "\n",
      "Epoch 01441: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3486e-05 - val_loss: 1.4536e-05\n",
      "Epoch 1442/1500\n",
      "\n",
      "Epoch 01442: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4742e-05 - val_loss: 1.5618e-05\n",
      "Epoch 1443/1500\n",
      "\n",
      "Epoch 01443: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4426e-05 - val_loss: 1.5717e-05\n",
      "Epoch 1444/1500\n",
      "\n",
      "Epoch 01444: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3762e-05 - val_loss: 1.3929e-05\n",
      "Epoch 1445/1500\n",
      "\n",
      "Epoch 01445: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3602e-05 - val_loss: 1.3422e-05\n",
      "Epoch 1446/1500\n",
      "\n",
      "Epoch 01446: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3621e-05 - val_loss: 1.3496e-05\n",
      "Epoch 1447/1500\n",
      "\n",
      "Epoch 01447: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.2956e-05 - val_loss: 1.3081e-05\n",
      "Epoch 1448/1500\n",
      "\n",
      "Epoch 01448: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3908e-05 - val_loss: 1.5437e-05\n",
      "Epoch 1449/1500\n",
      "\n",
      "Epoch 01449: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4932e-05 - val_loss: 1.5895e-05\n",
      "Epoch 1450/1500\n",
      "\n",
      "Epoch 01450: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5188e-05 - val_loss: 1.4245e-05\n",
      "Epoch 1451/1500\n",
      "\n",
      "Epoch 01451: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4096e-05 - val_loss: 1.4929e-05\n",
      "Epoch 1452/1500\n",
      "\n",
      "Epoch 01452: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3869e-05 - val_loss: 1.4761e-05\n",
      "Epoch 1453/1500\n",
      "\n",
      "Epoch 01453: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5552e-05 - val_loss: 1.6047e-05\n",
      "Epoch 1454/1500\n",
      "\n",
      "Epoch 01454: val_loss improved from 0.00001 to 0.00001, saving model to ./Liqour_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\n",
      "34213/34213 - 0s - loss: 1.3388e-05 - val_loss: 1.1453e-05\n",
      "Epoch 1455/1500\n",
      "\n",
      "Epoch 01455: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3319e-05 - val_loss: 1.5511e-05\n",
      "Epoch 1456/1500\n",
      "\n",
      "Epoch 01456: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4934e-05 - val_loss: 1.4404e-05\n",
      "Epoch 1457/1500\n",
      "\n",
      "Epoch 01457: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3635e-05 - val_loss: 1.2038e-05\n",
      "Epoch 1458/1500\n",
      "\n",
      "Epoch 01458: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3346e-05 - val_loss: 1.6358e-05\n",
      "Epoch 1459/1500\n",
      "\n",
      "Epoch 01459: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4453e-05 - val_loss: 1.2297e-05\n",
      "Epoch 1460/1500\n",
      "\n",
      "Epoch 01460: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.2479e-05 - val_loss: 1.1920e-05\n",
      "Epoch 1461/1500\n",
      "\n",
      "Epoch 01461: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3729e-05 - val_loss: 1.5580e-05\n",
      "Epoch 1462/1500\n",
      "\n",
      "Epoch 01462: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4831e-05 - val_loss: 1.5582e-05\n",
      "Epoch 1463/1500\n",
      "\n",
      "Epoch 01463: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4472e-05 - val_loss: 1.3084e-05\n",
      "Epoch 1464/1500\n",
      "\n",
      "Epoch 01464: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3975e-05 - val_loss: 1.3830e-05\n",
      "Epoch 1465/1500\n",
      "\n",
      "Epoch 01465: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.2671e-05 - val_loss: 1.2584e-05\n",
      "Epoch 1466/1500\n",
      "\n",
      "Epoch 01466: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.2613e-05 - val_loss: 1.1942e-05\n",
      "Epoch 1467/1500\n",
      "\n",
      "Epoch 01467: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.2716e-05 - val_loss: 1.2543e-05\n",
      "Epoch 1468/1500\n",
      "\n",
      "Epoch 01468: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3100e-05 - val_loss: 1.2871e-05\n",
      "Epoch 1469/1500\n",
      "\n",
      "Epoch 01469: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3857e-05 - val_loss: 1.5002e-05\n",
      "Epoch 1470/1500\n",
      "\n",
      "Epoch 01470: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5095e-05 - val_loss: 1.5358e-05\n",
      "Epoch 1471/1500\n",
      "\n",
      "Epoch 01471: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4955e-05 - val_loss: 1.4930e-05\n",
      "Epoch 1472/1500\n",
      "\n",
      "Epoch 01472: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4161e-05 - val_loss: 1.3061e-05\n",
      "Epoch 1473/1500\n",
      "\n",
      "Epoch 01473: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3692e-05 - val_loss: 1.2488e-05\n",
      "Epoch 1474/1500\n",
      "\n",
      "Epoch 01474: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3745e-05 - val_loss: 1.2292e-05\n",
      "Epoch 1475/1500\n",
      "\n",
      "Epoch 01475: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.2407e-05 - val_loss: 1.3564e-05\n",
      "Epoch 1476/1500\n",
      "\n",
      "Epoch 01476: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3710e-05 - val_loss: 1.4286e-05\n",
      "Epoch 1477/1500\n",
      "\n",
      "Epoch 01477: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4271e-05 - val_loss: 1.2515e-05\n",
      "Epoch 1478/1500\n",
      "\n",
      "Epoch 01478: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.2716e-05 - val_loss: 1.2576e-05\n",
      "Epoch 1479/1500\n",
      "\n",
      "Epoch 01479: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3878e-05 - val_loss: 1.3949e-05\n",
      "Epoch 1480/1500\n",
      "\n",
      "Epoch 01480: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.6790e-05 - val_loss: 1.4974e-05\n",
      "Epoch 1481/1500\n",
      "\n",
      "Epoch 01481: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3528e-05 - val_loss: 1.2645e-05\n",
      "Epoch 1482/1500\n",
      "\n",
      "Epoch 01482: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.2651e-05 - val_loss: 1.6597e-05\n",
      "Epoch 1483/1500\n",
      "\n",
      "Epoch 01483: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4638e-05 - val_loss: 1.5367e-05\n",
      "Epoch 1484/1500\n",
      "\n",
      "Epoch 01484: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5240e-05 - val_loss: 1.8338e-05\n",
      "Epoch 1485/1500\n",
      "\n",
      "Epoch 01485: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3762e-05 - val_loss: 1.2470e-05\n",
      "Epoch 1486/1500\n",
      "\n",
      "Epoch 01486: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.2373e-05 - val_loss: 1.3216e-05\n",
      "Epoch 1487/1500\n",
      "\n",
      "Epoch 01487: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4216e-05 - val_loss: 1.2346e-05\n",
      "Epoch 1488/1500\n",
      "\n",
      "Epoch 01488: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4193e-05 - val_loss: 1.5393e-05\n",
      "Epoch 1489/1500\n",
      "\n",
      "Epoch 01489: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3968e-05 - val_loss: 1.2717e-05\n",
      "Epoch 1490/1500\n",
      "\n",
      "Epoch 01490: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3655e-05 - val_loss: 1.3354e-05\n",
      "Epoch 1491/1500\n",
      "\n",
      "Epoch 01491: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4906e-05 - val_loss: 1.4467e-05\n",
      "Epoch 1492/1500\n",
      "\n",
      "Epoch 01492: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3253e-05 - val_loss: 1.2868e-05\n",
      "Epoch 1493/1500\n",
      "\n",
      "Epoch 01493: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3313e-05 - val_loss: 1.3437e-05\n",
      "Epoch 1494/1500\n",
      "\n",
      "Epoch 01494: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.4015e-05 - val_loss: 1.3027e-05\n",
      "Epoch 1495/1500\n",
      "\n",
      "Epoch 01495: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3663e-05 - val_loss: 1.4447e-05\n",
      "Epoch 1496/1500\n",
      "\n",
      "Epoch 01496: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3416e-05 - val_loss: 1.2233e-05\n",
      "Epoch 1497/1500\n",
      "\n",
      "Epoch 01497: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3305e-05 - val_loss: 1.4560e-05\n",
      "Epoch 1498/1500\n",
      "\n",
      "Epoch 01498: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.5020e-05 - val_loss: 1.5552e-05\n",
      "Epoch 1499/1500\n",
      "\n",
      "Epoch 01499: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3868e-05 - val_loss: 1.3072e-05\n",
      "Epoch 1500/1500\n",
      "\n",
      "Epoch 01500: val_loss did not improve from 0.00001\n",
      "34213/34213 - 0s - loss: 1.3652e-05 - val_loss: 1.2729e-05\n"
     ]
    }
   ],
   "source": [
    "# Setting parameters for model and begin training\n",
    "epoch = 1500\n",
    "batch = 256\n",
    "decay = 0 \n",
    "learning_rate = 0.003\n",
    "neurons = 17\n",
    "dropout = 0\n",
    "output_neurons = 1\n",
    "version = 0\n",
    "# Name of model file to be saved\n",
    "file_path = \"./Liquor_ckpt\" \\\n",
    "            + \"_V\" + str(version) \\\n",
    "            + \"_L1_\" + str(neurons) \\\n",
    "            + \"_Dro1_\" + str(dropout) \\\n",
    "            + \"_De1_\" + str(output_neurons) \\\n",
    "            + \"a.hdf5\"\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(neurons, input_shape=(1, train_x.shape[2]), return_sequences=False, name='L1'))\n",
    "# model.add(Dropout(dropout,  name='Dro1'))  # We could add dropout in layers if model overfits.\n",
    "model.add(Dense(output_neurons, name='De1'))\n",
    "# model.add(Activation('relu')) # We could add activation function for better learning of model\n",
    "\n",
    "adam = optimizers.Adam(lr=learning_rate, decay=decay)\n",
    "model.compile(loss='mean_squared_error', optimizer=adam) \n",
    "\n",
    "# Using checkpoint to save only 1 model that fits best on validation data.\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='auto', period=1)\n",
    "history = model.fit(train_x, y_train, batch_size=batch, epochs=epoch, verbose=2, validation_data=(val_x, y_val), callbacks=[checkpoint])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD4CAYAAADo30HgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deZgc9X3n8fe37+m5L50jWQPIGOFD2LIMsZ+sHWIj8MbYMSYisZds2AfvLnjxJvEiEofEfsKzkMNOsgs4OJCwPhAEe4PWEcvttR3bCEFkEEJCgxBodI5Go9HcPd393T+6RvQMPZqWNDM1qD+v55lHVb/6VdW3Cno+U0dXmbsjIiIyJhJ2ASIiMrcoGEREZBwFg4iIjKNgEBGRcRQMIiIyTizsAqZDS0uLL1u2LOwyRETeUp599tnD7t46sf2MCIZly5axefPmsMsQEXlLMbPXSrXrVJKIiIyjYBARkXEUDCIiMs4ZcY1BRORkjY6O0tnZyfDwcNilzLhUKkVbWxvxeLys/goGEalInZ2d1NbWsmzZMsws7HJmjLvT3d1NZ2cn7e3tZc2jU0kiUpGGh4dpbm4+o0MBwMxobm4+qSMjBYOIVKwzPRTGnOx2VnQwPPHSQe784SthlyEiMqdUdDA8teMQ3/zxrrDLEJEKdfToUe64446Tnu+yyy7j6NGjM1BRQUUHg2HoRUUiEpbJgiGbzZ5wvo0bN9LQ0DBTZVX2XUlmoFgQkbCsW7eOV155hZUrVxKPx0mlUjQ2NrJ9+3ZefvllPvnJT7Jnzx6Gh4e54YYbuPbaa4E3HgPU39/PpZdeyoc+9CF++tOfsnjxYh566CGqqqpOq66KDoaIGfm8okGk0n3l/7zItn3HpnWZKxbV8ce/dv4J+9x6661s3bqVLVu28MMf/pCPf/zjbN269fhtpffccw9NTU0MDQ3x/ve/n09/+tM0NzePW8bOnTu57777+OY3v8mVV17J9773PT772c+eVu1lnUoyszVmtsPMOsxsXYnpSTO7P5j+tJktK5p2U9C+w8wuCdpSZrbJzH5hZi+a2VeK+rcHy+gIlpk4rS2cgmJBROaK1atXj/uuwd/8zd/wnve8hwsvvJA9e/awc+fON83T3t7OypUrAXjf+97H7t27T7uOKY8YzCwK3A58FOgEnjGzDe6+rajbNUCPu59jZmuB24DfMLMVwFrgfGAR8LiZvR0YAX7F3fvNLA78xMwedvefB/N+3d3Xm9k3gmXfedpbWnLbUDKIyJR/2c+W6urq48M//OEPefzxx/nZz35GOp3mwx/+cMnvIiSTyePD0WiUoaGh066jnCOG1UCHu+9y9wywHrh8Qp/LgXuD4QeBi61w4+zlwHp3H3H3V4EOYLUX9Af948GPB/P8SrAMgmV+8hS3bUqGKRdEJDS1tbX09fWVnNbb20tjYyPpdJrt27fz85//fNbqKucaw2JgT9F4J/CByfq4e9bMeoHmoP3nE+ZdDMePRJ4FzgFud/enzawFOOru2Yn9JzKza4FrAZYuXVrGZpRaBrorSURC09zczAc/+EHe+c53UlVVxfz5849PW7NmDd/4xjc477zzOPfcc7nwwgtnra7QLj67ew5YaWYNwP82s3cCB05i/ruAuwBWrVp1Sr/ddSZJRML23e9+t2R7Mpnk4YcfLjlt7DpCS0sLW7duPd7++7//+9NSUzmnkvYCS4rG24K2kn3MLAbUA93lzOvuR4GngDXBPA3BMiZb17QpHDHM1NJFRN6aygmGZ4Dlwd1CCQoXkzdM6LMBuDoYvgJ40gvnaDYAa4O7ltqB5cAmM2sNjhQwsyoKF7a3B/M8FSyDYJkPnfrmnVjEDNcxg4jIOFOeSgquGVwPPAJEgXvc/UUz+yqw2d03AHcD3zKzDuAIhfAg6PcAsA3IAte5e87MFgL3BtcZIsAD7v6DYJU3AuvN7E+Bfw2WPTMM9DUGEZHxyrrG4O4bgY0T2m4uGh4GPjPJvLcAt0xoex64YJL+uyjcCTXjDH31WURkosp+VpKhU0kiIhNUdjCgi88iIhNVdjDoTJKIhOhUH7sN8Fd/9VcMDg5Oc0UFlR0Meuy2iIRorgZDhT9dVUcMIhKe4sduf/SjH2XevHk88MADjIyM8KlPfYqvfOUrDAwMcOWVV9LZ2Ukul+OP/uiPOHjwIPv27eMjH/kILS0tPPXUU9NaV0UHA2a6xiAi8PA6OPDC9C5zwbvg0ltP2KX4sduPPvooDz74IJs2bcLd+cQnPsGPfvQjurq6WLRoEf/8z/8MFJ6hVF9fz9e+9jWeeuopWlpaprduKv5UUoFOJ4lI2B599FEeffRRLrjgAt773veyfft2du7cybve9S4ee+wxbrzxRn784x9TX18/47VU9BGDBcng/sawiFSgKf6ynw3uzk033cTnP//5N0177rnn2LhxI1/+8pe5+OKLufnmm0ssYfpU+BFDIQ10vCAiYSh+7PYll1zCPffcQ39/4Y0Ee/fu5dChQ+zbt490Os1nP/tZvvSlL/Hcc8+9ad7ppiMGxk4l6ZBBRGZX8WO3L730Un7zN3+Tiy66CICamhq+/e1v09HRwZe+9CUikQjxeJw77yy8t+zaa69lzZo1LFq0SBefp9PxawyhViEilWziY7dvuOGGceNnn302l1xyyZvm+8IXvsAXvvCFGampok8lRSLBqSQlg4jIcRUdDIlsHwvoJq9kEBE5rqKD4aJX/wf/J/mHYZchIiGplFvVT3Y7KzoY8pYgSVankkQqUCqVoru7+4wPB3enu7ubVCpV9jwVffE5H4kRJ6tHb4tUoLa2Njo7O+nq6gq7lBmXSqVoa2sru3+FB0OCOFlGlAsiFScej9Pe3h52GXNSZZ9KisSJWR7P58IuRURkzqj4YADwXCbkSkRE5g4FA5DPKhhERMZUeDAkADAFg4jIcRUdDG6Fa+86lSQi8oaKDoZ8tHDEgI4YRESOKysYzGyNme0wsw4zW1dietLM7g+mP21my4qm3RS07zCzS4K2JWb2lJltM7MXzeyGov5/YmZ7zWxL8HPZ6W9maXkLLj7nR2dqFSIibzlTfo/BzKLA7cBHgU7gGTPb4O7birpdA/S4+zlmtha4DfgNM1sBrAXOBxYBj5vZ24Es8Hvu/pyZ1QLPmtljRcv8urv/xXRt5GTeOGIYmelViYi8ZZRzxLAa6HD3Xe6eAdYDl0/oczlwbzD8IHCxmVnQvt7dR9z9VaADWO3u+939OQB37wNeAhaf/uacpEiQi7rGICJyXDnBsBjYUzTeyZt/iR/v4+5ZoBdoLmfe4LTTBcDTRc3Xm9nzZnaPmTWWKsrMrjWzzWa2+VS/0p4L7krS7aoiIm8I9eKzmdUA3wO+6O7HguY7gbOBlcB+4C9Lzevud7n7Kndf1draekrr9+B7DKYjBhGR48oJhr3AkqLxtqCtZB8ziwH1QPeJ5jWzOIVQ+I67f3+sg7sfdPecu+eBb1I4lTUjjl9jyOnis4jImHKC4RlguZm1m1mCwsXkDRP6bACuDoavAJ70wrNsNwBrg7uW2oHlwKbg+sPdwEvu/rXiBZnZwqLRTwFbT3ajyuWRsWDQEYOIyJgp70py96yZXQ88AkSBe9z9RTP7KrDZ3TdQ+CX/LTPrAI5QCA+Cfg8A2yjciXSdu+fM7EPA54AXzGxLsKo/cPeNwJ+Z2UoKr2LeDXx+Grd3nLFHYpAdmqlViIi85ZT12O3gF/bGCW03Fw0PA5+ZZN5bgFsmtP0EsEn6f66cmqbDaLwOABvuna1ViojMeRX9zedssnDDkw0dCbkSEZG5o6KDIZKsYsgTMKhgEBEZU9HBUBWP0UMNeQWDiMhxFR0M6USUo16LDXaHXYqIyJxR0cFQlYjS4zXYcE/YpYiIzBmVHQzxKD3UEFMwiIgcV9HBUDiVVEMsczTsUkRE5owKD4YYR6glnjkG+XzY5YiIzAkVHQxVwcXnCHkY1lGDiAhUejDECxefARjSdQYREajwYEjEIvRFagsj+i6DiAhQ4cEAMJpoKAzouwwiIoCCgVxVU2FAz0sSEQEUDFhV8OZQnUoSEQEUDMTTDeSI6IhBRCRQ8cFQX53iGDU6YhARCSgYquL0UKsjBhGRgIKhKk53vlqP3hYRCVR8MDSk4xz1WvIDCgYREVAwFE4leY2+xyAiEqj4YGhIx+mhhsjwEXAPuxwRkdBVfDDUVyUKD9LLZWB0MOxyRERCp2CoKhwxALplVUSEMoPBzNaY2Q4z6zCzdSWmJ83s/mD602a2rGjaTUH7DjO7JGhbYmZPmdk2M3vRzG4o6t9kZo+Z2c7g38bT38zJNaTj7PWWwkjPqzO5KhGRt4Qpg8HMosDtwKXACuAqM1sxods1QI+7nwN8HbgtmHcFsBY4H1gD3BEsLwv8nruvAC4Erita5jrgCXdfDjwRjM+Y+qo42/NLCyMHts7kqkRE3hLKOWJYDXS4+y53zwDrgcsn9LkcuDcYfhC42MwsaF/v7iPu/irQAax29/3u/hyAu/cBLwGLSyzrXuCTp7Zp5YlHI4ymWxmK1OiIQUSE8oJhMbCnaLyTN36Jv6mPu2eBXqC5nHmD004XAE8HTfPdfX8wfACYX6ooM7vWzDab2eaurq4yNmNyb59XywBVMNJ/WssRETkThHrx2cxqgO8BX3T3YxOnu7sDJe8hdfe73H2Vu69qbW09rToW1Kfo8xRk+k5rOSIiZ4JygmEvsKRovC1oK9nHzGJAPdB9onnNLE4hFL7j7t8v6nPQzBYGfRYCh8rdmFM1vy5Jbz6JZwZmelUiInNeOcHwDLDczNrNLEHhYvKGCX02AFcHw1cATwZ/7W8A1gZ3LbUDy4FNwfWHu4GX3P1rJ1jW1cBDJ7tRJ2tebYqefDX5/tM7JSUiciaITdXB3bNmdj3wCBAF7nH3F83sq8Bmd99A4Zf8t8ysAzhCITwI+j0AbKNwJ9J17p4zsw8BnwNeMLMtwar+wN03ArcCD5jZNcBrwJXTucGlzKtLssfnwdFNM70qEZE5b8pgAAh+YW+c0HZz0fAw8JlJ5r0FuGVC208Am6R/N3BxOXVNl3m1KXZTQ2TkGOTzEKn47/2JSAXTb0AK1xgGPYXhkB0KuxwRkVApGICF9VUMkiyMZPS8JBGpbAoGoCoRhUR1YSSj7zKISGVTMATi6YbCwHBvuIWIiIRMwTCmNviCdf/BcOsQEQmZgiEQqRkLhhn/Pp2IyJymYAik65oB8KGekCsREQmXgiFQXVe4xpB/4cGQKxERCZeCIdBcWwVA9MAvQq5ERCRcCoZAc3WC/5d7NyM1S6buLCJyBlMwBJprEuzxVmxUT1gVkcqmYAg01yTpI010VO9kEJHKpmAINFcn6PM00fwojA6HXY6ISGgUDIFUPMpINHgsxsibXiYnIlIxFAzFUvWFf4cVDCJSuRQMRSxVVxjQ85JEpIIpGIrEq4MH6Y0oGESkcikYihwPBp1KEpEKpmAokqptAsB1KklEKpiCoUh1XSMAw/1HQ65ERCQ8CoYidfWN5N0Y7tMTVkWkcikYijTXVNFPisyAgkFEKlcs7ALmkuaaBH2kYVDXGESkcpV1xGBma8xsh5l1mNm6EtOTZnZ/MP1pM1tWNO2moH2HmV1S1H6PmR0ys60TlvUnZrbXzLYEP5ed+uadnObqJH2eJq+LzyJSwaYMBjOLArcDlwIrgKvMbMWEbtcAPe5+DvB14LZg3hXAWuB8YA1wR7A8gH8I2kr5uruvDH42ntwmnbqm6gR9VOl2VRGpaOUcMawGOtx9l7tngPXA5RP6XA7cGww/CFxsZha0r3f3EXd/FegIloe7/wg4Mg3bMG0SsQhDkWqiGT1hVUQqVznBsBjYUzTeGbSV7OPuWaAXaC5z3lKuN7Png9NNjaU6mNm1ZrbZzDZ3dXWVscjyZGK1xLMKBhGpXHPxrqQ7gbOBlcB+4C9LdXL3u9x9lbuvam1tnbaVZ+O1JHN6WY+IVK5ygmEvUPy+y7agrWQfM4sB9UB3mfOO4+4H3T3n7nngmwSnnmZLPllHOt8P7rO5WhGROaOcYHgGWG5m7WaWoHAxecOEPhuAq4PhK4An3d2D9rXBXUvtwHJg04lWZmYLi0Y/BWydrO9MyKaaiZGDIX2XQUQq05TfY3D3rJldDzwCRIF73P1FM/sqsNndNwB3A98ysw4KF5TXBvO+aGYPANuALHCdu+cAzOw+4MNAi5l1An/s7ncDf2ZmKwEHdgOfn84NnkquppBLfmwflm6azVWLiMwJZX3BLbhldOOEtpuLhoeBz0wy7y3ALSXar5qk/+fKqWmmxKvfeF5SVZiFiIiEZC5efA5Vqrrwsp4BPS9JRCqUgmGCVE3hnQyDffr2s4hUJgXDBNU1hfc+jwzquwwiUpkUDBNU1xWCITOox2KISGVSMExQF7ysJzukYBCRyqRgmKC+ppoRj5Eb7g+7FBGRUCgYJkjFIwxShY/oGoOIVCYFwwRmxqBVYaN6XpKIVCYFQwmZSBWW0akkEalMCoYSMtE0sayOGESkMikYSshG08Syg2GXISISCgVDCZlEHdV5XXwWkcqkYCghk2ymIX807DJEREKhYCghX9VCnQ0ymhkJuxQRkVmnYCghUlV4wmpf75GQKxERmX0KhhJi6cITVvsVDCJSgRQMJSSqCw/SG+hTMIhI5VEwlJAseoubiEilUTCUUBU8YTWjYBCRCqRgKKF6LBgGFAwiUnkUDCXU1jcBkNM7GUSkAikYSogHdyXlh/TeZxGpPAqGUmJJRojDiI4YRKTylBUMZrbGzHaYWYeZrSsxPWlm9wfTnzazZUXTbgrad5jZJUXt95jZITPbOmFZTWb2mJntDP5tPPXNO3VDliYxottVRaTyTBkMZhYFbgcuBVYAV5nZigndrgF63P0c4OvAbcG8K4C1wPnAGuCOYHkA/xC0TbQOeMLdlwNPBOOzbm98GUuHd4SxahGRUJVzxLAa6HD3Xe6eAdYDl0/oczlwbzD8IHCxmVnQvt7dR9z9VaAjWB7u/iOg1J/kxcu6F/jkSWzPtDmaXEQqp5f1iEjlKScYFgN7isY7g7aSfdw9C/QCzWXOO9F8d98fDB8A5pfqZGbXmtlmM9vc1dVVxmacnHyihrTrnQwiUnnm9MVnd3fAJ5l2l7uvcvdVra2t075uS9aR9iHI56d92SIic1k5wbAXWFI03ha0lexjZjGgHuguc96JDprZwmBZC4FDZdQ4/VJ1RMwZHtQtqyJSWcoJhmeA5WbWbmYJCheTN0zoswG4Ohi+Angy+Gt/A7A2uGupHVgObJpifcXLuhp4qIwap10sXXj09rGjujNJRCrLlMEQXDO4HngEeAl4wN1fNLOvmtkngm53A81m1gH8LsGdRO7+IvAAsA34v8B17p4DMLP7gJ8B55pZp5ldEyzrVuCjZrYT+NVgfNbF08ETVo/1hLF6EZHQxMrp5O4bgY0T2m4uGh4GPjPJvLcAt5Rov2qS/t3AxeXUNZNSNYWvTwz0dodciYjI7JrTF5/DVNtYuKDdf3T673gSEZnLFAyTaGhZAMDIscMhVyIiMrsUDJOobSgcMYz2KxhEpLIoGCZhqXqyRMkP6K4kEaksCobJmNEfqSMyrGAQkcqiYDiB/ngT6RGdShKRyqJgOIGB1EKasuF88VpEJCwKhhMYrVnEAg5zbHg07FJERGaNguEE4o1LaLAB9uzXdxlEpHIoGE6gqrnwhPAjh/ZM0VNE5MyhYDiBmiAYho7sC7kSEZHZo2A4gdqWRQBkjh4IuRIRkdmjYDiBeN1CAPL9B0OuRERk9igYTiTdTIY40WO6xiAilUPBcCKRCD2JBaQHpnrpnIjImUPBMIWh6iW0ZvczlMmFXYqIyKxQMEzBGpayyA6z63B/2KWIiMwKBcMU0q3LaLJ+du/XM5NEpDIoGKZQv2AZAF2dr4RbiIjILFEwTCHR/DYABg4qGESkMigYptL6DgBi3S+FXIiIyOxQMEwl3URfYh7zBl9hMJMNuxoRkRmnYCjDSNO5vMNeY8eBvrBLERGZcWUFg5mtMbMdZtZhZutKTE+a2f3B9KfNbFnRtJuC9h1mdslUyzSzfzCzV81sS/Cz8vQ28fSlaxo4L7KH/Ts2hV2KiMiMmzIYzCwK3A5cCqwArjKzFRO6XQP0uPs5wNeB24J5VwBrgfOBNcAdZhYtY5lfcveVwc+W09rCaVC18tcBGHnt2ZArERGZeeUcMawGOtx9l7tngPXA5RP6XA7cGww/CFxsZha0r3f3EXd/FegIllfOMucMe/sa8kTgSEfYpYiIzLhygmExUPwUuc6grWQfd88CvUDzCeadapm3mNnzZvZ1M0uWKsrMrjWzzWa2uatrht+wlkjTWX0+bQPbyOV9ZtclIhKyuXjx+SbgHcD7gSbgxlKd3P0ud1/l7qtaW1tnvChvOos2DvLS/mMzvi4RkTCVEwx7gSVF421BW8k+ZhYD6oHuE8w76TLdfb8XjAB/T+G0U+ial7yDhXaEn25/PexSRERmVDnB8Ayw3MzazSxB4WLyhgl9NgBXB8NXAE+6uwfta4O7ltqB5cCmEy3TzBYG/xrwSWDr6WzgdKlpfz8Ao8//U8iViIjMrNhUHdw9a2bXA48AUeAed3/RzL4KbHb3DcDdwLfMrAM4QuEXPUG/B4BtQBa4zt1zAKWWGazyO2bWChiwBfiP07e5p+HsiwF425F/oW94lNpUPOSCRERmhhX+sH9rW7VqlW/evHnG15P70wVEs0M8+ukX+di72mZ8fSIiM8nMnnX3VRPb5+LF5znL2v8NAMM/+7uQKxERmTkKhpMQ+fRdAPTvf5kz4UhLRKQUBcPJSNVztOYcrsw/zLPbXg67GhGRGaFgOEnV5/0qMcvT/tCnwi5FRGRGKBhOUvz8XwOgObOXkWwu5GpERKafguFkLfvQ8cF/fHpXiIWIiMwMBcMp8Pf9DgC7/uV7uggtImccBcMpsI//BcPxBt7Z92Me3XYw7HJERKaVguFURKIkVlzGr0d/wsoHP8hoLh92RSIi00bBcIoiH/9LAOb7Ye77yfaQqxERmT4KhlOVSOPv+/cAfOSJT/Dc6z0hFyQiMj0UDKfBLvtzAJZEutj7nesYyuj2VRF561MwnI5oHG58DYBfG/ln/vZ/3EJ3/0jIRYmInB4Fw+mqaoBf/hIAX+z7S268/bsc6hsOuSgRkVOnYJgOv/zfoLEdgL8b+iLfvu16fvrK4ZCLEhE5NQqG6RBLwA1b4NLCNYffjf0jtfdezK0bX9JjM0TkLUfBMJ0+cC385gMAvCuym3WbLuTmP76ROx5/iaHuPSEXJyJSHr3BbSaM9MOjX4Zn/35c8x3z/4QVF32cf/Oe5RReaS0iEp7J3uCmYJhJR3bBT/8nbL57XPPDyTUcPPsKlrYtYfX7VlGTnPLV2yIi007BEKbhXti2gfzjXyEy2PWmyR3xcxmsWUqycRHtF/06iXnLoX4xPHM3JGvh3VeGULSInOkUDHPFkVfhhX8k//M7iQwdKWuWZy7byLyFS1mSHCTSshwiujQkIqdPwTAXucMLD8L2H5A9uhcOvkAsV/53ILakVnO46b1E5p1L2kaxZRcx7/AmFrYtI3nOh7GoTlGJyOROKxjMbA3w10AU+Dt3v3XC9CTwv4D3Ad3Ab7j77mDaTcA1QA74L+7+yImWaWbtwHqgGXgW+Jy7Z05U31s2GCZzeCc0LCW77QcMPf9PVL+ykf6qxdQNvnbKi+zxGl72Ngarl5CKRVg0upuE5dicvIi9Np8L5sdYePApqqLQvfxKjvQNkm94Gwt6t+DLfpn6eYsZJkWu/zD1S84nHRllX3cvNTZCeugA1We9nzwRIiPHsHwW0k0QiY4vwh3MCv/mc4XpJ3MRPpsp3BpcLDda+Aa6iJy0Uw4GM4sCLwMfBTqBZ4Cr3H1bUZ//DLzb3f+jma0FPuXuv2FmK4D7gNXAIuBx4O3BbCWXaWYPAN939/Vm9g3gF+5+54lqPOOC4URyWRgdhN5Ohjd/m0wehvJx9nYdoSHbxVkHHg67QgAyHqXXamnlKPsiC1iUPwDA/sgCFgbDxQ5bE5lIFYtye8e1H400UJfvJULh/9PtyXeTjOQYJc7bh7Yc77creR7VNsz84VfJE2F/sp2D8Tby6Rb2dx1mYUOaVT2FffOL5o9Tne+jP9HKsKVI2Sjzep8n7UN0pc9iefdTALwy72P4yAC13su++NuI1zTTOLib/VXn0Dy4i/bDP+Rwoo14PE5PtIVM3VLSoz10N68i1bebY43nM5iLUuXDtB14jASjvN54ERaJUlWVoifaQnWul4RnyDS/g0xfN0ORaqqzRxlsPJeaTDdVg3sZTDQxcuww9QxQdfgFMrFaqmrrOdS8muZEjr5RwyMxLBpn2GOkcv1EDz5PJr2AgXyceZm9DFa3YQMHaayKkalqJVe9gOjAAWoPbmJw0S+x/+gALWddQLbndfLRKqL1C8n0HSZ2bA+11WmGujtJn7+GRH6IkZefZH/jamoiI2QGjxFb9ks0xzOMvvwYIws/wIhHiGUHiVqemmiOSLKWaP1CPDNAdO8zWP1iPJchm17AUD5KKplgJJuH/kPUtb+PrgOdxCPQe6SLFEMsPGclwwd24PFqYjWtDI1mqUtFiWUHGY2kGe3vJjrSQzSRZnfPMMuWv4uh3ZuoW/puLDtC7l+/Tf/bPsaWgyP80rltVNc10dXZQbpxHh6vwSIRIslakiPdZInQc6yP+niOfFUT6XiMXCzFUBYinqN/19PMP++DZIkSzY2Qy44yNDhIIh5hKNZA/NDzRPsPkGr/AKPxOvL5LHki5N1JV1XT3bWf6poaUqlqPDdKZLALx8iOjpBoPYfRrp1Yw1IyQ33EIlHisSierCebyxKPx+k62Ekzx4jMOw9wcrkcsfiEP5ZOwukEw0XAn7j7JcH4TQDu/t+L+jwS9PmZmcWAA0ArsK6471i/YLY3LRO4FegCFrh7duK6J1NRwXCqcqM4kB3sJRaLcmzHj6kiQ282xuDIKM25Q/QNjTCQjZIe2sdwbxe1/a+QiEZI9u+hP7mQ7lQbC3qfZ583cd7I8+ypOo/98SUsG0AiMQ4AAAnZSURBVNpGf7yJoVg9y/ueIRNJ8lrd+2kafIUIzhFrpDHXxYLRTvbFl7Jo9PWw94bIW0bejYg5OTei5uPaBjxJx4fv4D0fueKUlj1ZMJRzEnoxUPztrE7gA5P1CX6h91I4FbQY+PmEeRcHw6WW2Qwcdfdsif5yOqJxDIjXtgBQv/ITQCG9x9ScYPYU0BIM1wf/Lgl+AOYV9U0A5xeNzy8aXjRVnWOnm+CN000TpwPkMmBRiETx0SEslsSHerDqFvL9h4kk0rhFwAwfGWQ0nyNR3chQJosNdJGNxIknq0nGY/QPZ8iNDJHwYUbzEPUssVQ1g0f2kbQsueqF9GWceH4QevcSaVpGdmSI+P7NDKfmQbyKKhtlMFpH5sgeEs1vI5o5xmi8hhxx4pmjWCRC7shr0Hw2Q5FaEkc7iNUtIDvcR3xgH8MkSSXjDI1CPpokN9RHOm4coZZUJM9QZpRU1MhEUqSGD8GxvVC3mMjgYWzeOxg4sg+LV2Oj/cSrarHsELGju+kfhVQyxWDNUpoP/pTB6jbykTj5lnNJZY7S199HQ38Hwx6nv+5s6pIR8nu3MNy8gmRdC1VHdxIdOcqxXIJcoo7BqkVUH9xMLtVAxpLM63mOzsYPUJ1KUD3wOkOjeXpjLSR8hJbsAarI8HLtamoiGTKZES7Yt55di36NmI+SHDnM7uQ7qI3lSFqWyJEO+lMLicSSRDNHOTKQpTV3iEi6gSobJRtNYfks/VWLaRrowHNZDtWcS8vQbmKWI5eHgVHIppqoH3qdgeqlxHyUef3b2V11PvGenbQm8+yvPZ+ayCiJkW4GEi1kLEVVro+e5GLqR/YxODRMMh4lks/SnNnLqw0XYThxRmk59FNqIhl6qs8h73kiZgxHa0lkevCqRiL5URKDB+lLzicfTRIfPkKSEfrTSxkczdPc/zJD8UYS8SjZWA3vPLgBN6On6m28Xv0uFgx1cLDu3SSzvZxz+AmOJBbTVXsemVgt2cwITSN7mDfyGq81XoTnc1RljjDv7aun+lSdtLfs1Ukzuxa4FmDp0qUhVyPTpviaw8RQKJ4eS77RlEgX/q0uRFekpvDv2JIslmSsdzoVhVTbuEXWxlNQW/emVaUaFhwffmPqO97ocNY7x/VvAuDCN9dc0sqyei0rc2nl+e1pWs7vjBs7e4re4z+dX+Y9RWPnnGYly8vsV/zHyZJJe5XWVqJtfom20zGPN/64Kl7fIkr/MdU4zeufqJz7Hvcyfl+2BW0l+wSnkuopXISebN7J2ruBhmAZk60LAHe/y91Xufuq1tbWUl1EROQUlBMMzwDLzazdzBLAWmDDhD4bgKuD4SuAJ71w8WIDsNbMksHdRsuBTZMtM5jnqWAZBMt86NQ3T0RETtaUp5KCawbXA49QuLX0Hnd/0cy+Cmx29w3A3cC3zKwDOELhFz1BvweAbUAWuM7dcwCllhms8kZgvZn9KfCvwbJFRGSW6AtuIiIVarK7kvRsBRERGUfBICIi4ygYRERkHAWDiIiMc0ZcfDazLuBUnzDXAhyexnJmwlyvca7XB6pxOsz1+mDu1zjX6nubu7/pi2BnRDCcDjPbXOqq/Fwy12uc6/WBapwOc70+mPs1zvX6xuhUkoiIjKNgEBGRcRQMcFfYBZRhrtc41+sD1Tgd5np9MPdrnOv1AbrGICIiE+iIQURExlEwiIjIOBUdDGa2xsx2mFmHma0LqYYlZvaUmW0zsxfN7IagvcnMHjOzncG/jUG7mdnfBDU/b2bvnaU6o2b2r2b2g2C83cyeDuq4P3h8OsEj1u8P2p82s2WzVF+DmT1oZtvN7CUzu2gO7sP/Gvw33mpm95lZKuz9aGb3mNkhM9ta1HbS+83Mrg767zSzq0utaxrr+/Pgv/PzZva/zayhaNpNQX07zOySovYZ+6yXqrFo2u+ZmZtZSzA+6/vwlLh7Rf5QeNz3K8BZFN5G+QtgRQh1LATeGwzXAi8DK4A/A9YF7euA24Lhy4CHKbyg7ELg6Vmq83eB7wI/CMYfANYGw98A/lMw/J+BbwTDa4H7Z6m+e4H/EAwngIa5tA8pvKL2VaCqaP/9dtj7Efhl4L3A1qK2k9pvFF5etyv4tzEYbpzB+j4GxILh24rqWxF8jpNAe/D5js70Z71UjUH7EgqvFngNaAlrH57SNoW14rB/gIuAR4rGbwJumgN1PQR8FNgBLAzaFgI7guG/Ba4q6n+83wzW1AY8AfwK8IPgf+rDRR/O4/sy+CBcFAzHgn42w/XVB790bUL7XNqHY+9Fbwr2yw+AS+bCfqTwBtHiX7wntd+Aq4C/LWof12+665sw7VPAd4LhcZ/hsX04G5/1UjUCDwLvAXbzRjCEsg9P9qeSTyWNfVDHdAZtoQlOF1wAPA3Md/f9waQDvPGa2TDq/ivgvwH5YLwZOOru2RI1HK8vmN4b9J9J7UAX8PfB6a6/M7Nq5tA+dPe9wF8ArwP7KeyXZ5lb+3HMye63MD9Lv0PhL3BOUMes12dmlwN73f0XEybNmRpPpJKDYU4xsxrge8AX3f1Y8TQv/AkRyn3FZvZvgUPu/mwY6y9TjMKh/J3ufgEwQOEUyHFh7kOA4Dz95RRCbBFQDawJq55yhb3fTsTM/pDCmyG/E3YtxcwsDfwBcHPYtZyqSg6GvRTOAY5pC9pmnZnFKYTCd9z9+0HzQTNbGExfCBwK2me77g8CnzCz3cB6CqeT/hpoMLOxV8MW13C8vmB6PdA9g/VB4a+rTnd/Ohh/kEJQzJV9CPCrwKvu3uXuo8D3KezbubQfx5zsfpv1/Wlmvw38W+C3gvCaS/WdTeEPgF8En5s24DkzWzCHajyhSg6GZ4DlwV0hCQoX+DbMdhFmZhTea/2Su3+taNIGYOzOhKspXHsYa/93wd0NFwK9RYf9087db3L3NndfRmEfPenuvwU8BVwxSX1jdV8R9J/Rvzjd/QCwx8zODZoupvCe8TmxDwOvAxeaWTr4bz5W45zZj0VOdr89AnzMzBqDI6OPBW0zwszWUDi1+Ql3H5xQ99rgjq52YDmwiVn+rLv7C+4+z92XBZ+bTgo3mBxgjuzDKYV1cWMu/FC4Q+BlCncs/GFINXyIwqH688CW4OcyCueTnwB2Ao8DTUF/A24Pan4BWDWLtX6YN+5KOovCh64D+EcgGbSngvGOYPpZs1TbSmBzsB//icKdHXNqHwJfAbYDW4FvUbh7JtT9CNxH4ZrHKIVfYNecyn6jcK6/I/j59zNcXweF8/Fjn5dvFPX/w6C+HcClRe0z9lkvVeOE6bt54+LzrO/DU/nRIzFERGScSj6VJCIiJSgYRERkHAWDiIiMo2AQEZFxFAwiIjKOgkFERMZRMIiIyDj/H70o73HeCV/TAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x640 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot peoch loss graph\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.figure(figsize=(10,8),dpi=80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABQsAAAFPCAYAAADqRZiqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAMTQAADE0B0s6tTgAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzda5BjZ33v+++ju9SSejRXDx7G5uYUYOzhkgSoEDih6iRQ+3hTZO9AEiAkVBznxa5KUZVDNgGS4pALOSnn5EK2IdtsYudGcsh22CRQOyQkOWxCLgQnEI9tTDy+jO0Ze1rdWuq1tJak9ZwXz1qSerpnWt3Tao2k36fKNd1Ll37GrZGe9Vv/5/8Yay0iIiIiIiIiIiIimWkPQERERERERERERK4OCgtFREREREREREQEUFgoIiIiIiIiIiIiCYWFIiIiIiIiIiIiAigsFBERERERERERkYTCQhEREREREREREQEgN+0B7FSxWLRHjhyZ9jBERERERERERERm0tmzZyNrbXGr22YuLDxy5AiPP/74tIchIiIiIiIiIiIyk4wxT1/qNi1DFhEREREREREREWAHYaEx5owx5gFjzL3Jf29Jjr/AGPMlY8yDxph/MMa8eOQxu7pNRERERERERERE9t9OKwvfYq09lfz3yeTYR4GPWWtvAD4MfGLk/ru9TURERERERERERPaZsdaOd0djzgBvstbeO3LsKPAQcNBa2zPGGOBJ4DuA1m5us9Y+dLlxnDhxwqpnoYiIiIiIiIjIbLLWDv6TyTDGkMlcukbQGHPWWntiq9t2usHJXUmw9/fATwHPBp601vYArLXWGPMocBJY2+VtG8JCY8y7gXen3y8vL+9wyCIiIiIiIiIiMm1xHHP+/HlWV1cVFO6DfD7PyZMnKRQKO3rcTsLC77TWPmqMyQMfAn4beP+OftouWGtvB25Pvz9x4oReTSIiIiIiIiIiM+aRRx4hk8lw/fXXk8/npz2cuWat5cKFCzz66KM8//nP39Fjxw4LrbWPJn92jTH/D/Ag8Bhw3BiTG1lOfBJ4FLfUeDe3iYiIiIiIiIjIHInjmE6nwwte8AJyuZ0udJXdOHToECsrK8RxfNklyRcb657GmCVjzIGRQ98PfNVaex74J+BtyfHvBR631j6029vGHrmIiIiIiIiIiMyEdNmxqxeT/ZD+v97pku9xo9xjwKeMMVnAAP8GvCO57ceATxhj3ourGPzhkcft9jYRERERERERERHZZ2NVFlpr/81a+1Jr7U3W2pdYa/+9tfZMctsD1tpXWWtvsNa+wlr7tZHH7eo2ERERERERERGR/eB5HtVqlXe9613b3vfMmTPccccde/Jzr7/+eu699949ea69NP6CZRERERERERERkTnzyU9+kpe//OX88R//Me12+7L33cuw8GqljpIiImM42zrLWz/1Vu560108p/GcaQ9HRERERERk9t1yC3zzm5N57uc9Dz796bHueuedd/L+97+fj370o3zyk58cVBh++MMf5nd+53fIZDKUy2X+8i//kttuu41HHnmEU6dOcfLkST796U9z/fXXc88993Dq1CkAXvGKV/DLv/zLvO51r+P222/n93//9+l2u+TzeX7t136NV73qVZP5O+8RhYUiImP40mNf4ouPfpEvPvpFhYUiIiIiIiJz4r777uOxxx7ju7/7u+n1evziL/4i73rXu/jt3/5tPvWpT/HFL36R5eVlms0mxWKRO+64g5/4iZ8Ye/nw29/+dt797ncD8OUvf5l3vvOd3H///ZP8K10xhYUiImMIz50FIHryMbh5yoMRERERERGZB2NW/k3SnXfeyTve8Q6y2SxvfOMb+bEf+zFOnz7NZz7zGW677TaWl5cBaDQau3r+r371q/zcz/0cFy5cIJfL8cADDxAEAeVyeS//GntKYaGIyBjChx7Y8KeIiIiIiIjMtm63y913300+n+f3fu/3APB9nzvvvHNHz5PL5ej3+4PvO50OAFEU8eY3v5kvfOELfOu3fiutVovl5WXCMLyqw0JtcCIiMoawGwAQ9cIpj0RERERERET2wqc//Wme+9zncvbsWc6cOcOZM2f48pe/zN13380tt9zCHXfcwdraGgCrq6v0+33q9frgWOr5z38+f/d3fwfA3//93/PAA67IpNPpEEURJ0+eBODXf/3X9/Fvt3sKC0VExqCwUEREREREZL7ceeed/OAP/uCGYy984Qu59tprWVpa4nu/93t59atfzc0338wb3/hGwjDkpptu4sUvfjE33ngjt9xyCwAf+tCH+MhHPsLNN9/Mxz/+cV784hcDUK/X+dCHPsS3fdu38fKXv5xCobDvf8fdMNbaaY9hR06cOGEff/zxaQ9DRBbML/7f/57/7H+aDxa/h/f/1GenPRwREREREZGZ0u/3efDBB7nhhhvIZrPTHs5CuNz/c2PMWWvtia0ep8pCEZExhL2k50QvmvJIRERERERERCZHYaGIyBjCZPlx1FdYKCIiIiIiIvNLYaGIyBjSkFBhoYiIiIiIiMwzhYUiImMI+66yMIwVFoqIiIiIiMj8UlgoIjKGNCSM4u6URyIiIiIiIiIyOQoLRUTGEPZdSKiwUEREREREROaZwkIRkTGEVpWFIiIiIiIiMv8UFoqIjCGMe+5P25vySERERERERGSvXH/99XzLt3wLp06d4kUvehEf+chHrvg5/8N/+A984hOfAOADH/gAv/u7v3vZ+9977738wR/8wa5+Vrvdxhizq8deSm5Pn01EZE6FNlmGbFVZKCIiIiIiMk8++clPcurUKR555BFuuukmXvOa13DTTTcNbu/1euRyu4vQPvjBD257n3vvvZd77rmHt771rbv6GXtNYaGIyBjSisLI9qc8EhERERERkflwy+/fwjeb35zIcz+v8Tw+/f2f3tFjrrvuOr7lW76FBx98kO///u/nla98JV/5ylf46Z/+ab7ne76Hd7/73fzzP/8znU6HV77ylfzGb/wGhUKB+++/nx/5kR9hbW2NF7zgBfi+P3jOd77znZw6dYqf+ImfIIoifvqnf5rPfvazZLNZjh8/zl133cUHPvAB1tbWOHXqFK985Su54447+Id/+Afe85730Gq16Pf7vPe97+U//sf/CMBHP/pRfvmXf5lqtcqb3/zmPf3/BgoLRUTGEtLb8KeIiIiIiIjMl6997Wvcf//9NJtNTp8+zW/+5m9y5513AnDrrbfymte8ht/6rd/CWsuP/uiP8qu/+qv85E/+JG9/+9u57bbbeNe73sXXvvY1XvGKV/ADP/ADm57/F37hF3jwwQf5yle+QrFY5Omnn+bIkSN88IMf5J577uGee+4BYHV1lVtvvZU/+7M/4/jx4zzzzDO87GUv49WvfjXNZpOf+Zmf4atf/SrHjx/nve99757/f1BYKCIyhhBXURihykIREREREZG9sNPKv0l5y1veQrlcplKp8PGPf5zDhw/z3Oc+l9e+9rWD+9xzzz387d/+LbfffjsAQRCQzWZptVrce++9vPOd7wTgJS95Cd/xHd+x5c/5zGc+w4c//GGKxSIAR44c2fJ+X/rSl/i3f/s33vCGN2w4/sADD/D1r3+dN7zhDRw/fhyAH//xH+cXfuEXrujvfzGFhSIiYwiNwkIREREREZF5lPYsTP3VX/0V1Wp1w32stXzqU5/ihhtu2HC81Wpter4r3XDEWsuLX/xivvSlL2267etf//qe/qytaDdkEZExDMJCE095JCIiIiIiIrLf3vSmN/HhD3+YXs+1pmo2mzz00EPU63Ve+tKXctdddwHwr//6r3zxi1/c8jluueUWfvVXf5UwDAF4+umnAajX66ytrQ3u9+pXv5qHH36Yz3/+84Nj9957L1EU8V3f9V187nOf46mnngLgjjvu2PO/q8JCEZExhElImIaGIiIiIiIisjh+5Vd+hXK5zKlTp7jpppt4/etfz5kzZwC46667+NjHPsaNN97I+973Pr7zO79zy+d4z3veww033MDLXvYyTp06xQ/90A8B8PrXv54wDLnpppu47bbbaDQa/Omf/ik///M/z80338yLXvQifuqnfoo4jrnxxhv52Z/9WV7zmtfw0pe+dLCkeS8Za+2eP+kknThxwj7++OPTHoaILJjj78nxVKXPyfUcj/xSd9rDERERERERmSn9fp8HH3yQG264gWw2O+3hLITL/T83xpy11p7Y6nGqLBQRGUOYcZWFkZmtCywiIiIiIiIiO6GwUERkDGHWhYRRRmGhiIiI7LH1dXjd6+ALX5j2SERERBQWioiMI8ymfyosFBERkT32jW/AX/81/MVfTHskIiITk+7aO2vt8GZZ+v96pzsm5yYxGBGRedKP+/STSyuqLBQREZE9F0XuzyCY7jhERCYok8lQKpU4e/Ysx44dI5/PT3tIc81ay4ULF8jn82QyO6sVVFgoIrKNqBcOvu5m3ZvuTq/MiIiIiFyKH7T4P94BH+h9k9dOezAiIhN03XXXcf78ec6cOaMKw32Qz+c5efLkjh+nsFBEZBth4G34vht3KWQLUxqNiIiIzJsHV7/JXz4XXrX6sMJCEZlrmUyGa665hmPHjmGtVWA4QcaYHVcUphQWiohsI/Q3hoVh6FOoKCwUERGRvRFFbvmx3+9MeSQiIvvDGKPVWlcxbXAiIrKNiysLo057SiMRERGReRSFPgBBHG5zTxERkclTWCgiso0w2BgORp31KY1ERERE5lHYdZWFQRxNeSQiIiIKC0VEthVeVEl4cXgoIiIiciWiyC0/Dmx3yiMRERFRWCgisq00LMzE7ntVFoqIiMheSisLfRQWiojI9CksFBHZRpiEg7VkZZDCQhEREdlLUTepLDS9KY9EREREYaGIyLaGYaHbrSttQi4iIiKyFwY9CxUWiojIVUBhoYjINsIwCQt7mQ3fi4iIiOyFqOd2QfYz/SmPRERERGGhiMi2wshVEtbiHABRGExzOCIiIjJnwiQsDNIGySIiIlOksFBEZBthsuy4ZgsARJHCQhEREdk7aWVhkFVYKCIi06ewUERkG2ESDtYpAhBF6lkoIiIieyfsu13UghzQ11JkERGZLoWFIiLbSJuO10zJfa/KQhEREdlDUT/pWZgHOp3pDkZERBaewkIRkW0MwsJcGYCoq7BQRERE9s5oZaH1tYJBRESmS2GhiMg2wq67wl/PLQEQRbriLyIiInsnirsAxBnorremPBoREVl0CgtFRLaRNh2v5WvAMDwUERER2QthHA2+9tvNKY5EREREYaGIyLbCngsHa6U6AFFPYaGIiIjsnSjuDb4O1lenOBIRERGFhSIi2wqTpuO18jIAUTec5nBERERkzoS2O/g6WF+b4khERER2ERYaY37YGGONMW9Kvj9qjPmcMeYbxpivG2O+c+S+u7pNRORqEvbc0qB6uQEMdywUERER2QuRHaks9BUWiojIdO0oLDTGXA/8KPDlkcO/CHzZWvsC4IeB3zPG5K/wNhGRq0YYJ5WF1YPu+57CQhEREdk74UhY6Pva4ERERKZr7LDQGJMB/ivwn4DRM+XvA+4AsNb+A/AE8NorvE1E5KoR9t3SoFr1EDDc8ERERERkL0SMVBZ2vCmOREREZGeVhe8G/pe19ivpAWPMISBvrX1q5H5ngJO7ve3iH2qMebcx5vH0v3a7vYMhi4hcuXSHwlr9MADRyI6FIiIiIlcqsv3B1woLRURk2sYKC40xNwLfC3xossPZzFp7u7X2RPpftVrd7yGIyIIL46Sy8MAxAKJ+93J3FxEREdmR0AzDQr+j4ggREZmucSsLXwNcD3zDGHMGeCXwMdxS4p4x5pqR+14PPGqtvbCb23b+VxARmazQdsn1obzsliGHfVUWioiIyN6JGKksjBQWiojIdI0VFlpr/4u19ri19npr7fW4DU5utdb+F+CPgNsAjDHfClwL/HXy0N3eJiJy1Qhtj0IfCpUaAFGsykIRERHZO6GJB18HoT/FkYiIiEBuD57jPcDdxphvABHwNmtt9wpvExG5aoT0KPahUFoCINJblYiIiOyhaDQs7CosFBGR6dpVWGitfd3I1+eA//0S99vVbSIiV5OQHsXYqLJQREREJiLMDMNCX2GhiIhM2U52QxYRWUghfYpxhkyxRK7vliWLiIiI7JUoE1PoGwCCXjDl0YiIyKJTWCgiso3QuLCQXI5CHyKFhSIiIrKHwozlQM8t+gp6nSmPRkREFp3CQhGRbYQmpmjd22UhVlgoIiIieyvKwIG4AEDQD6c8GhERWXQKC0VEtjEaFhb7hpD+lEckIiIic6PfJ8zBMkUA/FhhoYiITJfCQhGRbUSZmKLNAlCIDZFRWCgiIiJ7JIqIsnDAurAwsNGUByQiIotOYaGIyDbCjKXISFioykIRERHZI7bTIcxBjSLGKiwUEZHpU1goIrKNMGsp4pqOF2yGiHjKIxIREZF50ev4ABQzeco9g093yiMSEZFFp7BQRGQbYRaKxoWFRZshzCgsFBGRKWk2oaeNtuZJGHgAFE2ect8QoN+viIhMl8JCEZHL6Pd79DNuAg9JZaFRWCgiIlOwsgLPfjb85m9OeySyh6LOOgCFbJ5KnCUwCgtFRGS6FBaKiFxG2GkDw8rCgs0qLBQRkek4dw7W1+Ghh6Y9EtlDYRIWFjMFynGWQCsYRERkyhQWiohcRui3ADeBByiSJcrYaQ5JREQWVRBs/FPmQpRcmCxkC5TJ4me0kZqIiEyXwkIRkcsI/aSPUBIWFsgSZhUWiojIFKQhYacz3XHIngrDZIOTbJGKzRNoniEX8dorvOF9z+Gr//Sn0x6KiCwIhYUiIpcRBsky5GwSFpockd45RURkGtKQUJWFc2XQszBXoEyeIGfBKjCUoXv/6g/4XP4Mn/2zX5v2UERkQeiUV0TkMsKRpUHgwsJeFmKrfkIiIrLPtAx5LoWRqyws5IqUM3n8PBBF0x2UXFV8fxUAL/KmPBIRWRQKC0VELuPiysJ0o5OoF05tTCIispha7Qsc+j/hvy5/c9pDkT0UhS78LeaKlE2RMAexvz7lUcnVxA9cD22v257ySERkUSgsFBG5jMFuyLki4CoLYbhkSEREZL+cXX+SlQp8rbg27aHIHooGlYUlKhk33+i0m9Mcklxl/MBVFHo9f8ojEZFFobBQROQyhmFhCYBCJg9AFOjKroiI7K8g+Uzy0RLVeRJGSWVhvkQ568LCoL06zSHJVcYPXVjY7qsFgYjsD4WFIiKXMdihUGGhiIhMWdBxgUFAd8ojkb0UJWFhIV+mnMw3fFUWygg/dPNOz6oNjojsD4WFIiKXMQgL827yXsy43oWhliGLiMg+C0L32RPQm/JIZC+FXbfLdbFQopKrABCsa6m5DAXJUnUPhYUisj8UFoqIXEa6Q2ExXwaGuyJHocJCERHZX0HkPnt8o7BwnkQ9FxYW8mXKyXwj8FvTHJJcZfxuEhYaVRWLyP5QWCgichmDsLCQhIXpMmRVFoqIyD4LksAgyPSnPBLZS4PKwmKFcsFVFvqBKgtlyE82NvGyulAgIvtDYaGIyGVEUbo0yE3e012Ro1C70S0iay0//pkf557775n2UERkAQVJb7sgE095JLKXop5bWlrIlykXlgAIkt1vRQD8pPq0ndW/fRHZHwoLRUQuI+wmOxQmYWEh2aUwVFi4kFphizu+cge///Xfn/ZQRGTWRBG8+c3w+c/v+ikGlYU5oKcKo3kRJkFQsbREpVgFhpvZiAD4ffca8Qp2yiMRkUWhsFBE5DJGlwYBFHJpz0KFhYuoFboeUmnvIBGRccX3n+Y/t/47//xnH9/1c3T6rgLNzwNBsEcjk2mLehEAhUKZcjGpLFRvZBnhx+7ffjcL4br6WYrI5CksFBG5jMHV/mTyXkiXIUc6SVtEXhoWPvX4lEciIrPmgTP/yC++Bj5h/mXXzxEkn0lBDoWFcyRMQuBiaYlyqQaAH7anOSS5yvh2uAuyt/LkFEciIotCYaGIyGWMLg2C4a7ICgsXUyuZoPtPPTblkYjIrDl//mEAvP7uK5ODZClioMrCuRL1k8rC0hKVch0Y7nwtAuDb4S7IXvPcFEciIotCYaGIzCbPg0cemfiPuTgsTCsL012SZbF4q26C7tPd5p4iIhudW3EXGdpxZ9fPEVgXKvl5oLP755GrSxi732uxtER5EBZqniFDo/OOduvpKY5ERBaFwkIRmU3vfz+cOuUaxk9Q2Esn8K7heCFfAiDq6iRtEbXWzgPgG4WFIrIz59eeAKA9spxwp4IkVApzEPuqPJsXw8rCKuVKEhZ2VTkqQ4HpD772Ws9McSQisigUForIbDp7FlZXXYXhBA36CJVdWJjuiqywcDF5npug+0a7kIrIzpzz3cWGNru/yJVWFgJ01teueExydQhjdwGqWK5SXloGwO8pLJQhPzOcd6RzERGRSVJYKCIz6aneKn9/LdCebAPwdGlQIa0sLLjKwlBh4UJqeRcAWM/2t7mniMhG56ImAO0rqEwOGAYG/vrqFY9Jrg6Rdb/XQqlKpdoAhv0pRQD8bDz42muvTHEkIrIoFBaKyEz6mWOnee07IVyb7IRp0Eeo4nYnLKQbnPQ0iV9Enu9Ozv1MvM09RUQ2Ot93u6m3r+Biw2hYGPiqLJwXg8rCQpmywkLZwoaw0G9OcSQisigUForITHom26GTh9bqUxP9OcOlQUlYWEyWIfd233NKZlcrcBP0bha6ffUtFJHxncu4DSuuKCw0o2Fh64rHJFeHQWVhtjAIC/1Y8wwZ8nOwHBoA2oH+7YvI5CksFJGZ1Ekm1u0JN3kehIVLruF4MQ0Lu5rELyIvHPbIDNRPSkTGZS3nC+7zpJ23YO2unibIDINGvzPZnr2yf0J6GAtZk6WctD1JN7MR6YUBUQ6ORnkAvFBVxSIyeQoLRWQmdZLqCi/pITcpoe2R60Mm5yZohYJbhpxufCKLpRUNT879rj/FkYjITGk2ObfkvvQKYIPdXWwIRlogBAoL50ZkexT7YIwhl8mR72/czEYWW7DmLowf67s5qBdOtl+3iAgoLBSRGdUxrrqiPeEmzyFuAp8qlNzZXtTXJH4Reb31wde+JusiMqb1sw+zXnBfxxnorO2iKj6OCbLDisSgo/egeRHRpxCbwfflntnQn1IWm99yc91jGdcSx+vq376ITJ7CQhGZSYOwcH2yTZ5dWDicwA96FiosXEit/rAayG+rwbiIjOf84w9u+L69en7nT9LpEOSH3wbh+qXvKzMlNH2K8fC0rBwbfNQXVxw/uTB+tOD6WXp9rWwQkclTWCgiM6mTLMXygsn2bQnpUxy52l9MeglF2txiIXl2uDulwkIRGdf5c98EIN3QtL329M6fJAjo5Ibf+pGqi+ZFRLyhsrDSzxKY3W+EI/MlaK8CcKR0EGPB66tnsohMnsJCEZlJad+mdmeyO8KF9DZc7S+U3TLkUI3HF1KLYa9KhYUiMq5zzzwCwHVdV53ebu2i326nQzASFgZdBQbzIjR9ina0sjC7YTMbWWz+ugsLlwpVqhG0rfpmi8jkKSwUkZnUSfo2eeFkw8LIxBsm8IOehbEqCxeRlxn+3tcnvAReRObHubWzADwvcxiAdnsXYWEQEOQhn3wmBdpkaW5EJqYwGhaSxVdYKAnfd6toKoUlar0MHgoLRWTyFBaKyOyJ48FSrPaEl2GFJt5QWVgsu+bSCgsXUys3PHlLJ+8iIts5v+56FD6vdhLY3eZcNgkLD9oSAL4qC+dGmLEUbXbwfdnmN2xmI4ttEBYWq9T6ObyMNr8RkclTWCgisycMB2Hh6O60E/lRmZgiwwl8vux6FoZWE7VF04t7+DlLOcmJ/WCyVa0iMj/OhS4cfO7B5wHQ9ld3/BzhugsMDmVchXvQ71zu7jJDokxMYWSuUSFPkFNYKE463yiXa1RjhYUisj8UForIzLEjTd7bvckuw7r4ar8pFMj3IbKqLFw0aRXrNUkxq9/xpjgaEZkl53su6HtO/ToAvF2EhUESFh7Mugp3hYXzI8zYDRcmyybv+lP2tRRZhvONSqlGjQJePp7yiERkESgsFJGZ0/U9bLJpoBdP9mTp4spCjKHQh8hqAr9oWt4zABxTWCgiO3TOrNOIshysHwN2tzlXkFQXHcy5sNCP1bdsXkRZKDDcvaacKdDLQretCnYBP3QTj0p5mZop0c5bsKo8FZHJUlgoIjMnWB9WZEx6R7gwC0WT23Cs2IcILQFZNN7KUwBcExWA4eRdROSyrOV8LuJYv0StdgiAdrjziw1B0rfsUL7uvo+jvRujTE+/v2muUcm4vpRB+9IbaX3jwjd4aOWhiQ9Ppm8QFi4tU8uU6GYh9HZenSwishMKC0Vk5nSC4UmWxwRPlqxNJvD5DYcLsSFElYWLptV8EoBjxvWt9KPJ9ssUkTnheZxbshw1Var1ZDfk7s4vNgTJZ9/B4gH3vVVYOBfC0FUWjoSF5WwRuHxY+H3/7/fxA5/6gYkPT6bP77r5RmXpALVsBQBv5clpDklEFkBu+7uIiFxdOv5wWU47M7negb2oQ5zZOIEHFxZGCgsXjrf2NADX5A4AK/jRZPtlish86D15lgsVONY7QHX5KADt7s7fP4Kk9UGjuAyAr965cyEOO/Quqiws55Idr9uXrh57/Oz95FX3sRCCbgB5qCw1qOXdBkfe6jkO86Ipj0xE5pk+YURk5oxWFrYnuCNcmISSxcxFlYU2o7BwAbVaSVhYPgKAP+HNdURkPjz9+AMAHC0fpnogCQt38f4RdFw1YrVYJx8bAhQWzoMocL/Xwshco5wrAxvbroyy1tI0HZo2mPwAZer8nvs9V2oNqgW3uiG9gCkL6B3vgF/6pWmPQhbA2GGhMeZ/GmP+xRhzrzHm/zPGvDQ5/gJjzJeMMQ8aY/7BGPPikcfs6jYRkcvZsAw5F0+syXO4vnVYWIwzREY70S0ar30BgGvqzwJgvaeTNBHZ3rknXV+5Y7XjLFUbALR3sTlXkPQtKxeXKPczBEYXreZBmMxpipnC4Fgl75aappvaXKwdtelnoJOzdHraFXve+cnvuFI7RK3oepa2k03XZMG023D33fAnfzLtkcgC2Ell4S/KN7UAACAASURBVPdZa2+y1p4Cbgc+kRz/KPAxa+0NwIdHjl/JbSIil9TpDHs9tQtAZzIT5a0m8OAqC8OMwsJF01pfAeBg41nk+uD3dYImIts7/8wjABw7eIJCtkCht7vNuYKk9UG5VKMcZwgyCgvnQdRx/eg2VBamYaG/dVjYbJ0bfr1+YYKjk6uBH6dh4UFqZdeGwPP0e19ID7hKddbWpjsOWQhjh4XW2tE6+GXAGmOOAq8Afic5/ing2caY5+/2tt3/VURkUQQjYaFXwF1lm4AwWRpUzG4OC1VZuHi8wE3M6oevpdIFP57sTtwiMh/ONR8H4Ojh6wGo9jJ47Pz9o5OGheUaFZvDV1g4F7aaa5SLLiz0L1FZ2Dz/6ODrlWce3fI+Mj/S+UapXKNWcdXJXntlmkOSabnvPvdna+v3BpG9tKOehcaYu4wxjwH/F/B24NnAk9baHoC11gKPAiev4DYRkcvqhMNdaP0C9FuTubo2nMAXNxwvkiUyk1n6LFevVicJC6+5TmGhiIztfPspAI496wUAVPsZ2mbn/QaD7jAsLNscQVYXreZBFLrfa2F0GXLSl2704uiolWceG3zdfPqxLe8j88O3EeUuGGOoLR0EwPMvvfmNzLHTp92fqiyUfbCjsNBa+w5r7bOB9+GWDk+cMebdxpjH0//aE6ogEpHZkYaFhb4BYH1CTZ7DztZhYYEsUUZh4aLxIrcsvXb8ehcWEk15RCIyC84FrrfY0eMuLKzF+V1tzhV0XZ/UcmWZMjmCHNCb3CZfsj/CdE4zWllYSsLCcOvznmbzieHXK09seR+ZHz5dKn132l6tJWFhoLBwIaVhoedBrAtGMlm72g3ZWvvbwP8GPA4cN8bkAIwxBlcd+Cjw2C5vu/hn3W6tPZH+V61WdzNkEZkj6VKsI303sW6vnp/Iz0kn8MXcxWFhjjCrsHDRtLrupK1+zUkqPTd5FxHZzvmeqwA5Vj8OQNXmae+iKjDoDcPCiing54FAGy3Nuqjj5jSjc41yqQZcJixce2rLr2U++fQGYWGtfgSAduhd7iEyr+67jwcPwZNLdmJtmERSY4WFxpgDxphnjXz/JuACcB74J+BtyU3fCzxurX3IWrur2670LyQi828QFlrX08eb0I5wYScNC0sbjhdMlig7kR8pVzGvH5DvQ7FxhEo/i48qekRke+doU+4ZlvJLAFQp0M7tIizsu9YH5aVlyqbgKgsntMGX7J8omdMURsPCsgsL/Wh9y8c0veGKimZrMhdM5erhZ3pUYjfxrC27sDBd7SALJAyx33yI7/xh+PF/h/oWysTlxrzfMvBHxpgyEANPA//OWmuNMT8GfMIY816gBfzwyON2e5uIyCV1kqVYh7M1oEl7Qk2ew6SPUDFf3nC8SI5+Bvpxn2xGqeGiaMUB9RAoFqnEWW0uICJjOZcLOdYr4hbSQNUUaRfAhiGmWNzm0UNBsgN7uVynnCkQZMH6PmYio5b9MphrjFyYrCQ73qY7YF+s6Q93wm2uT+aCqVw9/Eyf5di9V9QOHAPA624dJMsc+8Y3eKISc64KZ2u4voUnTkx7VDLHxgoLrbWPAN92idseAF61l7eJiFxOuhTrSOEAxI/ieRe2ecTupFf7i/mLKgsz+eT2YNBXSOZfi5BazxXkV2wWP6NlyCKyDd/nfDnmWpYGh6rZMr0sRK0mxSPXjP1UQbKpUrlQoZIp0s9Ad71FYZvHydUtitycppAfqSys1IFhn8qLNYPmll/LfAoyMcf7bu5ZO+jaGXi9rYNkmWOnT3PaFZbSKqLKQpm4XfUsFBGZpk7PVVccLh8GoD2hJs9hMoEvFjZWFhZcu1WiQEtAFomX6VLvu999hTxBzhJbNZcWkUuzTz3F+SU4ljswOFbNus+UnfbbDWJ3gaKcK1NOqtCCdW1yMOvCJBAcrSwsL7nKQr93ibAwGoYEK5FeA/POz1kquLCwUm1gLHix+pUunPvu47Q79XFhoXZElglTWCgiMycNC49UjwLQDra5svbEE/Bd3wUPP7yjnxNGWy9DHlQWBloCskha2R416373S0ktz6WqPkREAFbPPkQ3C8eKhwbHqknvQm9th2GhdTuwl/OjYaFOFmfdoLJw5MJkpdoAhkvPL9bseyxFUOhBs6sLl3Ot38fPQznj5h3GGKoReDac8sBk36myUPaZwkIRmTmdpMn7kQPXAuB1Ln+yZP/qr/jCw18g/vyf7+jnDCoLi5UNx4vJhC3qKCxcJF4upm7dMrGKca8Bv6tlQCJyaeee+AYAR6vHBseqBde+ot3aWa+5INmBvZQrUcm5zyXfV1g468KuCwRHL0yWtwsLY59GAI0ONGPNReZZt92im4WKGS5Tr/WytImmOCqZivvu4/QJd6HIL0BvdTI920VSCgtFZOYMwsJDzwagHV7+qvrnz/8t3/VO+PTK3+7o5wyWBhU2hoWFrKsuC7UMeWH0+l2CPNQy7mSuknGTdoWFInI558+7ivZjycUtgGrR7XTb3mG/3YAeuRhymRzlvPtcCrarrJerXpSEhaOVhYOwMN66emzFdGh0szRCQxNVuM+zwHOBUCU7Ehb2s3jqm7xY+n148EFOHxq2v/FaT1/mASJXTmGhiMycTuyuph4+cj2w/Y5wj7XPAnB2/akd/Zz0an/hosrCwTLkUEHRovBWzwFQz14UFobtqY1JRK5+55qPAXD08HWDY7WS60fXbu+sKiQwPcp9N3UvFxQWzoswaa0y2h+5lGxw4sdbV481s10a/TyNbo6mUYXZPPMHYeHw9VGzebxsf1pDkml4+GFWMiHnCsN/7y2FhTJhCgtFZOYE1l1NPXLsOQC0t9kRrum7iVars7Mm4IMJ/MXLkJOruwoLF0frmScAqOfd8sFKzk3afW0uIKN6PXjjG+GP/mjaI5GrxPmWu0h17PjzB8eq5SQs9He2i23H9CnbLDAaFuqCxayLeq56cLSy0GQylLvD+c4oay2r+T4NW6JhizTzvX0bq+y/QViYHw0LC3g5hYULZWRzk1oyF23t8IKTyE4pLBSRmZNWFh6quR5Q3jZh4WrS03At2tmy4TCZwBdL1Q3HC7kkLFTPwoXhNd0Jf63glg8OwkJN1GRE+ORjPOeGz/Jrf/NL0x6KXCXOBa7y4+izXjA4Vl1yOyO3d9hvMMjElGMXFlaSpcx+R+0wZt3gwmRpacPxcs8M+lSO8iKPfgYamQoHbZkwa7XZ1hzz191FhUp+eOG6miniFcD2FRgujJHNTb71mpcD0PI1B5XJUlgoIjOnQ49CH+pFt0ynbbduAJ5q9twyrVZ3ZxUYYT8NCzdO4AtZt7lFuluyzL9Wugw5WT6YTtpVWSijnmme5UwD/s48Me2hyFXiXNed6B9bHulZuHQQgHZnZ0uIg2xMmRwA5eQiVhDpotWsi3ruAmjhov7I5TiDv0VY2PTcLtqNbJVG1s1PVgKFBvPKb7t5RqUwnIvWMmV6WYhaO6tOlhk2Uln47SdfBcDaNhs8ilwphYUiMnM6tku5ZyjlSmRj8OzWDcBTqz13MrUW7yzc27ayUMuQF4aX7FpaS5YPppN27UQqo/y2O3G7YPXeIM752CMbw8HywcGxai0JC8MdhIW9HkGOkbDQVRYGocLCWTe4MFneeGGy0s8QmM2VY83zjwJwsLBMI+9eB83VJyc8SpmWwHfvE5XCcC5ay7pgOV31IAvg9GlOnyhSyBa4+ZpTALRCzUFlshQWisjM6Zg+pTiDMYZqL0PbXH5HuHSnwNY2FYgXC5PlzsVKbcPxYr4EQBRp2c+iaHkuLKwvHQIUFsrW0gqQFbOz9xqZX+eyHY50C2TMcMpdrbvykHa0g2r3IEjCQlfZXikny5C32eBLrn5R381hCsWLliHHWYLMFmHhM27TnEapQaPolrQ3n35swqOUafEDN8+ojFy4TnvWpZuvyZyz1oWFRw03HLphcPGptZPPEJFdUFgoIjOnQ49S7N6+av0c3nZhoXFX7dd2uGNg2E/CwvLGsDCtLAwVFi4Mb90t8apV3QRtKanqWVdYKCN834WFF3KXf0+SBRGGnC/1OWY3Li+tLrvGU95Ogr4gIMhD2eQBKFdclXOgdhgzb3Bh8uKehWQJMvGm+zebroqwsXSIRhIaNJtnJzxKmRY/cH1Jy6NhYTEJC9e0G+5CePxx/LDNI8UOLzryokEbplZPF4tkshQWisjMCTLxICys2jzt7OUbPK9m3Yl7K7OzE/gwdve/uLKwkOxIp8rCxdFKdi2tJyf5lZI2F5DN/GS52Eqh7yoBZLGdP8+5JTiarW84XD1wFID2NptzbdDpuMrCjKssLFfccwY9fQ7Nuii5MFkob2x5UrZ5/OwWYeGaW3raqB2hkVS7N7UcdW75oZtnVErD95FqEhZ5LYWFC+H0aR44BNbACw+/cBgWxnr/l8lSWCgiM6eT6VOybkfIGgW8fHzpE/M4pllwk+21fAw72DkutGlYuPFEr5AuQ+5qqeGi8JLeYrW6O8mvlN1rIp3EiwD4gXudrJahv67lQQthfR3+5//c8jPIf+IR2kU4Vjy44XhtOQkL++N/hvTX20Q5KGdcZXtlqeF+Rk+fQ7NuMNe4qD9yhTxBbvPraiXd4GT5Gg7WjwHDTU9k/vih+yypJNXEMOyf3G5rY5uFcN99g52QN4SF6P1fJkthoYjMnE7GUkqavFdNkXYB8C9RoeF5rLpsj1YRaI3fUD6tLCwUyhuOFwsKCxdNKwkL6wevAYaTdl+bC8gIf2R32+a5M9MbiOyf//bf4Lu/2wWGFzl/9kEAjlaObjheyBXJ96G9gz66nXXX8qCcdZ8/5arrVRfsIHCUq1OUzjUuriw0eYI82HhjdWFz/QIAjcazaBxwn0nNtirM5pWf7HheWRoJCyvuYoGnsHAxnD492An5hUdGwsJsH7pqeyKTo7BQRGZOJxtTti4srGVKLixsb13F03nmKTquxRNrJbDN5tg/J7Rd8n02NKYHKBRc/6lQYeHCaHXd66t+6FoAKhV3op5O4kVgWAECsHL+kSmORPbNuWSDgT/4g003nT/3bwAcW37WptuqXUPbjt9HN0j6o5ZzSViY9ixMdtKV2RXaHjBctZBKl5ynQXGq2XHzmMbhEzQOus+kFV+h0bzyk76kaTUxQG3JVSt7SZ9cmXP33cfpa/NkTIYbDt1AtVDF2J0XQYjslMJCkV36kT/5EW7/29unPYyF1MlCySSVhdky3SyEaxe2vG/z6UcHX/czEKyMv3NcRJ/iFquW00rDSMu/FoaX9BarpZWFyRV+v6vNBWRoNCy88Myjl7mnzA0vaUXw3/87RBvDv3Mr7jVw9NDJTQ+r9rO0d7DpVpAERqUkLKwkGxz4VmHhrIvSsDBb2HA8XXIetDde5GxGLhxoHD1J4/AJdyxUaDSv/GT+UamOhIU116vS62iTtblnrQsLr8nxnAPPoZQrYYyhTtGFhWt6DcjkKCwU2QVrLXf/y93cc/890x7K4un16OSglOwIWcu7E6b2JXaEW72wcYfAtQtPjP2jQnoU+2bT8eIgLNzZ7soyu1pxQLE3DIrTSbuvzQVkxPpIpelKsmOpzLk0LFxbgz//8w03nVtznzfHjj1v08OqcY52pjf2jwmSfpjlZIOt9M8g1ufQrItsj9wWqxgqyZLzTWFhz2Mpgvyho5QPH6fYg2ZX/XPnVTrPqNSGYWG17takeh1Vlc29p5+mt7rCN5ZCXnjkhYPDdVNiTZWFMmEKC2X2fPaz8I//ONUhrHfX6cW9QR8z2T+9YJ1eFkrGXYGv5pcA8Fa3bu7dTE7Yy333dtfawY6BIX2K8ea3yULR/cyop4qOReERUusOXwslhYWyBb87Eha2xq9iltl1NjjPz70Gwizwh3+44bbzvvtcOnbtDZseV41ztLPjb7gVBC4MKuddG4xitoixEKB+VbPOzTU2X5hMl5wH6xurBpuxz8EAqNWg0aARQLOvDZXmVdpqoJJUE8LIJkmRfu9z7/RpvtmArol54eGRsDBbUWWhTJzCQpk9b3sbvOc9Ux3CasdN3LxIV3L3W6ft/t+Xkl4+tWINgLa39TLk1TV3wn7SumbAa63xdwwMTZ+i3SosdBUdoXpFLYyWiaj3s4PvzdISlQh8bS4gI/zuMDy+0NbupIvgd0sP8r7Xw+/cch3ccw90hu8J50JXEXa0cWLT46oU8HLxpuOX0ukkYWHBXawyxlDuGQLGr06Uq1NEj8KWYaGba/gXh4UmpBFlIZOBapVGB5rowtW8SucZG5YhN9wu2F5PfZPn3kU7IafquSX1LJSJU1gos8Va/rHc5KFwusu7moE7AVBl4f7r+O7/eSnp7VMtud5xXvsSPQuTHQKvK7hP2lZr/B0DQxNforLQVXZEfS3/WhRepkctzg8PVCpUurAeKzCWodHweCXY+j1J5stq352sf+ylsTtpG9kV+XzsPq+OLh3d9LiqKbrNufrjVRcGHVdBVE4q2wHKcQbfqLJw1oVm68rCSlJFGvgb55rNbESjn3weGcPBbo5mRp9F88q3EcZCMTfcAKd28Dgw7Kcsc+yinZBT9UJNlYUycQoLZbb4Pre81fKfXnRmqsNId6LzQlUW7rd0V8By1jX+riW70rbbW+9y3Fx3J+zX168DYG19/BP40MQUtnibLJZdNaPCwsXRysfUKQ4PVCosdd0kXiQ1GhZe0IYDC6FlXUXX38ePce81bFiKfM74HOjmNm1cAcPNuSJvvNdJEG4dFgZm/KXMcnWKiClscWGyvEVYGNuYZr5Pg2Fw1IgLNLM9rLWTH6zsO99GlHuumjhVKdfJxODFqiide6dPc/q429RxQ2VhsU67CP01zTVkchQWykyxrRbnl+Bcfron6Oky5LAfEqpv3b7qJH2bSknj72oSFnrB1h+W6e/quqMvAKDlbx0qbiXMxBRtdtPxQWVhrIqOhdDv4+UtNTMSFpbLVLrgo7BQhvyRStOVrirPF4E3shvxb73hKPzJn0AQQLfL+WKPY3F5y8fVsu54uzleb8sgdBWM5VJtcKwcZwkyCgtnXWjiLVuelJO5RtqvEtxF6jgDDVMZHGtQJspaAvXQnUs+XSr9ja8PYwzVrtnw/iNz6r77OH1tgePV4ywnq6kA6uWkWOISGzyK7AWFhTJTwrUL9DOwlu+7reSnJF2GDOpbuN8Gy5CT5RjVJdfDpR1sXYbfjNzx606+BIC1zvjl+mHGUmSLsLDkKjtC7UK5ELrNC3Tyrpn0QD6fhIXqFyZDvo0o9KAawUqsXlKLoGUisjHcfOxmfue6FuthGz73OXj6ac4twdFMbcvHVXPuc2TcE72g65YbjoaFFZvDz4zf91CuTlEmprBVWFioAuB3hvPMpud6oTZy1cGxRsZ9Nq0EK5McpkyJb3pU+pvnotV+hrbRPHSura1hn3iC++vRhiXIAPWlgwC0vGemMTJZEAoLZaZ4yRX4tSIQTu9qWlqtBlqKvN/Svk2lvAsLa3XXyMO7RP/I1Z67/3UnbgRgLRq/2ifMWorkNh0vlN0kXZWFi8FbcT1Sa7mlDccrcRbfKCyUIVcBYjgUZrlgtPnNImhlXT/TW19+Ky06/OGLgT/8Q3pPnuVCBY7lG1s+rppsVNJujXeiF0RJWFgeqSwkT5DT0tNZ5yoLN4dBlZKba6RL0AGaTz8GQKMwrDBq5Nxrormu0GAe+Zk+lXjz66PWz+FlNA+da6dP83gd2pnehiXIAPWq2x27dYme7SJ7QWGhzJR0Ur1WBLs6vR4NzZGrt9rkZH+lO0KW8m4JVzUJC9sjk+lRzdjHWDix/GwAWt0xq32sJcxC0WwOCwc9CxUWLoTWhScA10x6VCXO4mcUFspQulzsYC/PijYcmH9hSCtvqds8P/iSH6ScK/Ox11Xhf/wPnvnm17AGjlaObPnQatG9n7THrApJl5iWK/XBsbLJE+QYe5MUuTpFGUthi1UMaRXphrDwmSQsLB0YHGsUl5PbHp/kMGVK/Eyfit08F63Febys/u3PtdOnuW+LnZABlutu46xWMH57JZGdUlgoMyUNC3tZCJrnpzaOVW+4bEjLkPdXp+PCvjQsrC27D8v2JULAJh0ORBkOJBPrtXi8neNst0uYg6LJb7otV3JLfkKroGgReGvuvaZerG84XrE5/GyspvIykFaAHLQlLhR0Ejf3PI9WEeqUWC4t89Yb38qXD7T5l+o65/74LgCO1Y9v+dBqyb2ftNvjLR0dhoXDirKKKeDncT0SZWa5VQxbhIXJhUk/Gs5vmk138apROTQ41ii56tXmBYWF88jPWSpsnovWKODl1IZgrp0+zekkLHzRkRdtuCldhrx2iZ7tIntBYaHMFG+k1HotWRo4DWnPGFBl4X7rpDtCJku4qsvuU9Trbx0CrmYjGv08taQqrGXHWxrYC9axZuvKQpPLUehBpLBwIbRW3b/3WvnAhuMV8sRGu2LLkAsLcxwyFVpFS7en18ZcS8PCjGuLcevLbwXgt14G5//xrwE4dvDZWz60Wnahn7c+XlVI0HOfXeWl4ftQOVMgzEHsqz/mLIsyUNhirlFJXiPpEnSA5pprx9OoDStWDy65FRbN1acmOUyZBmsJspbKFheua6aEV3AXt2VOPfEEp90/7809C5ML2DoPlUlSWCgzZfQK/FpzemHh6vowtFTPwv3VCd2kuZSEhWmA0+5vXVnRzPc4YItkM1mqvSxrZrylgWGykUoxU9jy9mJfYeGiSC9S1Csbe49VjHtt+N3xqlVl/vnZPhVyHEx6iK1qWeB88zy84nDzo2+/9tt5ydGXcPfLsjycZHpHjz5ny4dWK8lnlz9eVUjQd59d5aVhZWE543Zo77RVWTKz+n23imGL/sjpkvOgO5zfpBerDy4PK1YbNbfCIg0SZY6EIX4eKqa46aZqtkQv6zZ/lDnVbHL6mOFA6QDHlo5tuGkQFna3bsMkshcUFspM8UYm1Wtr01uG3PRHehZ2NEnfT50kmCkVXVi4lISGXrxFxWCnw2oRGrgly/U4TyvbG2sn7TBwIXAxs/lqLkAhNkRomeEiSJtH12qHNhxPw8L1cftgytzzs2652KGCC4IunHt4yiOSSeq3VlkvQD3vNqIwxnDry29lLd/nI9/q7nPsWTds+djqkrv40A7GqwoJYhcWlkrDXXArWVfR6I9ZnShXH9vpEGWhkNkiLEyC4XQJOkAzuVjdOPiswbHGgWvcbe3xdtaW2dFtrdLLuirii9Vy7iJFu6mQeG41m5w+bHjh4RdijNlw0yAs7GkOKpOjsFBmSruzNvh6zZvepKgZDsfRamlytp/S5TilkgsJc5kc5Z6hzeblfv2VC6yV4EDOnVwtU3Q7abe3vwoXBu4+l6osLMSGUGHhQmj57kS8Xju84fhSsvRQlYUCQBwnFSAFDiY9xFYuPDblQckkeWmLgvwwwHvbTW+jlC3xLy6/4eihk1s+tlZLN+caNyx0n3HlXHlwrJxz70FBe23Lx8jVr9fxk5Ynmy9MpkvO/f7wYmgzuUDdOHRicCwNDld8VZjNG99zv9PKyL/7VPq+k74Pyfx5pn2eZ8rxps1NYCQstOpZK5OjsFBmitcZTqrX1qc3KVrteiwl2ZTnaXK2nzqDsHB4clbtZ/HM5rBw7elk18C8WxJYz5RpFYG17U+sBmFhbvPSD1Bl4SLxAvd6STfTSQ2qei6xE7csln7bI8y55WIHqy4IuqANB+ZaK938qDRcGnygdIC33PiWwfcXLx1LVZNK5faY7x+BdX3JyvnRsNB9HfgKC2dVlKxiKGyxiqFScxsYBCNh4UrkftcHjg57YabBYTPUSpd543tuJVM63xiV9uL2prjSSibrdMadY17crxBGw8JwrBVTIruhsFBmSjsaTqrX/Oktu2n225xM5uattsLC/dRJmryPhoW1fo52ZnP/wLRfWKPors4v55ZcZeHq9hPqsJOEhdmtw8JinCEyCgsXQdo8ut64ZsPxSi5dAqgTNIFgcFJX5FDNBUQr6iE211reM8DGsBCGG52UciWqheqmx8Fwc672mG0MOklYWMoNQ4NK3i1DDHw1uJ9VYcf9/resLKy6CuV0CTpAs+9RDSF/aHjxqnT4GkpdaEZ6Hcwbv+3OddJ/66NqyY7qXvI+JHPGWk4X3cWEy1YWFiwEqi6UyVBYKDNldFLdmtJW8d1+l3WiQVjojdmcXPZGJ2n0XS7XB8eqNoeX2xzcNZtPAHCg7Cbc9VyVdhH6ze0D3iidwF+qstBmiEy8s8HLTPKS5tG1gxeFhcnkXWGhAPitZLlYtsTBA27zgZW2Kj7mWXqxsL60cfOjV514FS+95qXccOiGTX2mUtUDLuxp98ZrYxDQpdg3ZMxw6l5O34MCVRbOqnSuUchuDgvz1TrZGPx4uHKiGfs0OkB9OAfiwAEOBtDsq3fZvEnnF1uFhYMd1bXCaT75Pvc33LnNVpWF6YWocVdMiezG5m66Ilcxb2RSvTalK6irSb+YZ6eVhVMKLRdVpx9CBkrl2uBYjSLnc22IY8gMT6RWV58CoJEsCVwu1aEN3oUnObDNzwnDNCzcvPQDXFi4brpX8DeRWZE2j65fvAw5l4aF2lxARipAchUOHXJLBC+sq+JjnnlpP9Pqxn6mxhj+/O1/Ti/eXPGeGoSF/fEqQgLTp9zfeI2/nGz0FXTUCmFWpS1PCtkt+iNnMlS6wyXoAE0T0oiyG+Y6HDhAowPNovrnzptBWJhs5jeqVkk2SdIcZD41mzyZFKafqJ/YdHM2k6VmC7SKEbRacPz4pvuIXClVFspMacfDSfXalLaKT8PCwz5UIvAibyrjWFSdfrIjZGWksjBTxCsC6xuvqjc9V9XTqLmTsnrJRYSt1e2XBobbVRaSIcqoR8gi8JKT+VqxvuF4Onn313VFV0aXi5U5eMRtarHS0UncPGslFX0Xb34EcKhyiGPVrfsV9+JjEAAAIABJREFUAhQLFbIxeLZzyfuMCkyPcnypsFDzkFkVpRcmL7GZWrlnCBgJC7MRjf5FVYj5PI0oQ9OEyHzxkxYDleLmdga1qutp6aloYT41m1yoQI3C1hcTgLopufZKqiyUCVFYKDPFs8OJ0NqUlls0k5O/Ax2oRdBSWLivgjQsXBr2iKpmyqwXIPY2Vps2266q50DSa255yTWUXxujj1gYuiv0xfzmHegAimSJMlqGvAhadCj1DPmLlomlk3ctARTYWAHSOHodABe6em3Ms1Yn2fyovjks3I4xhmrX0LbjBTxBNqZssxuODd6DtMnSzIo6bq5RuMSFyXI8DAtjG7Oa69Owm1c8NPoFmrkuVhsdzBU/SMLC0hZhYbJJkqc5yHxaWeGZChzKbN33FqCerbhlyC31K5XJUFgoM6VNSKlvyMSwNubV+L2WVhY2OlAPh0sUZX904s2VhbVsshx09ekN911Nlog1GtcCUE+uwrbGaAYdJrsuF/KXWIZMllCVhQvBI6Ley246nk7e/UAXDAT8ZEfaSrFK/tAR6h1YUQ+xuZZeLKwfuHQF4eVUexnaRNvfEQgyMWW7sXtQueTacQShXmezKoySysJLbKZW7mfxk83UvNAjzsBBs7l/XYMS3YxlfcwNc2Q2pFXDlVJ90221utskyQsVFM2lZpMLZTiU2/y7T9XzVfUslIlSWCgzpW161Hs56t0Ma0wnLGwGSQAVQC0EL9YOVPupEyc7Qo5WFibLQb21jZsJNCP34XngiOv1sVx3y5HX1le2/TlpWHipysICOSK9gy6EVrZLLd7c4reSbLKz3tFEXUYqQIpVyOU4GBpW2LseYr24x9888jd79nxy5Vo9V9F3cc/CcVXjHO3MpfsaDlhLkLWUL2o1Xk569waRAqJZFYVuDnmpysJKnCXIuLCwmfRAbeQ2Vxo1kgAxnaPKfEirhssjfbpTtaSPshepsnguJcuQD5cOXvIu9UJNlYUyUTrVlZniZXtU4xzL/RxrmelsLjFYhlw56CoLp1ThuKg6NiLXh1xhWPFXS3YEa69trBhcTTbBaRx2mw2k1R8tf/vJdBi5CXyxcIllyCZHtLnYTOaQl+1Tt5v7xVSSqh4tARQYXS7mXheHunkuZPauh9gd97yP137itfzj6b/Ys+eUK9PquzC4Xrx05cfl1OIc7Wx/+ztGEUEeylzUCqHiLpr5XW1sMauGLU+2XsVQJkuQtDxpPv0oAI3C5tdbI+fed5rB9hdDZXak84v03/qoasPNaduqJp1LnZXzrBdc/9tLqRfreEWIV3WRQCZDYaHMDmtpZ2OqFFiOC6zlxrgaPwGDZciN49RD8Ix6xOynju1RuujcqpqcqHkXLS9uxm4CdaDirsotJ70L1zrbN4MOu0lYWNy83AegYHLEGej3tCPyXAtDWgWomc0ncksVt2GOryWAAvjpcrGk4vRgXGQlt3fvD3/+pbsBeOpf/teePadcGS92Fwt3GxZWKdDOjdH7NggIclA2Gy9alJMAIehqhcOsirqXryws2zx+1r1GVp55DIBGqbHpfgeTALG5+uQkhilT4idVw5WlA5tuqyVzWm/MHdVltlxYfQKAQ9Wjl7xPvdLAGlhvXdivYcmCUVgos2N9Ha8INVNkmSJr+RimENI1fXfV9kDjOLWuoWtiwr52oNsvHXqU+mbDsVrSy6V90fLipg2odM1gF7F62U2w18bYlCbsupPAYuHSYSFA2FFV2VxbW8MrQj27ucI0nbz7/z97bx4lyXXX+X4iIrfIJTKzsrpU1d3qliV1tyzJwgbLEnhhmzFgeM/MY4CHMYuNH9t42I7fMMwM5zC8M5hlwCyGhxewzbHxGRaDeQ+e2cZItmzLMjaSLaurW0t3dVVXL1WVmZGRGblFxPvjRmQtmRGZVZW1ZPX9nKPjVmYsKWf2jXu/9/v7/uSuvgRo+KVgaV0IOCV0agmPtjNaJl0U7toqn4iJhYNlSQfBYSGoLMgl+0sERyFLEivuDZ/L2DZ2HFLqVrFQPPvsrqxwmFR6kSchVQxp4tgx8fsor4kxoJjpdxoVdbEpWl5d2ouPKTkgAtfwILEwncyiujIO6aiyaopmjKX8XOgxRlqsa8zazdBjJJLdIMVCyeRQq2ElIKumyCu6aBXf2P/Sm4olBuRi7livNLHWkg0O9gtb6ZJyNg9dWd/hVduyiK6obYrd9YynfEos4kdpStPyF1/JAR3oABKqKAdrN+R3f5Rpr92kFYNcLNP3XjorJmmyBFAC6w7ToFxsShNjR7m++x3/pz/0Dsq+lmDJzpeHBlNtozsqMbU/03QUsmqKVgw69ejnSKdRw1FB3yIWpjP+GCSdRRNL29+YTMRCypCVOB0NHNehXL0GQDF7rO+4ol9BUS5f3aNPKjkIGl1fLMz1C8RBR/Ua0rBwFFn1M0qnp06GHmPkRF6uOUIWu0SyE6RYKJkYXLNKPSE63+ZjGdoxkeew35T93Zu8MdMrTTRlJ7J9o6l0Sbmbh65cRkySLXtzeXFZ61Bw1xdX+aRYxFed4eLOuljYLxIBJBVfLGxKV9lRplYWizMj3i8a61nfWdiVC3XJBgeI/7uYigvX19rNy7u+9qOf+GDvz5ZsqHNoMNUuhrMzoRAg6zuWrfL1yONsSzzbdG2zoBSMQbasbphY1qsYBjsLA4HYbtZ6889ifrbvuGJOlCqWq9G/Jclk0fDnounc4CYXOUejNmJHdclksWKLjcaScVvoMT2xcIQsdolkJ0ixUDIx1CtCGMzG0+T9TnDVAyi3qDRWybUgVpjqlSZKsXD/aCouKW+Ls9B3eNU2LqJdl3LSpcj64irIlRqlKU2rKxZfYWJhQpNi4a2A6YuFg8oMtUyOZBcajiwBlGwUC8V4VPI7GK7e2KVYeP48j3rr15Bi4SHBdTHj7sDmR6OSjYuYC6savfFp132xcEuune7/1mxXioWTSs9ZGCoWiu+8UVvruZSLU8f7jgsExDVLliMeJYL5RZhYmHW00TqqSyaO1bYY90t6eIOTfBCv1JQVB5K9QYqFkonBqogJUDaeJe+LPlV/Ib+flO0yhSZQLPZKE2sjZOBJxkNTdUh5m9sQ53KiJMfaUA7uVatUUlBU18W+dDyN5ilUleELqyCHMrwMWSwQZWbh0aZWFeOOkerPC0LXSXekWCgRBA7TdFYs6qb8DoZra4u7uq73gffz6GnIK0JMqLflBsWhwLIwk2AwuDHFKGTj4vlkVVcij2v6cRd6bLOgpCfF+bYnG21NKr0qhpB85LTvJrWtMuWmcA8VS/1licWiyDUbR+yBZB+xbbhwIfTthifmounk4LlozotTG6WjumTiWHXEuD+dng49pmeCkKYVyR4hxULJxFAzxaI9l8xh+Nlz1cr+l1tUWiZFGygUMBLCbWTKHZ19o6l66N7msq9sXjxIN4q2jZVlOhoUYusTLEVRMNw4ZsKDZrTA0+qKso6kHiIW+k1T2k2ZV3eUMYNxZ0C4OOm0EAulq0fCBgeIIcTCoIPhankX3Ukdh2f/8n1cy8FrT38DAJZsqHM4qNWoJcBQBzvCRiHrzyGsWrRYaDfEHEOPb75XTI0Rd6DhyTLESaXdEc+PRDLEWRgLxMIK5bb4HRRnTvUdV5wWAmIgKEomhF/5Fbj/frg+eD1jux1Ud33OuZUcSWrxETqqSyaOVU+sL0rpcGdhTyzsSOOCZG+QYqFkYrAsEd6aTRnk/e5PVfMAMgu7NYpNoFAg5w/SNUsGy+4XTc0jxWaxMJcXi3JrQ6OJys0rABQT+U3H5kmK5jiVzfmGW2m50WJh0i8Ha7ekWHiUMS3h0jAGdJ/siYVyoS5hXTTuOQsLwumzZu5iU+sf/5FHdfGce92Z1wFgjZC5Ktl7vGoVMylylHdKLiXmENaQOYTd9J2F8f576V0FG+ksnFR6VQzJwZEngZvUblQpdy0Rg1Pqb3CSnJoh3YZyRzqMJorPfQ46HXjuuYFvN2ijOwqKogx8P6ekqCXAa8lNyyOF57Giig3IqDLknlgo5wWSPUKKhZKJIZhM59JF8lnhJKta0bvx48bzPCpuQ5QhFwoYunAbmQcgWt6q2APEwmzeL0Pe8LAsr4k8y8KW8lFD1TG3JRb2Z9XBBmdhS7p8jjK1unBpGMaAMhBfLKzLhbqEdbEw5bu/pooiV2ytsYvn1PvfzyN3iD9+832vB8CSnW8PBc3qCl0NjPhgkWcUsrrYzLLq0W4w2/bFwkT/vXRXxUZmlk0q7W7gLAwpQ/YF4ka9yppbF5Ut+Xz/gcUixSaUu9JhNFHMz4v/vTq4i3WDLmknfLme03S6GrQq+7sekuwx9TqrKY+kq/bGgEH0xEJXzgske8NIYqGiKClFUf5SUZQLiqI8qSjK3yuKcrf/3oyiKB9TFOWioihfUhTlNRvO29F7Eskgan6np2ymQP6AWsVbbQsHd70MOeM31jDlQ3o/cF2HdgxSfifigJxfll7bsIgul8XEq7jFEZbXMlRTjCAWCgEomTYGvp/QhLOwJZ2FRxqzKX4nOaPfyUE8TqYDDSkWShAO03SHngOkdEyUCq7aO3xOVavwF3/Bo2eT3DN9D7P5E2TaYMmy90OB6TddC+JIdkI2LTazrEb086jnLBwgFqYdlYYixcJJpeX4G5MhzdQCN6ltm5SVJsWOCuqA5VuhQNGGsivnJBNDqwUvvCD+vDS4YWND6ZJ2w5fr2Zj4fdQOIMNdsoesrbGahmnSoa5S2CAWKm1wZHalZPxsx1n4buCc53lfAXwUeK//+i8Dn/E87wzwJuCPFaW3kt/pexJJH5Ytslqy2Snyhig7rdrRE+xxE2TBBM7CXEaUmwWlipK9pWWLHfOUunmoSMVSqC5Y3voiuuK7PYu5zSKPEc8KZ2E1OmeyHYiFIaVBybjIEWq35W7eUSbosG0UZge+n3Y1GqqcoEmgrnQ2OUAKfq7YWmuHz6k/+RMuJ5tc1lu85pTYT812FSykWHgYCCoKgsXaTsj6G47B/CYM23ew6wOeR7qnYasys2xSaftiYZizUE8EYmGNstah2A3pvp3JUGxBWZENtyaG557jy1MO/+3V4C0NboTVUB3SbmzgewC5XpMk2QX7SFEus5KGkhrtXO+JhUmgJpttSsbPSGKh53lNz/P+xvM8z3/pM8Ad/p+/C/h9/7gngKvA1+7yPYmkj2DRnstNk58SWVDV1v42Fqn4LqMgs9Dw3Ua1ISVEkvHQrIv//1Pq5smyoihkuyo11rPjyjUxcSrkb9t0bD6VpxWD1lr0xKrlCadGWKh0IsgslGLhkcbsiMlXrnjbwPfTbkyKhRLAd4BsEAtjpWMU7PWOhtvmAx/gE3eLjZHXnA7EQg1LOlkPBabflMTQizu+RtbPt7Sa0TlzPbEw1Z+hq3sxbDkGTSzDIk90vwtuvWlSiTsUw7pvKwpFJ8Ga1mZ9uSY51MzP846H4b98I5y/eX7gIQ3NIe1FiIVJ8bupSbHwaFEus6pDKRa9GRV8/2YSMGVeqWT87DSz8CeBjyqKUgLinudt9D5fAk7t9L2tN1IU5WcURVkM/rEsmcVxq2K1xIIra0yTnxJZUNX2/u6ilG3fWdhSIJvtNdYIShUle0uz4Zdiqf2T5ZyjYanri+hKQ5T+Ff3csADDzzA0K9ElGy26JLqE2v8Tcb8MuS1Lfo4yNb/zrJEdHDCdJkZH9eg4UsC51WkoDpmNDhDDYMqGNW8HY8TFi/DYYzz6ajEtCsTCjBujrsqS08NArRY0P9qFWJgT40qtFT2Xsdu+WDhAUEp7MRoxKQ5NKkEVQyKkDDntC8TXGzfxFCgq4fllRTeJo4rIHMkEMD/PvB+HvFhdGHhIQ/NIE154l0v6MTwyDulI0V1boaLDdCr6+RJTY6SJj1QxJZHshG2LhYqi/CfgbuDnxv9x+vE87zc8zzsZ/JPNDu5MKjn6WP6iPVe4bV0s7O5vc4mes1BNg6KQLQhnoTnEFSAZD82GeBCmtH6xMOvGqG1YRPdKxksnNh2X90vHq9XopjQtr0vCDc8JSfhNDNodWfJzlDH9HMxcSC5ZMIm3u9JheqvT5wBRVaY6Gqs7KQv8oz8C4NG5FncU7uD2/O0AZL04liZdZIcB089RDuJIdkKvOVcnei5jd4TgnEr1j0M6cWxNioWTSpCPnNAHr290/zu/aouu6sVY+DpoShVCYjD/kRxy5ueZ9/chlxrX+9/vdGjEIa2GlJ4DOT/3tLbbOKT3vAdOn4a6bNp3GCivigzLkj6gud4W8opOVToLJXvEtsRCRVHeBvxvwLd4ntfwPG8V6CqKsjHM6Q5gYafvbf8/QXKrUOuKyXI2UySXKaJ4UB1nkHOrBb/zO5GNL3oClD9Z0wpTZNpQ68hd3P3AbogHYSqW6nsv5yWwYuu5TeW2OLZ47PZNxxm+kyMoIQujpXRJRoiFyUAsbEux8ChT88T3G5R6bCWtiEl8oyMdprc6gxwgpW6Ctdg2XafdLvzRH3H97lnm7cWeqxAgy+ZxTnJwmE2xeWXkhi/mwuiJhd3o8cPuiM0IfUDDLV2JY8fB60rH6STSy0ceUGIO627SJWsZgGJ8QCdkn6ImrhFUwUgON5XnnuaG/7UvdtdgS/m4V68LsVAJKT0Hcmm/0eJu45CeeAIWFmBxcHaiZH9ZqfhiYXZAc70tGFpaOgsle8bIYqGiKD8DfA/wrz3P26im/Cnwo/4xDwIngEd2+Z5E0oflO3yyiSyqopJrK1S9MQo173oX3k/8BHz4w6GH9JyFCX+yls9jtMDcZ4fjrUozEAsHOQuVBLW41+sGVukKAbdYmNt0XK85zpBd2BZOpFiYSPgNTrpSLDzKmLTQHZWYOjgzKJjES7HwFsfzaMT6xcIpdOoxl1Z3G01JfvM3YWGBT7zx1QC95iYAWTWJlQBPdj08cHpi4aBO6SOS9bNQLWdEsVDvFwvTqig97TRkuP0k0sLPR473b4LC+nd+tSU2OIsRGZnB3LRsyfy6SWB+Zb7350W90+cM61hVXBXS2uDfBkA2GzRJ2mUcUtAcQwpOh4LVqohKKuUHN9fbSK9xo3QWSvaAkcRCRVFOAr8OFICPK4ryL4qiPO6//bPA1yiKchF4P/BGz/M6u3xPIuljq8Mn39Woqu2oU0bHdfnzj76dmf8TPn71sdDDgt3aop97h2FgtKA2ZKIvGQ/Npt8N2Xf1bSSnpLASgJ9rWvYaaC5k4ptzgAy/4Yk5ZOe9pbgkvfAhMpEQn6Ely5CPNDW1g+GEh4unNSkWSsBrtfxysc0bGSVFjD9r9tpoF7p4EX7+5+HMGR75SuGC3uQsVHU8BezaiNeT7BlB8yOjOHwxF4aezqO6YLnRzxHbEe/rmUL/NfzfnF2TbrJJpO12UTzCN6TSQgBccoUYNJUenJ8LUEyK30d5bWnMn1IydlZWmE+siztLOWBp8/fW8Md5fcAGeUDOdzbXhnRUH8b7Ek9z5t9DY21AObRk31mtC8F/eurk0GONRE46CyV7RvgKaAOe5y0CAy02nuddB147zvckkkFYXgvVAz0mRJq8G6eqDtGXn3oK/vEf4ad+CkIaVQD8jw//F773VddwVHis+gxfH3Jc2V/wFYLJWj5PrgU3xulwlITStMXibJBYmNV02jFoV9dI5PNUaFFsa30NSvJ+o4pqM/qh2lJckq4W+n4gFra34xiSTBaeh6k55LzwXf1gx7/eklEEtzLtWkU4QLaIhVNx4Qpaq11nLjc36NR1XBfe8hZoNuG97+XRL/97ZrOz3D11d++QrP/8syrXSRd27miT7J6a33TEKAzulD4KiqKQ7Yj5TRQ9sTA7QCz0hQTbKhNeoCo5rLTpknDCm6npGV8s1MSGVDEX/ve+mBHzm/La1TF/SsnY2ZBXqKGyaLhCLLz33t4hgViYjvXPeQNyvrN5WJOkYXw6eZNnS7C4+gJnd3UlyThYsVehAKXS7UOPNVJ5zCR41epgsUYi2QU77YYskew7ltIh210Xf/Jugmp8SCnWO94BP/Mz8Gd/FnrIh576EG+4+HZKvjHocju8fKPil3b0Jmu5nHAWMiaHoySSZsvvCBnv7waYi4nXrIpoXFLW2hSc/g5yRlIs3s0hnbRbqkuScLEwmRD3k2LhEcayqCXBIEIs9CfxjbrsiH4r0/A7UW4tF5vyO1Wu3hwhkvld74JHH4Uf/3HWHryfL17/Il97+ms3iQiBU9oqRzdokuw9pl9RYKR2J9FluxoW0RufTVfMMfR0/72C35wtx6CJpIVD0glf4gdu0rqfVVqMKEss+vlma34Jo+QQ43dCVlB4WfpOlgzg6maRt2EJt3B6wJw3IFcQ0Tq7zU43fdODacoS9sPAalsYGkrGCGXIegFXXZ+HSCTjRIqFkomhpnXJuutm2LyiU014wo0Rwv9of56H3wK//YEfxxyQUfeBf/kA3/cX38cJEx67/I2kOwoLXkSDk9oKcQf0gh9ormnkHA1T6+J5shvhXhOIhalE/8QpmxAp0bWqmOiU412KXn/pRt5fvFe70ROrluqS9CKchSmxaG87Uig+spgmZhIMNXxXP+OL1I3dhotLJpqGXwK6VSws6cI6srZyZeB5V6pX+NSVT+Fdvgz/4T/AqVPwy7/MYwuP4eFtKkEGyCbEuFOv7bLzpWTXmF50p/RRyToalhItFtqBWJjM9L2nyw2LiaaNQyIiHzmd3ZxRWCweDz226MeslGtyM+HQc/488yW4I3OCuwt3cjMDzcVLmw4J5hXpeP/f+4Ccn3taG9JRfRimIsaYYc3/JPvDalcYGqbTwxtoGZkpAEw5L5DsAVIslEwGrouluWQ3hMfntTTNuCg7DeMj6cs8fhJ+8sEVTv7GSX7y//tJnl17FoA//MIf8qaPvolTbo5H3gd3/8jPcaqV5HLCDr1exV6jaINSWJ+8GSRwFA+7G36eZDzYEWJhsGCzzJvQbFJJQlHpPy5wFg7rpN1SvUhnYSIprt2SDU6OLtUqtQTkIkqA0r54IxfqtzYNyy8X2+IAmQqcPuXBZYFv/Is38so/fCUv/r/v47fus6j87q9DLsejlx8FGCAW+uOcXBQcOCYtVDfa9TMKWS9GTYvuZGx7vlg4YCzS/VgOuyHD7SeRlhLdTC2V2yIWTodnmE35QmK5LseHw44zf56LJTh3232cmH4RAFevXdx0TKMu3GXpAZsEAdm8cBYOa5I0jEAsrNZlHu5hYNUT651SREZpgJETx5jy771kD5BioWQyqNepJSHHulPM8B091ZXwIOdltUGxrfHef8pzx80Ov/3Z3+bs75zlNe97DT/0Vz/EHfnTPPI+eNHsPfAN38Ap12Ah0w3tNFm2yxSaQGE9NyinCCfJbvNCJMNp+hlRqQETp6zf+May1uis3sRKQkHrPy7vl4yZQzKiWhoklfBY12QycBbKvkxHlVZlhXYMjHi4c6gnFjZksPStTCAWB7+HgKCEaNXsLwtsO20eX3yck9oUN706P/UtcOKpH+BH/p8f4a8v/jVT+hT3Hrt30znZlNjskGLhwWMqbYxufy7udsl6caxYeIUEgO11UDxIaIm+93R/88y25RxkEmnjkIhopqamMyQ3aMmFY+EZZsWpEwCUm9LpfthZWPwSrRicO3YPJ28TKYFL5cubjmnYYgMgncyGXiedyKC6UBvSJGkYZkyse4Y1/5PsDyuqjeauV0NFYfhZpWZDblpLxo8UCyWTQa2GlYCsul7ilU/4DrEQxwaOw7Vkh+NOmh/6nl/lyXc6fNz9fl5/z+t57Mpj3FW8i0dSP8rpKya89a2gKJyOlbDjsLp4YeAlK50axS1iYVCiaLbkrv5e02wL9+YgsTCni++kZq1RvSlK/or+b2QjPWeh1oVOiNDX7dKKQTKiB1SvDNmVZchHFXNtGYguMwwm8cGkXnJr0nOAbBELp4qiqcma1Z8D9eS1J2k5LX7oUw0WPzjD+/7V73DvsXt59+ffzTMrz/DqU69GVTZP07JpMc5Z0sl64JhaF8MdqU9gJFkliRWLjjGx6aB3lYHCZFCiKMegyaSluCQjxELicXR/qpJrQawU3uAkXjpGpg3llty8OtR0OsxbQhg8VzrHycIpABatLZmFgViYCp+DKIpCtqtQYxf52d0uZkKMQcOa/0n2Ac9jNdZhykmMtBkV5OaaTTkvkIyf3c9yJJL9wBcLcxuyw/LJPLShWg4Jcl5dZTkLD2PAm9+M8o538HW//ud83bPPsvy63yMbz5B76NWQy8H3fz8Ap9KzwDNcvvQk06df3HfJcrfG/VudhTExUa8NaZgh2T3NjiizSKX6d1mzmWARXaa8sghAYUDwfEJLkPI0zKQD1SpM9+eBeM2mEAvV/gYpvev4YmFLZhYeWWp+sxxDD9/ZTady0ICGdPXc0jTsoFxs89g0NX07XIDVRr8T8DOLnwHg4YtN9N/+ID/4yu/gB1/5Vp5YeoIPf+nDvOElb+g7pycW2nJRcKB4HjXNwfB2V4IMkFVSNOPQbTaIpQZfz1a66O5gQSnIMbSbcgyaRNqqi+6FzzVQFNJdhQoeRRvIRziNikWKNpRju2t2IdljXniB+aJwE5+bPteLF1hsbc4LbPh/p9Op/o3vjeS6GrUhuaeR1GqYfuHWsOZ/kn2gVmNVh2kvPAJnI70s9rbcMJKMH+kslEwEnWqZVgyysfWJdN53klWrg4OcraUXsJIwlyhBLAZvfzvU6/CLv8hcbo7cZ/8FnnoKfvAHhWAInM6fBmDh6jN912s7bRpeW0zWNjoL42JxaDakdX+vafr5gLreP3HKpkWuT82uUl4TpelFfWrgdQySVFNAZfCCu2OLiXZSGS4Wtl1ZhnxUCboCBq7VQQQ7/g0ZQ3BLE1YuVjh2O4oHa63+sSYQC1+Rvxe+4zt6rz944kF+45t+g5cff3nfOdmsGNMsW7o/DpRmUzQ/iuiUPirP3dvdAAAgAElEQVQ5TSwI65XwLqS24oSLhf7mmd2SAtEkMiwfGUD3uyUX2ypoEccWChSbUB6SySw5YPxOyOA7Cw2RQ7mECRtikIKcbj09RCx0Y9TUnc9FvWq1JxZWu7trlCIZA+UyK2koqeHl5xsJKqbMXTa5kUgGIcVCyURg+Q6fbHx94MwHGQ0hnbuWrwjBby4jOoXx+tfD13wNvPvdcOECvPOd4vV/9+9655w6dgaAyzc3hwwDVHx799bMwmCQDlxIkr2j2RFiYUrvL8nI+QG/VsukUr0OQDE7uFwnr+piYlQdvOBuNYTwk1T786ECkmnxGdqeFAuPKjW/g3rQaW4Q6bTY0W3IhfotTc8BsmUjQ5uaptCEtW6/mPz45cc4uwJTL3/1yPfJ5HyxULrIDhbfiZNTdy8WBpugVsQcwlZddHewSLS+YSHHoEmkrXqRmYVA77ufcsLnJADk8xRtWEM23DvUzM8zX4KspnM8d5zZ7Cyqp7BoANev9w5rtMXf6WCeEUbWi2Npg7PWR8Gu3MTxf4KmFJoPHG9tjTUdSrHw8vON9MRCR4qFkvEjxULJRFALHD7J9YEznxXiUNUaHPR+7fpzAMzl/c5xigK/9mti1+6Hfxg+8hF47Wvh3LneOadOiDD5BfNK3/XKfujv1szCnF+iaFav950jGS+2E4iFA8qQDbFNW2vVKNfEoqtgzAy8jqFlqCYJdRa2/JLS5IAw+YCE/xnabnQXS8nkYvpdAQMhehDBJL4uF+q3NI2mv6jb6nqemqLUgFVn8+/jZv0mz9Uu89AS8IpXjHyfbE6Mc/W2/L0dJE61Qj0BhjaGMmQ/c9CqRjgLVZdUSHKQ7ouFdlsu8ieRlupFNlMDSPtiYdFLRh5HLMZUJ0ZF6+B50TmYkgNkfp4LJTg7dQZFUYhrcW5TskIsXFpv2tjwo3fSmfDqBhDNH2txD3b4nZuV9fWL6e2uUYpk91RXFnFUmE4Whx/MBrFwl01uJJJBSLFQMhEEnR+zG0J+874QVA3p3LW8egmA2ek71l/8mq+Bb/92eOQRIRq+9a2bzjl5x0tQPFiw+3MQA2dhXxmyX/4alCxK9o6gDDk1oCQjlxcuQqttUfEF5GJhbuB18omccBaGiIVtf+Ef5SzU4kkUD1rSWXhkqfmd5YwQ0RnWJ/HBpF5yaxLqAMlkmGrCmrLZ6fPZpc8C8PAi2xMLg3FOlhsdKDV/cW3ERysTiyKbENewQqokAGzNRfdCxEL/eWjLMWgiaWuQGBIhr3u+WKgMF6eLXhJH8WSO9iGmfuFpFvNw7rb7eq+dTEyzlAOurjc56YmF2WjRKKekqCXAs3fmKDU3xDlVFZnDfdCsrgjDSikdvlG9kZ5YGHOgtYtGNxLJAKRYKJkILMt3+GzIDssXRHlxNSTofdkUu3Nzc2c2v/FLvwSqCi96EbzudZveis+d5HgNLjtrfdcrN4UoWWgrkFnveJnzHY41q/8cyXhp+p2HUwNKMrJ5IejUunXKTfFdFKYGi4VG0sBMglceLDS3GmLhn4hwFiqqSsKBtrfz0g/J4SbocJ4rhIuFmYyYxDe6cqF+K9NoC/GuzwGiKEx146xqmxdgQV7hQ6speHF/M60wskXxW7RkrtSBYvqN1YyITumjEmyC1mqDqyRwHOwY6CEZumm/ukFuWEwg3S4tjaHOQt0XE4ux4eJ0UREZmOWQjXTJwXPhpohJOldar2w6mTvBcg66iwu91xpdIf6lc+FRKCByT7satCrhGw5RbDQ7mLvIPpSMh5WyaNJYColS2kpPLEwCpmxyIhkvUiyUTAQ1v3lI0MQCIF8UQlBY96flhtgpmzu5ZSH24hfDX/0V/Omf9gdFJ5OctmIsaP07sj1noZoRJc0+hl8WFpQsSvaOpiN2zFKZfrGw5yx0bMotkUVYPHZq4HXyehFXhXpI2VcrcBbGokt+kg60PVmGfFSp+U1LjHy4WBjPGsQcaHRl+cetTFS5WMlNYWsudmfd9fH44mdIdeCBUw9GNyzYQrApYjny93aQBNEowSJtN2RT4nlm1UPEnVYLOx4uFur+89CWY9DE4baadDVIDBULxXdfTERn1wEUNSEoBhvckkNGucy8JtYT56bXxcITxVM4Kly/up6ZHswrhoqFMWFgqK0t7+gjmRvinKoJD5pyLDlIVk2xGVUyZkc6Pojoispil0h2ihQLJROB1RCDX3aDFT8/fQKAandwdtO1thDv5mbu6n/zW78VvuqrBp53qpvmRqKzaWEH67u0hfhmJ0EgUtVCHI6S8dH0OigeJDIDuiH7D8ua26TSESJPsXRy4HWC0vFqSM5kqylcO8PEwoSr0EY6C48qQYdzI6IbMuk06Q40HBkofysT5QCZUsVCLli8u57L41c+w1ctQ/zBh7d1n0Q8RcwBS2YTHShBY7XIsWFEsmlxjTCx0Gs0fLFwsNNdioWTS9v2NyZDhOCAtP/dF1PDf2/FhJgflRtyA/tQ4jc3gS3OwtmzACytPN97reGP88ManPScZWv9EUqjsNHsIAWng2fVEptR08XBa5itJLQEKWLSWSjZE6RYKJkIak3x4Mr5Lj4AY+o4ANWQcqxlaqS7Si8PaFROKWIydmVLk5OeszC5+aFtFMTOj9mUA/Re0/Q6pLqgpPo7UMbUGKkuWF6Lst8RLJ8enPOSD9ygIRlRrVYgFkZ3uky4Ci1FioVHlcDRnIsqNeyJhTIn5lYmSiws+U6f1YZwb8yvzGN2LZFX+NBD27qPoihkuwqWJ39vB4kZRKOEPGO2Q08stAcv0Nt18bquDt68SmfFb64hBeSJIxALE2q0WKj7+cnFETLMiknxeypXduYyk+wx8/PM+0uZs6WzvZdPHrsbgMXq+tqj4UfvpOPRWZUFf9OiusPv3NxgdpBi4cGzYou5QunY7SOfYyi6/O4ke4IUCyUTgeWXAwYdbwG0WJxsG6ohnbuWYzZz7STKhpLhUTidFFmICzcubnq9l1mob14cZPwMqVpHdqfca2xfLCTkO812NSylTYUmubZCTB1c2mMYwg1aDSkdH1UsTLqqdBYeVdptTH8jIrLUMBALPRkKfisTiMVpo38xP+WXDq5Vheujl1e4zeYmAZmuSl2RuVIHSc91HNEpfVSyvthnhWw42oFYqA0WC/WcmJPYcsNi4miNLBaK776YC4/ECCimxe+pXL465EjJgeA7C0/qs2QS6/nnJwxRLbXYXK94aXhtNBfiQ34fBV9ErlQGV8sMw/QNGVNuUuR5hzT/k+wPq23xfZSOnR75nHwsQ1U6CyV7gBQLJROB5XeazG3JDst3NKrKALHQcVhOOcx5mf73hnAqKx7Yl698cdPrFb8MubglcFYtFMm1wJRi4Z7TpEPKCRd/c45GTelSVlsUuuEZQPms7yxshDQ4aYv8sWR8uLOwrbjDPrZkEllaouZX/UW6k6VYKAEarhBq9GT/b2VKF4v31RURXP/40uMAPNw6BidHKzPaSNaNYSkyK/UgMe1ALJwecuRwcv4mqBXSvda2xMJdD9m8SmbyKB7YcgyaONpBPnJEMzWAtCa++2L+tqHXnMqIOepaWToLDyPe/HkulODczOY89ZOGeBYsddfnpQ066I4y1PRQ8Mehinkj8rgwgmZut6t+nnd5Z6KjZDysdsX3MW0MbtI4CCOWkc5CyZ4gxULJRFDzHT5BuU5AvhujOsBh0b5+lZUMzGnbzxM6XboTgIXrW5yFNZEhkd+6OMjnMVpgOrIT4V7TxIkUC7NeHEvrUo51KbrheYOGHyhfbQ5+qLZaoqQwmdAjP08CTYqFR5WFBZZzMKWk0dSIBhS6LsRCpNPrVqbhtYk7ENf6HSCltHhmrK0tAfCZK5/iuAkn73s41CUdRTDOSQ6OYHFtGMOdXsPI9sTCwZEqtp/ZrMcGP48UTUPvSLFwEgnykRMDxo2NPMxJ7l6FszPDO6eX8iIaZ7UmBZ/DyNXLT2Ml4dyxzd/liZzvLEy2oS5+F7bSJe0MX6oX/HGoUg/pqD6EoIriZEpcpyqF5gNlxRXfR1EfPebCSBoys1CyJ0ixUDIRWH4e1FaHj+HFqcb6F003Fp4BYDa1/RKhU7P3ALBQfmHT65X6CkYTtMKWTCrDINeCmsyQ2nOaikPKDR+2cl6CWsylknApEu4KzPu5k2aIk6PldzZNJqJzYhKeSkuKhUeTy5e5UIIz+hDnVzxOuossC73FadAh3R0s/E35C7m16jL1dp0v3niah5ZAecX28goDsiSwYnLcOUhqfrWDURju9BpG1m+SZnVCxEJbLP7CnIUAuqPQ8OQYNGm0W/5cI6TEPODbY/dz8XegMEKG2bGiEJ1uWFIsPHQ4DvOmaGCysRMygB7XmfJSLBrAVVFC3lC6pN2IzUqfQlE40Co7bGrTEwt9wdKs7syhKBkPq6pNoa2FRikNwkjlZQm5ZE+QYqFkIqj5uYRBe/iAPEmq8f5F0/LVeQDmsse3fa/8iTvJN+GytbTp9XJjjWITKGxxK6ZSGG0FU5Fi4V7TVBx0L3zilFWS1BJQSUFBCy9BDzLoqs7gxVmrLX5vw8TCpKfRVuWi/ShiXr7A9SycnToTfaCikHY1GrLRzS1NQ+mEOkBKBbEAW63d4HNXP4eLK5qb7CCvEMQ4Z8UBR/7mDoogdiRXHINY6AuOtZDqBLvhi4URTQ50R8GWpekTR7sVOAujy5B5+GE4e1b8MwRj6jiJLty0Bzdwkxwgly4xnxd/Tzd2Qg44GZtiyQCWxPqjoTmkveGCUcFv+Fhp7awE1XSFIeN2vxS6at7c0XUk42FV61DqRruNt2KkC3Q1aNZkF3TJeJFioWQiCDo/bnUW5hWdRgI67c25hcvXnwNgrjh6J6kes7OcqsJCe/PDstyqUhgkFgI5N0ZNOov2nKbqkIrYZc2pKawkOCoUY+E5c3m/DDmsdLzV8cuQk0OchWi0VW/Yx5ZMIBevPQ3A2RMPDD027cVoai6uJ4XjW5UoB8jUlBAL1+qrvbzChxaBl798R/fKqimacehastzooAieHUZ29w1OdGMKxQMrpJux3RQOeD1i80p3VCkWTiCtwFk4pJka3/7tMD8P+fzQaypTU8zU4UZ7cCaz5AAJ6YQccFKfZdEArycWuqQZLhoVpsVap9Le2TPB9JokHDjmZ+SZIc3/JPuA67KadJj2omOQtmJkRNWbWZObBJLxIsVCyURgKW0SrtK3+5r33WPm6uaub8vlKwDMzdy5/ZvNzXGqCleoblr8VzomRZuBYqHhJajFHDxPCkd7ia15pIhwFm7IdCokwyfVQRlyVWmB2y/wbEcsbEmx8EhysSJKhc6cHC4WZjwxmbf9343k1qOhhjtA8tMnUF1Yba7xmcXPoLrw8vTdUBw9j2gjGU2MczKE/uAwPfF3PZfIDTlyOKqqkemA5Q6uTrD9JhipRLhbXrqbJ5OgDDkRiy5D3hbFohALu7LRwaHD74ScVOKcyp/qe/tE4XZaMVhdugiuSyPGaGJh4CwMqZYZhqm0Mboaeb+JZFjzP8ne45kmK2koqdtr0GkEjRvr8ruTjBcpFkomgpraJev0L8TyceEeq65uLhm+Zolw3rnjw0s2+igWOW2qtBWX637mi+u5VJx6uLNQSeIq0OjIJid7SVN1SRFekpGNrT9ci6nwhXivDDkkDLjVFYu2ZDL6YZ1QNNrD42QkE8iFltiAODugVGgrwWRe/v2/dWlobqhYqJamKTZhtVPlM1c+xUtuQOarHt7xvbJ+OWq9IkvFDgrTa6F3lYENbXZCtqNiMbhBSSAW6hHPI92LYctIjImj1fY3Joc5C7fDzAzHGnDT25lwJNlDfGfhmcJdAxunnTx2FwBL1y+CbdOIQ1oZLiSn4joJByruzuYgptbFcGIYBdEcp9qUuXcHRePmVVoxKMWMbZ1npMWaRwq9knEjxULJ4cdxsDSXrNc/Kc8Hos+Wzl3LTbGImr393u3fT1E45Qm3wEJ1AQCrbeHiDc4sBAxVOD2CDomS8eN5Hs0YpCJ2WXMbytSLmenw4/zsSzMJVPt331tdP7MwFS0WJpU4bQ28Ae5EyQTjeVxQxYTrTGlIZiGQVoTjWYqFty4NzQt3gBSLTNnwJe8Gy/Xru8orBMj6m2SWKcuNDgpT7WB0x7dTlHU0rJAoE9vvkqynwqM1dGLYmnwOTRrttu8sjI9RLPTLkOtql3pIh23JwdC8+GUuFeDc7H0D3z95XDRYXCwv4FmWEAvVIXmWgKIoFNoqFQZHGQzD1LoYboK8n58q1zIHx+p10VxzOtG/1owiMEGYUuiVjBkpFkoOP5ZFLQk5+h+Y+ZQYTLd27lruVog5MD3db/MfhdOayCG6XL0MQNkWwkGos9B3tJlNWfaxV3TcDp4CKSVcLMxuKAkr5MLFQlVRyZEUzsIBncN6zsKIxRlAQonhKeB0ZHObI8XaGhfzDnNOui8ndRBpVez8S7HwFqXbpRGHTJgDpFik1IA1VSzkHtqtWJiUYuFBU1OFE2dc5NwYljY4c3BdLAwveU4TpxGTkRiTRq+Z2jjFQlVlxs87u9mQ7uPDxLPXnsFTBjc3AThREGuWxfpVWlYFT4G0Ntpvo+DEqaiD3cmRuC7VhIdBEkMX65uq38BJsv+srIoYrZK+vTzcXsVUpzb2zyS5tZFioeTwU6thJSCr9i/E8mkR6FrdKhYqdW5raqjKzn7ipzIi/2OhIsTCir9TE5pZ6IuFNdlBbM9o+nlwesQuay61btsv5mcjr5dXdeEsHCQWOmLCNYpYCNBqyIfzUcK7dIkLJTirzYx0fDCZl2LhrYlTt2hGOUB0nanW+rPo4WsxeOlLd3y/rN+gybJkCP1BYcacgRuYOyXrxbG0wZmDtu8+0/VwsVBX4rRiyCZLE0a7I8TCsToLgWOamAvdqN8YcqRk3zBN5j2xwXNuerBYeNLvRrzUXqVRWwVAH1UsdBNU4jtoclSvYybBUFPr7rSQ5n+SvWe1LGK1Stlj2zovyGI3pdArGTNSLJQcfiwLKwE5tb/ZRD4jdl6q1uqm16/FW8x1t9dJaiNB8PDC9QsAlJvRzsLeA3Ztue89yXho1oVrM9JZqK83NSkU5yKvZ2gZqikGioXtQCzUo8XCpCo+S7spS32OEisvfImKDmdzp0c6vicWypKvWxK7JkS7KAfIlCs2u/IthXO3vxSSO29oEIxzlgwyPxi6XcyEh8H4BJ6sksQKcQYGjZN0PTzDSvejEJotOQZNEr1mahGdrnfCTELkl92syw3sQ8OFC71OyGHOwkAsXFRM7JoY39Px0dYyBXQqCQ+22WixVVmhHQNDS683/3Nls7aDYtW8BkDJiDY8bGVd6JXfnWS8SLFQcujxTJNaYnOn24C8IXZeqo11h4XX6XAt7TKrDC8fDGNu5k5iDlxeeRbY4Cxsq5Dun9Tl/MVbrSp3cfcKuy6+g5QWvsjOZaZ6fy5On4y8Xj6RC88sdIVYmBgiFiYCsdCWO3lHiYsL/wLAmWP3jHR8MJlvNGRWzK1IoyrcImktfFFXQrz3ikUP9RUP7ep+2bTYsJJi4cHgmWbPiTMuskqSRgKcTn8ZYbPri4XpfN97Ab0ohJp0m04SPWdhYrzOwpm0cMXfMK+O9bqSXTA/zwW/sjTMWWgkDTJujMWsR+OKyK5Lx0cTkgtamloSuub25iHmqjA5GLFML3bFREbrHBSrlhD4p6dObOu8nljoNbctGEskUUixUHLoaVZXcVXIxvubTeTzYkJUtdcXTauL83Q0mItP9R0/KtrcCU6asGCK7Iggs7CoZUFR+o43dL8LlcyQ2jOaDRG4HCUWZjPrHZCLx6LzKo2kEZ5Z6Iqg+WRE2RdAwi87lM7Co8WFG88AcPbUV450fNofm+qWFG9uRRrWcAfIlCZ+I7vNKwTIZMU4ZzVlCP1B0Kys0NWEE2dcZH2huV7td4LZfsMtPRMuFuq+WGhbcsNikug1U4vodL0TZgxRWXHjxgtjva5kF5w/z3wJZpJTFFKDm1coisJJtcBSDhqXLgLr84thFGK+0LeyuK2PZVauA2AkcmiqRs6JYe4k+1AyFlYaolKuVLp9W+f1xMKEB/XR1iRv+uib+LXHfm17H1ByyyHFQsmhJ3DrBR1sN5L3S02rGzp3LV8RC/05fbS8sYHMznK6CpdtYQfvlSEnBotHuawQJmtbyqEl42MUsTCXWw8ELuRvi7xeXi/SSEC33P+dtbwgszB6kpbQhFjYkqVfR4oL1iUAzpwZzQEW7Pw3GrLB0a1ITyyMhYtHJzXxjHjVArsWC7P+86belFmpB0GtLOYFRnzn1QtbCcRCq9JfnWA7I4iFMeFMCxz4ksmg7TdTSyR2HpsziGNTorLiht8sQXLwePPnmZ+Gc8deHHncyeQxFg1oLD4PQDo52jhTSAixqHJzm2KhKcYcwy9BNtw41YQHLekuPAhW22IML83csa3zemJhSMXUVprdJu//l/fz3i+8d7sfUXKLIcVCyaHH8kN+s8n+vJ78lC8WttfFwmvLYjduzji+85vOznKqCmW3Tq1VWy9DThUHHm74nXfNuiwB2iuaTVHqm4qFl+tkDfE9JBzQB5Stb8TIBG7QfidHyxUh0cmIewEkNeksPIpc7N5A8eCumdHKkDN+6c6oYqHnefzqY7/Kp698esefUXJ4CL73dCJ8c+EN3v38vx+C197Mwdmzu7pf1t8UsdpSLDwINjpxxkVQOWENchY6YtGuZwfPPwDS/vPOlhsWE0XL/26TyfFmFh6bFnm7N02Zo31YWHnhy5T14WLhidxxzBRcv+6LhRFd0DcSuBUr5e1952ZNVEQZfpxSXkmNLDhJxs9qR6xnS7Mv2tZ5m8RCc3jVwWW/gefF1YuyOZ8kEikWSg49lu/Wy+n9u+pGSWQ6VLvrYs3yyiUAZkujNScYiC8WAlwxr/TKkAvpwZP1nJ+dWLPlrv5e0bTFwliPKPXLFYSbtNDRUAaUi28kHwi8tf7S8RaiDDlwDoYRvN9uyQftUeJCvMYddpJkbLQmFGm/a3bDHq0s9JMLn+Rn/+Fnefsn377jzyg5PPTEwohSwlRhmm+9CMqDrwB1d1OvbF48b6y2zEo9CIINptyADcyd0ssK84XIjdi+0z2ywUmQm1qXC/xJot3185HHXIacmb2dTFt2Qz40uC7PlkUG+pnSmchDT04JkehiQ7hCRxYL02ITqVzZrlgo1liGv74xVF1E9Eix8EBY8SwybUiltudcT8aSJNBGFnqfX3sOAA+Pp288vZOPKrlFkGKh5NBT80u8glD3jcSNAun25s5dyxVhwZ+77a6d33R2ltO+7ne5cplKkFmYGdzK3iiKklezKR+ue0XTDpyF4WJh1hcLi060yAdgZIVYWK0PKkPuknAYKjgmfDFJliEfHdymzbNGlzNuuItnK8FkvjFihtw7n3gnAOevywnaUWBdLIyY3E/5Gbq7LEEGyPrPG6srNykOgkAsNPTBuWM74XRJzFeeu/T5vvdsv+GWHtExN3DS2yNuWEgOBy1ntMiTbTMzw7E63GjJapdDwZUrLCXFd327EZ1Fd3JOOM8vxMUGeTpik2AjhawQCysDqmWiMP0GkUZGnG9oGeksPEBWlSaltrajcw1VH9lZ+PyVJ3t/fmrxn3d0P8mtgRQLJYceyxYPrI3NK3ooCkZHoeptEAvrIk9o7sRoJYQDSaU45YjJ20J1gbJ1k0QXUvnBTVNyfjm0KcvC9gzbz+dKRWT75Hy3YJHh+T95v2TDHNDBtoVD0okWCmFdLJTOwqPD1Qufo5GAs8m5kc8JJvON5nCn15K5xEe+/BEAnq+8QKsrc4EmnYY/NkU6QGb8DN2HdtcJGdbjFizHHnKkZC8I4kaMzM6bqG3l/nu/DoCnr/Qv2myvQ8yBmBoLPV/3S+ClWDhZtH2xMLEHYuFMHW525e/hUDA/z5L/eDhhRHe5PTErxMKLfgR3OqIL+kYKhnjGVKztNVo0/TWW4T9X8okcVhKcihSaD4JVrU2pO9zwMAhDS4/sCn1+4anen598+h93dD/JrYEUCyWHnprv1guEoK3kOxrVDZ27ltvCKTZ7x/27uu/phHjwLlQXqNRXKTZBKQx2G2WmZlE8qHWkw2yvaPruvVSUuyKeRlM0imdeMvR6Qb5HtdX/UG3hkHSHD49BmWq7LRftR4ULz34WgLP50Z3JwWS+0R7+9//d//xuul6Xl1wHR/F4rvzczj6o5NDQEwujHCBveAO85z3wbd+26/sFGZmW29z1tSTbx2yISoNxioXnvuq1qC58yewfD2ylQ2rI5lVQAt+QTW8mipa7t2LhDaWO53njvbZk+8zPc9UXC4/novPUT+aF8/CCLxbq6RGdhQWxwVlpbE/kM1uBWCjWPMHcuFbuj0SQ7D2riS7TbnReehhGPCuchZXhkVjPr1xEcyHXgqeW+h3tEkmAFAslhx7LL+3Lbuh0u5G8G6eqdXv/fs0xKdkKiexou3FhBKUCl6uXKTfLFJpAYXDZkZLPY7TAdKTDbK9YFwvDJ9WKovCe/+U9/Odv+m9Dr5dPid/HIDdoS3FIuiM4C+Pigd5uy0X7UeHikthtPTN338jnpPXRxMK20+bdn383pyyNn/Z7m5x/7vGdfVDJoaHREo7SSAeIYcBb3gLazsqLNqKpGqkuWJ50pR4EtS1OnHGQyhW524rztDKgwQld9CGbV7pfAm+3ZI7lJNF2RT5ycpv5ZEPJ5ZhpqrQVF7Ml3YUHzvw8S77mN1QsNEQn62VfXEwPqqoaQMGvcKq0tpedbrbEHNgoiHiLvB+vMCg/VbK3tDtNakkoqTvbPMhnSkIsvHRp6LHP1xc5VYUHrsNT7StyU0ESihQLJYeeIMQ9l58Z+H7eS1KNOb1/X9YazLbiu75vZuYEpQYslC9RaVUp2oSKhWSz5FpQ86RotFc0O8K9lxoSBP6ml72JVyZRNY4AACAASURBVJ565dDr9ZyF3X6Bp6U4JL3hw2PC75bcakuR+KhwYU10Uz9754Mjn5POinFhmFj4kWc+wjXrGj/2GYf7XLH58cwXP77DTyo5LATf+6jlYuMg29Wo+42YJPtLIL4ETpxxcb93jGezHZq18qbXbcVBd4aIhalALJTVDZNEyxcLE/qYxUJF4RhiriSbnBwC5udZKqgUUgXS8ejO19PpaeIb5p/piC7oGylMC5Gxsk1x2OyKNZZRnBX/mxaO6aopfzf7zer1FwAoaaM1tdmKkZ8RYuGFC5HHeZ7H894qd5bhgVaBcqzDkrm4o3tKjj5SLJQcemq+mJMd0A0ZIE8KK+HhuEIwXE52mHOiH8Yj4Tc5uVy+RLlToxjhLERVMboqJtLpsVc0fUFOH9MOfD7pOwvdBnzyk7BhV62luCS94Q6gZOAs7GwRiT0P3vhGeOc7x/JZJfvHBXuRuAOnzo3eiCKVEeNCfUjDiXd+9p0kXZUf+jzc8zbRCfn80r/s/MNKDgWNjvjeR3WAjIOso2EpUiw8CMy2WIzn/EYz4+I+425cFc5/7mObXrdVB33I86jXZEk6CyeKtuc7C8ctFgIzmtgQvdnYXsMLyR5w/jxXp+KcyEXnFQKoisoJb/33kM6NFndQmBbVUBVne2NAUBFl+M7EvF/FZVr9zf8ke8vq9UsATCd31jzL0Au0Y9B69nzkcSuNFSy1y512igfmXgrAU1/4WOQ5klsXKRZKDj1Bx8dsYvBkKq8JYdC0VrHqZayEx5w6WsZHJHNznKrCYn0Z22tHliED5JwYNbl42zNGdRaOSs9ZmPDg1a+G++6D3/xNWF2lpXojiYUJv9lKn1h45Qp86EPwH/8jrMoJ1yRx0VvlropCbGr0EkMlkyHdhkY33Fn8heUv8NiVx/jfn1Y49hVfjfHdP8DxGpyvXx7Hx5YcID2xcEQHyDjIenEszRl+oGTsmP4GZuDEGRf3nfoqAJ7+8j9tet1W3aFioe7nZdrS5T5RtD0RoRNEmoyTmZQQfaSz8ICp1/EWF1lKO0NLkANOxtZjl0Z9rqT0HIkuVLYZh2S6TVQX0n6UgZE7BkC1Lhuc7DcrKwsAlFKDY7eGEaxrzIVnwXVDj3uhIhyMd8ZneODerwfgySf/dkf3lBx9pFgoOfTU/BD3XHKwLTsfE+JRdXWJawtfBmAuOYYsodlZTlfBRQy4kWXIgOHFMWNy8bZX2P6CPKXvzJ6/lV5m4Q9+D/zUT8G1a/DTPw0nTtBSXZLKLsTCz4omGdTr0l04QXTdLs+lGpxtZkAZnlnZI50m3YFGRMOJ333idwF466cdeNObIJHgnrbB+YSJFzGpkxx+Gn5X4rQxvoYXw8gixcKDoufEyY/XWXj//d8AwNNXn9z0uq156ERHqwRNEOyObLY1SbS8LjFHuMnGzbGMEH1urF0Z+7Ul2+DCBWpJqKvdoZ2QA05k1jci0hE53RtRFIVCW6XC9uKQTJoYHQXFn/ME2YWmvb3sQ8nuWS1fBaCUPbaj83tiodeEpaXQ456/+jQAdxqnecnXfhcAT12VVS6SwUixUHLoCULcQ52FCd8htrbM8qKwXs9mxjCJn53l1IZGuUOdhaSoxV1cTy7894Km79oal1jYcxZmNHjHO+DqVfjgB+Ghh2hpkBySKwPrboDWVkfZ44/z098EH3w4Db/1W1CTHSongUvlF+iqcEbd5kQtnSbTgYY7OIZgzV7jj7/4xzxUyfLysg7f/d0A3JM5jZWAq09/ercfXXKABI7SdEgTrr0gSxIrAXS7Q4+VjJeeE2fERfyonHnpNxJz4EvW85teb2oeOrHIc9N+FEKjK8XCSaKNQ2KPpowzeeFiu3lTutcPlPl5lvxp6yhlyAAnC6d6fx6WcbiRQjdGRW1v6+OZShuju745nvfFwmqrGnaKZI9YNa8BUDJ25lrviYVDcguff0F0P77ztnvI3XkPd5oaT3VkZqFkMFIslBx6LMSDLxMfPDHvZc9VrrF8/VkA5gond3/jLWJhZGYhYKhCOKoPaXIg2RnNrhBixiUW6jGdmBqj2vS/5FQKvvd74ZFHaBWyJM++eOg1kinxm2x3N4tEC08+ym9+NfzYtzhcb5fh3e8ey2eW7C0Xn/8cAGf1bY4fuu47CwdP0t/3hfdhd23e+j8t+Lf/VnTGBV588mUAPPP4X+/8Q0sOnEAk3s8GJxlViIWe3IjYd2q0NjlxxkUioXO2nuLp2Ibyv04HOw66kog8V/fFQtuRYuEk0fK6JJ3x/o4CZqZEht2NsnQWHijz81z1p60jlyHP3A1AzIW4NnrDxoKboBLbXhySqXXJO+v3MFJ+N+S2fLbsN6uWiAyYLo4mKm9lZLHw2jMA3HmHmIM+oMwyn2vRvHF1R/eVHG2kWCg59NTUDmlHQ1MHl4XmdfFgq1ZvsLwmdlDnpl+0+xvPzXF6gwt/qLMwyE6sy4y6vaDp+M7C9BjyKBElG0bS6HW2DLhSvYLtNEkmR3AWBmXIG8XCbpfH1oSd3/Ja/MLr0vDrvw4t2fzmsHPhki8Wls5u78SgDJl+sdBxHX7vc7/HMS/Ndz6NKEH2uecBkRVz/qJ0Fk4yDd/9norr+3bPbCyNo0LblLlS+41w4kQ7/XbK/eptvJBzqN8QLg/XbtCKga4MKUPOCKHa7srnzCTRVhwS7t6IhcdmxDz4hu9WkhwQ8/Ms+dPWUZ2FJ06Izep0d3vL9AIpKvHtWVXNmIPhrW9G9CJ6utL4sN+sNFYAKJV2ZnjpVUyliBYLq5fIN6F45gEAHph5CY4Kzzzy5zu6r+RoI8VCyeGm28XSXLJe+MQ8nxGlX9XaCtdMsSsyN3dm9/eemuJUfV2gLLY10MMXg0ZclEnXVpd3f29JH01HCDH6GN07+WR+U6nF3z33d7zsXS+j63Z5/bnXDz0/4QuKbWeDSPTMM3xiVvz7udI53nNfk/OdZfjAB8b2uSV7w8VrIvP0zMkHtndiTyzs39H/2LMf4/ny8/wfX1BI3n4HfO3X9t6754FvBOD8SnTnOsnhpuF1SHcYu9MsimxMjD1WRTYv2G9MrUvO3Rux8L7iWTwFnvE7IjdrZQB0NRl5nu43QbBDohAkB8dHnvkIZ37nDGt2v7DfwiHp7c1SLHHbcfJNuNGQY8SBMj/P0kkh4ozsLJy+EwDd3aZYqOpYSeg2R2xy4nmYcQ+D9fGlJzi50qW836y2hEOlNHN6R+f3nIWZWLRY2L7Oi8qg3C0crA/cK/Jyn3zq73Z0X8nRRoqFksONZVFLQs4LL8HJ50Qzk2p9hWVbTIpmbx9eQjoUVWUme1uvRKQQz0U2PTD8BixmWe7i7gVNfxGUzIzHWQj0nIWu5/Jf/+m/8s0f/Ga6bpe//O6/5C1f+Zah5yf8zswtZ8MC7bOf5ZOn4Hhsivd/+/txcPnZb4nDr/yKzBc75FyoPk+6DcfvfOn2TozHhViobP5+Xc/llz75S6io/Og/1YWrUF1/7J4wTpJ1NM67N6AjO6lPKg06pJ39nU5l/VgOq3pzX+8r6XfijJP7Tj8IwNPnPwGAbYnFo65Fi4WxTI64E56bKjk4/vL8X/Ls2rN8fvnzfe+1FZfENgWhkZmZYaYON9uyUcWB4Xlw4QJXT4h566gNTk4awlmWNraXg1uICdNC9eZopefdhkUjAYa6boToRTt522uUItk9qx2TRBeyx3bmLDyWFnnbS3dMhYqFbafNFc3izpoGMzMAPPCKbwPgqeUv7Oi+kqONFAslh5taDSsBWSV8opw3xOBYbZRZbpdJtyF34s6x3F6ZO84pS7gLi4lokSrnW/drletjubdkM7bbJtEFVR897HkY+VSelcYKr/vQ6/iFR36BB257gH/+4X/m9fcMdxXCxszCdWdh+YlP8KUZeNXtr+Thkw/znfd+J391V4dHnefhT/5kbJ9dMn4udJY5swbK6W3u6ioKaS9GQ3XwPK/38u9/7vf51JVP8eMrL+L2mgI/8ANbTlO4R53hmSkXvvSlcfwnSA6AhtIl7Qzvnj5Osv7mlGWu7Ot9b3k8DzOx2YkzTu7/in8NwNPXngLAbgjnu66lok/UdfQO2N72mhtIhnDzpmh8tmFc3y5fvPFFAJ5de7bvvZbi7pmzMBALbzjm8GMle8PVq2BZLE3FURWV20ZsvjibnUVBIZ2f3tbtCv46pXJztGYVtTVRCWVo6/PqdDyN6kF1m41SJLtnxbMo2aAUizs6/4HbHkBB4XOnYvDCC9Du/w4Xqgu4CtxJsWeAuevYOdKOylPOsty4lvQhxULJ4cYXC3NaePlvviC6RlWbVZapMddQUSLKhbfF7Cyn1hwAiqnowdtIi/dNUzo99oKm1yHVRTQiGROBs/Bvn/tb3vzSN/PpH/o0d03dNfL5iZRfhuyuP1w/fekTeAq86pxY9L39G99OXI3ztm9Wcd/+S+DKbtmHkWa3yYJS4+wqcHy0UqGNpD0NV1l3mS5UF/jZf/hZbs+e4Jfedxm+4RtggAh5T+kcVw0wP/PIbv8TJAdEQ3VIe/ssFqZ8sdCSmYX7iWPVqG9x4oyTu178ShJdeNpeAMCuC1dYKjbkuaeq6F2wB0QhSHbB7/0efN/3wZNP7uj0jtPhyzdFvMVza8/1vd9WXRJ7JRYeO8axOtxUbFxPzjsOhPMiYuRqxmU2Oxuavb6VuBbndOE0pfQ2nYVJkateKY/WqMJcE5VQQYwS+Hnebhwz5sqs7X1mFZtSSwVtZ/OJXDLHi4+9mCcMCxxHCIZbeGHlIgB36usuV1VReYk6x5PHXLwvhLgL//7v4W1v29XGiWQykWKh5HBjWdQSkI0QC42iLxa2Ta7Fmsy1x7jjPzvLvTc8kl2Y1qMf2rnMFABmTTo99oImXSEWxkfvDDeMu4p3kYql+IP/9Q/4g9f/Afo2GxQkUmKC1Qq64DYafNIRD+dXnXqVuMfUXfz4gz/OE3Muf8LT8Ney8+1h5Lm15/AUONvO7eg3lvbEOY1OA8/z+LG//jGstsW7vG8lV+9uamyykXvuegiA+af+584/vORAEWLh3mTYhZH1nexSLNxfrGBxrY3P4b6RmBbnHjvNlxIV8DzshnCFjfJsSncVGsioi3Hy0ZXHuPsn4MZ8fwnxKFxcu9jLNH6u3C8WtlSPJHu00aDrzLRjuIo3MC9Rsg/MzwOwpNVHbm4S8JHv+gjv+rZ3beucgm9aqJRHy043K/54lshtej1PUjTJqFYHnCXZK1ZjbUrd3a1xHjz+IJdVk5tpBpYiP//cPwNwZ+nuTa8/cNsDrGTg+ic/1n/RlRX4nu8RzRoXR3OtSo4OI4mFiqL8tqIolxRF8RRFeemG188oivIpRVEuKIryhKIo9+32PYlkI45ZoZFYz2caRL4kXEArXZObKYdZwo/dNrOz/OLH4bPvgZwRXQ5gZIWYWKuXx3d/SY+m1yHlKJG5kdvlv7/2v7P0M0u8+WVv3tH5SV2IhT1n4Re+wCdv98iR5CW3vaR33M+/5ufJJwx+7l9B6+3/l9yZO4RcWBWTqjPx2R2dn1ZEhlmj0+DDX/owf3Pxb3jjS76Xb3nvI5DPw7/5NwPPu+cOkVF2fmFni1HJwdPQ3J5YvF9k/EZPVkPmke0npl+2l9vgxBk398eOs2C41C5fxLZrAOix4WKh7qrYUiwcK//kPMtzU/DxSx/f0flPXX+q9+dBYmFb9UjslVgIzCjid3qzLiteDoT5eVwFljvlkZubBLxs7mXcM33Pts4pZMQ6pTJihZPpZ94GjTECDEXHTCLFwn3EcR3KcYdpd3eu9QePiznlEyeAixf73n9+UYxJd558yabXH7j36wF48ov/0H/Rt70NVlfFny9f3tXnk0weozoL/wx4FbD1F/Iu4N2e550FfgV4/xjek0h61KvCpZfbsuu1keTUDMkuXPTEQDanFcb3AebmKDThgetAIfq6ubzITjQbUizcC5qKI8TCMRJTY0zpUzs+P3AWtl2xQGs9/ik+ewK+unA/MXXdaVRKl/hPr/nPXCrA7/IEfOpTu/vgkrFz8booFTubu2NH52d8sXDh/2fvvOPbKu/9/z7akmV5b8fbzk4gIQkjzLJ3SymjdEO53FLooLS9v97ejtsJLYXectvSRaGU2XIpu2WHkb2H4xU7tuPY8ZJkW7LG+f3xSE4cL9mWdGTleb9eecUvnSPpC1aOnvN5vt/Pp7+FO1++k2xbNvcFLxSdBbfdBrbxO5HCNwP7vG3gcs3ovSUaoqoMGsGmxFcstIc62QeGpB9ZPHH2C0/i4ztxosniLBHQtmfTSwyFfr9W09SboNagniFdIGZ1nYh0+IVYsr53Zp6yYbGwMLVQdK8ft1EoOgtj15WcaxDr1s4BmYisCbW1dGaaCaiBaXcWzoT0kId7nztCsdAt7rHCNkph0vQ2KRbGmV5PL6oCWcrsutZXFQmxcFMh43cWdtejqFBafcqox5eViSmXHR3bRzc0vP46PPwwhH0UW1pmVZ9k7hGRWKiq6tuqqo7qO1UUJRc4BXg09NAzwDxFUapmemx2/ymSZCRs3h42cx8Xi4U0D9QaxZdagSUnegXkH9NlNIVY6Ah5J7o88uYtFgwpfqyxSg2cIaZwZ6EqOgs373oVrwHWLrxozLl3rLmDEnMu/30W9G6RYmGisf/gNgBqcmeWpG7TCU+xL7z4BY4MHuGBi+8n+2f/CyYT3HnnhM+ryqxCj4692cAW2V0411CHhoRYqItN4MVE2O1CLHTL75u4MtKJY5k88Gw2LK4QN2276t9lyOsGwGqeWiy0BfUMSrEwqnQoAwCsD8ysm2Zn506MOiOXVF3CgG+AwwOjA/CG9WBSYicW5oTsczoHZPCeJtTW0rZ4HsC0OwtnQnqauA/pG+iO6HxnyMbieLHQYbTTL8XCuNLtFoJ+ln52G1HL85Zj1BnZWGIYXywcbGNeP5iqR3etLstbBsAOq/No96DHA//2b2C1wq9/LR6TnYUnHLO5854HHFJV1Q+giu2yFqBkFsckklG4XeILL9WaNvFJikKaT4dHL3ZCChxR/EKejlgY8k50DsvuoFjgUQJYYmUEPkP0BiO6IHjF5Yx1R4TYs7b6/DHnWgwWvrXsi/Ra4YWOt+Nap2Rq9h+pJWMIskrmz+j5tlBa6baObVxWfRnX9xbDBx+IBOT8iUebzQYzFdZC9mUDGzbM6L0l2uFz9xPQaSAWOoQI4PbK75t4MtKJY43iBMNxLDlZbDbt7tzNkCckFpqm7jaxYmBIL4Msooaq0mESAQ9brH34AtMPj9lxeAcLcxaOdJAfG3Ki+nx4DWCOoViYaxfpu11HZDdQ3BkaguZm2ivF76DIEYfOwswCAPo8kdlTOAdDYqF9tCd7msmBxwjDvdKDPV50d4l/o9mm2X23mA1mluUtY2MRqPtrxxxvVHup6ANKRssu6ZZ0SgxZ7Mjj6PTTD38oRpm/8x26Vi/mnRKkWHgCklh33uOgKMpXFEVpDf9xu91alySJI64B8UVmn0wsBNICRxdbBRnzoldAQcHRn6caQ84S57p88jMaCzy6IJY4J45GgjkAw6ofurpYZ+/BoCqsLlo97rknLzgHgIP9B+NYoSQS6twtIgl5nMTiSAiLhammVP73sv9Fuece4a95111TPndBwVLqM8G34YMZvbdEOwadYkMr/PuPF/aQ7YV7WH7fxJOjnTgzt6+YivLyFVh9sNvbiscrOtuslqm7TawYGdKrY0ZdJTOkr4+OUEOnR6+O8h+MhH5PPy39LSzNXUplRiUA9T31I8cD3iFUJbadhbnpYvO8s+tAzN5DMgH19aCqtBWLLuS4jCFniffo80bWce4cEqKiI3W0J7sjFKDl7JMdqfGisX03APOsebN+rVWFqzhs8dPqaodjdJPeoV76DD7K/fZxg/yW5S1nbw4Mv/s27NkDP/4xLFuG94v/zgUvf5xzPw09bfVjnidJbmYjFh4EChRFfMspiqIgugNbZnFsDKqq/lxV1eLwH7s9dqbSksQjbN5uT8mY9Ly0oGnk5/yciugVkHfMRXsKsdCamYc+CM7AYPTeXzKCEAvjmzgaCaagwjABghvWs64EVhhLSJnAX6o4V7gttHqlf1Ai4fQ66Qj2U93NmN3WSCnWievDPef+kHkH++H550WoSU3NlM9dkL8Enx6a9r0/o/eWxJBgkOAD97P10hVweOyN06BLiEcphtik406EPS0XALdfft/EE1fIk9iRmjXFmTNHp+hY5E1ll9V5tLPQPPXa14aRgA58wel3wEnG4mlvoc8KmaF/YuvrphdysrNzJyDG+yozhVh4bMiJN5R0bdLFzu80J1N8n3X2tcXsPSQTEEpCbs8WXedxGUPOEb/vPn9km0jOkKjoSB8tUIXHkp1OuVaNF7tCmxFL7JWzfq1TCoUf4cYihGgdojHU2VxhHF+QXFa+Bp8e9u14HW69Ffx++O1v+ebb32b74e0EdFDXNzaoSZLczFgsVFW1E9gC3BR66BqgVVXV+pkem2ktkuTFNST8MlLtky/M0zja1VFQNLMxwnGx2cAR8iaaQixUzGZSh8GpeqL3/pIRPHoVSwyNwGeKKQjD+Nm78UV6rbB23hkTnpubkoshCK1BmWCaSIS7PWpmIRZeQhX7H4Bba26Ee+8VD959d0TPXZgtfBL3+g5BR8eM3l8SA9ra4OKL+dZzX2LFmq28/8Kvx5wy6BbikS2CtNpokmIXN3NSLIwv4U6c1BiKhQCLTfNoT4VDh8W1yWqNoLMwFLI05BuKaW0nCocP7gXgshbx/3V9w1vTev7Ow0IsPLaz8FixcNgjukbNMQxHysorQ1Gh0yW/V+JOSCxsSxE+ovEYQ7Y4MjH5oS84ENH5zlBn+vFiYVronsvpkmPIcUFV2bXzdUx+qCo9adYvFw452XhcyElTs/DmrkgrG/d5y/KWA7DDWQfr1sG//zsvZ/Vy3wf3jSRmN3gOjQ5AkSQ9EYmFiqL8RlGUVqAYeEVRlLCwdytwq6Io+4FvAJ855mkzPSaRjBD2Y7I7sic9L00nbtQMAcieF0WxEI6OIk8hFgI4fDpcqje67y/BH/Tj15GgYqEOLwHWHRA+hGeefPWE5+oUHYU+K21GDwSlt1SisL9bLKaqh6yQNrnlwUTobClU9yBuEP7yFzj7bFizJqLnjiQiZwMbN87o/SVR5sknYelSdm/7J/ecIVLYdxzcNOa0EbEwgrTaaBIO/XIH5eZUPBnpxHHkxvR9FucuBmDTQB0AVtvUgSpWXUgs9MrR9GjQ0SFudZbayqnogQ86pxdAFR5bXpa3jBRTCvn2/FGehd4h8Xsy6WMnFhryCsgahK6hyAIvJFFk3z4A2g1DWA1W0swzW1tMB0WnI31YoU+NbMPAGepAdGSO9lUOjyX3h2wXJDHmBz9gt7eV+cOpGK+7cdYvtyhnEVadWXQWHiMWNjaJa1hF3oJxn7c8PyQW5gEFBRz+jzv51LOfIs2cxp+u+hMA9Sle6JGfixOJSNOQbw2NARtUVc1TVbUq9Hitqqqnqapao6rqKaqq7jzmOTM6JpEcS1gsTJ1KLNSLEbC8AdDlTRwmMCPC4QQRiIWpAQNO3XB031+Cxy9uiK0x3IGfKeagwjBB1nnFTd0ZFedMen6xkkZrqgqdcrwjUajrFr+7asssdv5toTHUH/5QjG58/esRP3V+ttjgkCEnCUBfH3ziE3DddagGPbfdvQi/TuyiN/Q2jjl9cFB0v8dbLLQarCgqDMjNqbgSDjA7vhMn2iypFh3qm1LE58tqnVosDPtmDg3IBNNo0HHkAAD5JYtY0wb7ve30DvVG/PydnTvJsGSMjJ9WZlSO8iwc6SzUmcZ9flTIzSV3ADp9kdctiRK1tZCfT9vgYYocRQjXrdiT7jPQp0R2HxK2TbJnFox6PC1NXN+c0/i8S2bIX/6C6/v/yYEMWLLiYjDMvinCoDOwIv9kNhVC8JiQk8bDQsCuqFg57vOqMquw6M3sKLUQ/O1v+PQbd9A50MmvL/8155WfB0BDBtAiA5NOJBI+4ERyYuPyicWU3TJFwIlRdFnkD+nBHOVUymmIhY6gEZc+EN33l4yIhRYlhovqGWJSdQz7PazLG2Z+IJ2clJxJzy+25HDYDsPNY4UHiTbU94TEwvRZ+J2GxcLnn4elS+HiiyN+aqY1k1xbLvtyFSkWasnwMJx6Kjz6KFx+OQ8/9S3eGdjDzSffjD4I9cNjR/lGxMIIPOWiiaIo2P063MjNqXji9Is1ieO4m+tos/ikCwFoC2mE1pSp1x9WvVj7hLtdJbOjI+Tzl7/gFNa0isc2tEV2fVZVlZ2dO1mWt2xEJKrMrKR7qJt+j7hmDIfCa0z62IqFOYPQGZSp6XFFVYVYuGABba62uPgVhkkPGukzROZb6lQ9pHpBZxp93+QIeeKGP6uSGPH22/DZz7JnsbhvWFI4+xHkMKvmnUq/BepbjwYzNTqbSRmGnJqTx32OQWdgce4SdizI4JfZjbxc/zKfPunTXL/ketIsaWTrUmnIRCYin2BIsVCS0IT9mOymyW/E0kxiRV3gj0Ei5cc/DjfdNDrsZAJSFTNOgxQLo004EdISyx34GWJSdbSkqhzIgLXpy6c8vzi1GID2puklK0piR93hveS6wVE8C2Np6zGedXffLZKQp8HCnIXszdWhblgv/WC0orZW/Pn85+l+/I/ctf775KXkcc+F91DiMVOvH3vjNDAkxlJtlviHr9kDetyKDLOIJ65QJ05qRpQnGI6jpGABdt/Ra4glZeoRxhS9uAa55ehgVOgYEJsD+SetZU27+F2sb1sf0XNb+ltwep0szV068lhVhgg4C/sWeodCnYX6KG9wH0tWFrkD0KPz4gvIa0XcOHwYnE488yvpGeqJSxJymHTVQp8xsvsQJ14cvrFSQJo1FHASYaqyZAbU1sLVV4PZzO7vfAGAUtqiFQAAIABJREFUxTmLo/byI76Fg/Uja8pGXycVvaBUTrzWXZa3jEPuQ3ztn1+jKrOKBy5+YORYZUox9VIsPOGQYqEkoXEFhe9Gqnlyc+80q1hIFyhTj+pMmyuugEceAd3U/1wcipUBEwT8clEWTTyh1EBLLHfgZ4hZ1eMMadRrF03dTVaUXQ5AW9veWJYlmQb1vQ1U9TDjcBPgaGdhSQlcd920n74gewH9xgCH/f2j0uskcWRv6N/k2Wfz9de+QfdQNz+/6OekW9KpUjJpcARQe0d3bQ16RMeOzRKD754psAcNuHVycyqeOIMerD4wmmMbaKMoCouHj3YTWiMQC4sMmQC09sobuWjQ4RWia37RAk7SF2EMKhGLhcf6FYYZSUQO+RYODwvhOaadhUYjuX4hRnZL38L4EU5CrhabCnEVC3VW3Cbw+6fuOncqwzj8+jGPh8Ms+iNMVZZMk64uuPRScDrhqafYZRYbkUtyl0TtLVYVhsTCjEHo7sYf9NNsGKBiwASpE99Th69ZKip/veavo+6/K7OrOWwHd3Nd1OqUJD5SLJQkNGHz9ik7C21ikVxgzIx5TZPhMAjBwN0rk+eiyVGxMIY78DPExNGF1tpTPjLl+cWFwli4tathijMl8aDf00/XcK8IJ5mNWJgZuvZ8+ctgnL635qiQk+3bZ16HZOaEDOnfzfXw+62/50PlH+KGJTcAUGWbx6AJOnaPFgsGQ766tgg85aKNHSNu2ckeV5x4SR2nEycWLLYevR5ZU6de25SYxChbc9+BWJV0QtER7EcfhKyUbCzFZZzcqWd963rUCDq/d3aGkpDzjnYWhhORw76F3rBnoSEGEzHHkKMTN/udA9InOW6ExcJiIfLHdQxZL/xz+7vbpzzXqffjCI5dr4TFwrCnoSTK3HgjNDbCgw/CRRexq3MXVoOV8ozyqL1FVWYV6VjZFEpEbnW24tepVOiyJn3eWaVnAfDjD/2YUwpPGf2aReJ61thZO+Z5kuRFioWShCbsx5RqmryzsNQxD4AaW3HMa5qMVIP4knb1SLEwmswFsTBv2ERlVvWU5xeXii/b1v7WmNYliYzwjVtVD1BVNfMXuvpqePppuP32GT19lFh44MDM65DMnL178eng32p/hklv4sHLHjzqN5ZdA0DD/g9GPWVwWNzw22yxT7o8nhTMuI0Ir0VJXJioEycWLM4/2pVmjWDMPbwOau5tillNJxIdukHyho3oFB2UlrKm2U/3UDeN4wQdHU+4s/DYTqGRzsLecGehmJwxGWK7rsk1iQ5VKRbGkZBY2JYtfrdFjvh1FmYYhdDX13VwynOdhiAOdWxna1rIJ96peqJbnER8X7/+OlxyCXz+8wDs7trNopxF4loTJRRF4RR7DVsKwF+7l8a2XQBUpEx+n7yiYAU9d/fw1dO/OuZYeB1U3y+/Y04kpFgoSWhcOh86FSxT7Lyeknsym34D16efGafKxscRatd29hzStI5kYygkFloNsR39mglhsXCtoSKitLvibBGi0eqVC/dEoC4cbtINzJ8/8xcym+Gaa2acZDdKLGySCzFN2LeP+y9OZ9eRPXzjjG9Qk1UzcqiqTBiCH2sWDseIhfaM+NUZwq6z4DYBbjkqFi9EJ87s0yojYcl8sZ4x+YnoJrKw6mT0QWjpluFZsyYYpMPsIz8YspcoKxsJOYlkFHln504qMirEVIyqQlcXWdYs0sxpRz0LQ2Kh2RhjsdCaDUCXS25ix43aWjCZaLcKS6K4dhaGhL6+nsk7C4NqEJdJxaGMvb8aGUPGG/0CT3SamyEYhCViI6F3qJd2V3tUR5DDrCpezZAR9tS/T1P9JgDKs6feFM+wjr+eCXdHN4wT9iZJXqRYKElc/H7c+iD2oHFqEWb5clYGctGfdXZ8apuA1PBunBxDjiqekC/YVKKxFpgVceO4dt4ZEZ1fYC9AUaEt0BfLsiQRMtJZqGRBWvy7w8KUpJVgNVjZm6vIzkItCAZxNe3jOytdVGZU8s0zvznqcOX80wBo6BntJznoE2NamoiFeguDJgi6pAl9vHAaA+N24sSCxSddAIA1EFlYkqFmAcVOaB6cevxQMjlqTw8dKZCvC9kLlJayRoQj80HrBxM/EfD6vdQeqT3qV/iDH0BxMUpTE5WZlWM9C42xXdfkpgrfvM7OAzF9H8kx1NZCVRVtbtE4EFfPQquwLOjrnbxpYWCgD1UBh27sJrzFYMEU1OE0BmXnerRpCFkQhUJGdnftBqLrVxhm1cLzAdjYuZXGNmGNUDFv2WRPmZSqzFBIk2kABuWI+omCFAsliYvLhcsMqWoE/l+lpSJ9bO3a2Nc1CQ6rGPdwuY5oWkey4fGIzhmLMQE7C3Xi87l29UcjOt+oN5LnN9NqHIKA9BvTmrpu0VlYlbtA0zp0io752fPZl6eXnYVa0NLCuhwPA/oAt6++fczGREWJSDqv94y+ARv0i+4gTcTCkEfuYF9X3N/7RERVVVxGFQfx2bQqzCglzafHGoxw7Lm8nJJ+aA72Tn2uZFKcrfV4jJBvCvl7lZVR2QNZSsqUnYV7j+wloAZEErLbDT//uRBcnn+eyoxKWp2tePwevD4x4mmO8bomJ10IVZ3d4wffXPbYZVz/9PUxreGEwu8XG37V1bS5hMJckFoQt7dPTxGf2T7n4UnPc4Y6Dx0h+6TjcWCi3wz090e1vhOexlDnd4WYMtrVKcaDo5mEHOaUUrHJuXH4AI2hTYqymjUzfr3clFxSVCMNGcDBqcfcJcmBFAsliYvLhdsEdiXxfOomIjVF7Og5nVIsjCaekBG4xWTTuJKxrF79YU4yzGP54g9F/JxiJY1WB3BIjqtrTf3hveQMQFp19Hd1p8uC7AW0pPgZaGsSo2uS+LF3L2+ViR/PKTtnzGGb0Uahx0S9bnRH8KBf3PDbIgigiDZ2o/Cxc/dLsTAeeIdc+PTjd+LEAkVRuLjqYpaVR3hzZzJR6k+hx+DDPSxH02dDR6vwnMtPyRUPlJaiAKv9eWzr2IbXP/F4ZtivcGnuUvj97yGcoP7qq1RmVKKi0tTbdNSzMMZiYW52KQCd/WM7TtucbbxY9yKvNb0W0xpOKFpahGBYWUm7q50sa1Zcp2LSU0XQUZ9r8u8FZ68QEx3G8cXCNCw4pVgYfY7rLAyLhbHoLCx2FJM3bGKjrZfGoXYKnWCtWTTj11MUhUpjLvWZiHFqyQmBFAsliUtILEyN08I8GjhyhHFsz7v/kl1jUcTjTdzOwq997H62/r8WjPrIE3CLzTm0p0Kg5UDsCpNERF1P3ez9CqPEgizR3bjf5oFO6WkZV/bt480yyDCkHh0fPI4qNYMGh3/UzdNgUEOx0BQSC+XmVFxwhtJFUw3x27T666f+wSu3rov4/FKTELdaeg/EqKITg44OYTeQnx4KAygRydRreqwMB4bZ1rFtwufuPCzG/ZZlLRRdhbm5sGYNvPEGVQ4h3DX0NhztLDTFdl2TnleKIQBd7rHfKS/UvQDAkcEjuELJ7pJZcowY1OZsi2u4CUB6Wh4AfQM9k57n7A+JhSbHuMcdehv9FqRYGG0aGoS3deiasrtrNw6zg2JH9AM6FUVhFYXsyFHZTzcVTp24Hs2CqtRSWtJg+EBDlKqUJDpSLJQkLi4XLpPwZZorrDz34+hVhWd9O+Fb39K6nKTB4xXeGBZz4nUWzoQieyEBHXQ27da6lBOafk8/Xb4+kYScAGLhohyx47sjD+lbGGfctTvZVAhnFp8+YZhElbWIXiv07N0y8thgUHQYWU3jd2fEkpRQQq7b1R339z4R2bHvLQBKzXlxe89IQrOOpSRdiFHNjVtjUc4JQ0eP6JrJzyoTD1gskJ/Pqc1iE3iyUeQdnTuwGCxU/Wur6DK74w648koYHKSyXQiE9T31DIe6E2PtWajLyyd7EDqHxm4qPL//+ZGfm/qk/UVUCImFakUF7a72uIabAKRniJHnvqHJ7Qic/UI8dljG92p2GFJkZ2EsaGgQ1lkGA6qqsvPwThbnLJ72tT5SVqUvxq+HPlOQioADZvk+lTnzCeqguWVnlCqUJDpSLJQkLuEx5An8NBKRAkchl1VfxsvVcPDBH8Nf/qJ1SUnBUChx1Gq2a1xJdBhJRG7bo3ElJzbhcJPqHmCBtp6FACsKVgCwpQApFsaZdzs2EtDBOTUXTnhOZXY1AA373h95bFAdxhhgWp3F0cIeuslzuyfvIJFEh5c2/RWAi0+KzJ9WC0rzxaZHc+OWKc6UTEZHv/Cayy84Jjm0tJTVO4UAM5lYGL75199zL6SkwG23wYXiulK5XnjkNvQ04A35nZrNMV7j5uaSOwCd/tGiz5BviH81/gsFIR409UqxMCrUi3VF37wchvxDcQ03AUjPEu/X551c5HOGNpkctvRxj6eZUnGaQe2TYXxRQ1WFZ2HIr7BzoJPuoe6YjCCHWVVy2sjPFab8Wb9eZdi/+UjtrF9LMjeQYqEkYRl29jJsgFTT3BKIbj7l8wQV+NNZqfC5z8HGjVqXNOfxDIc7C+fWZ2EiiguFMNV6pFHjSk5s6npC4SZOg9jp1ZiKjArSDalsLkSGnMSZt4KiG+TscfwKw1TNCy2SDx4dQRzEh80fm46AqbCHbvIGBmXnRzx4sX8ThS5YdtEntS5lQkrLTwag5dA+jSuZ23QMiK6r/HkLjz5YVkZG82FqMqtZ3zq+WNg10MUh9yGW+jNhxw645RbIzIQVKyAri6JX38esN9PQ28CwX6TMmswxtlcJiYVd6mgfyzcOvMGQf4grOsV1pLFXrkeiQkMD6PW0pYtb7Lh3FubMA6DPP/lYuTM0puywjW+h4TA78OnB0ydtLqLG4cMiRfi4JORYhJuEWbX8kpGfK9LLZ/16VQVC2GxwSs/CEwUpFkoSlrAPk92cqnEl0+OS6ksosBfw+zPtBBXgqqugfayxtCRyPD6xA2+xJIlYWCK+bFv7ZZqYlox0FqaUgD7CxNEYoigKK/KWszUfAgfkjVvc6O7mzdwh0oImluctn/C0yvlih74hlKANMIgfW0CbpZQ9RSQwu4dk50esaerazz7rAJe48lHsifs9VLJIfEab++SN3Gzo8AkhJT+38uiDoQ2lNY5FNPQ2cGRwrIiyszPkV/jBAfGd8uUviwM6HVxwAbotW6lwlArPwlA4Usw7C9PTyR1UcOp8eELvCUdHkO94SXRLNvXJ75yoEBozbfeIgJF4dxZaMnMx+6EvMDDpec5B8Xt32LPGPZ5mFd8v4XFlSRSIY7hJmOzq5ZSFlggVBTMPNwlTmSlqb/DLz8WJghQLJQlL2IcpdQI/jUTFoDPwmZM+Q/PQIV775VdE4u3VV8PQkNalzVlGxEJr4t6kTYeiDHHT0Tokv2y1pK5LdN9UFWqfhBxmZcmpDJqgtkP6WcaLgd1b2VgIZxoq0esmFo0rS08CoH7o6ObPoM6PLaiN0HxULHRq8v4nEi+99hsALik8W+NKJsdWtZDsAWj2Hta6lDlNR9CFzaeMhAgBUFYGwBpFdG6N110YDjdZ+m4d3HDDSIgBcHQUeTiFpt4mhgIhz8JYezHrdOSoonuxa0AIWKqq8vz+56lSsjj3AJj80NQhu1FnTXjMNBRuAvHvLMRgIN2r0KdOfs/h9AgFyeHIGfe4I0V0HPY75To1amggFqLXs9olQmwqKlfO+uXmOeZhDCrUG10i9VuS9EixUJKwuEI+THbb3BILAT578mcB+F16A/zHf4hR5Ftv1biquUt4NzxZOgvD6XhtwckNqCWxpf7QbnIGIK06gcTCwlMA2DwkuzzixXs7XsSvh7MLTp30vHRLOlnDBuqVo/9uB3UBbKoh1iWOi92RDYBbppjGnJdqn8cQgPPP+YzWpUyO0Uipx0yzTgrIs6HDMES+zzQ6dCDcWegWa9LxfAt3HN4BwLLDwNe+NvrgBRcAUNnuwRf00aCKDXGTJfbBbbl6IRZ0hsard3bu5KDzIJcfMKFTobQfGo/Ux7yOpOfwYRgYgMpK2l1iUyneacgA6T49fYp30nOcXnGNcKSPH9iUliq+X5xuGaAVNY4TC3d37SbLmkVuyuwSiqfiv91reORvULBw9axfS6/TU66m0ZCBaIaRJD1SLJQkLO5Qi7zdlqFxJdOnMrOS88rP4+97/86Rb94pFomPPCIvrDNkRCy0OjSuJDrYjDYy/SZajR4YHta6nBOWur5GqrtJiHCTMCsLxc7vFkMnBIMaV3Ni8Fb7ewCcs/SKKc+tCqbTYPeBU9xoDeqD2Ih/uAkcIxb63FOcKZkNHr+H1wL1nNGuI+3087QuZ0pKlXTaLX58vsnFAskEBAJ0mP3kq8eNB4fEwmWHgpj1Zp7e8zR/2/s3+j1HPUN3HtxErhty114Ey5aNfn5xMSxaROVO0XG2B9HlZ7bE3mon1yTW0V2D4j3DI8iXv3UIsrIo74UDQ+2oqhrzWpKaY8SgNpf4Pcd7DBkgPWCkT+eb9Jx+n9hkmkgsDHcc9g/KTe2o0RjaBC4vR1VVdnXuYknukpglIYep/vRXuGnNLVA+e89CgEpTPo0ZEDwgvbVPBKRYKElYXCEfplT7+Oa7ic7NJ9+ML+jjkZ1/gcsvFw/ukem3MyE8rmNNmXtdphNRrDhodSD9LDWi39NPV8BJVQ8wf77W5YxQkVGBI2hic14QOjq0LueE4M3h/aR64aQVl055bpWlkI5UcNeKccNBvaqZWJiSFhILhyf3ppLMjrf3vcKQPsil1IBRm9/1dCixFRDUQVutDFebCYGuTjpTIF9/3HojJBaaDhzko4s+yt4je7nmyWvI+mkWa/+wlu+/9X12Hdkjugrvvnv8F7/wQqoOiI2GfXohwsSjszDHJoSfzj4hYP1j/z9IVSyc2Qx88YtU9Ipk93DnoWSGhJKQw52FBp2BnJTxx3xjSbpqps8YmPQcp18EB6ZmjJ+Q63CIbjen9MSNHg0NkJsLqam0Oltxep2xHUEOc/HF8NvfCu/UKFCZVobXAG0N26Y+WTLnkWKhJGFx1wlhzZ45+6h3Lfjwwg+TYcngd1t/h7owlKgnxcIZ4QmJhRZbcnQWAhSZsml1gHpQhpxowUi4SYKJhTpFxwrDPLbmQ7BJjiLHmkHfIBtsvZzZY8dgNE95fmVmFQCNe98Dn49BI9gUU6zLHBd7yFPKHRjU5P1PFF5c90cALlk4dedpIlCaWQFAc+34ib2SyTnSso+gDvIt2aMP2O2QlQXNzTz6kUdpuKOBBy99kMtrLmf74e18+81vM6T4WUYunHvu+C9+0UVUhhq1PIoQc8zWOHQWpop1dGdnE50DnaxvXc9FR9IxKQb4whco7xedTTIReZYc11lYYC9Ap8T/VjtdseI2qfiDE3vKOQODWH1gTB+/ISMtNNUVHleWRIGGhrgmIceKqjwRlNLQukPjSiTxQIqFksTk4EHczSJxMnUOjiEDWAwWPrHsE+zp2sMH2SGjYSkWzghPcBh9EAy25PAsBChOLcJrgJ7mvVqXckISFgurAmmQllgdqyszFuE2w/7972tdStLzfsNb+PRwthLZeE7VPDFaWN+yjeCAG48RbLqpRcZYEA5fGAh4pjhTMhtean+b4n5YcsFNWpcSEaXF4uazpXmnxpXMTTraawHITxlno7qsDA4cAEQX+G2rbuPZ65+l++5u3sz8Kj/5J3zl7G/CRGOFZ51F2YAR3THTvqY4BLflZhYD0NlzkJfqXkJF5Yp3u4SomZ1NuVmMojb1ybHCWREWCysqaHO2xT/cJES6XozQ9/dPHHTkVD04vIDFMu5xh1lszofHlSWzxOWCzs74hpvEiMqykwFo6JY+pycCUiyUJCZ/+xuuULPGqDS6OcbnVnwOgN+1PCsECSkWzgiPOozFz4SLmrlIcZYQJ1pb5WdCC+q69wNQnV6pcSVjWVmyBoDNrRs0riT5eWvbswCck7MqovMra8TvpqFrP0NOEcJl02tzXUoxiRtCtyq96WJFQ08D+/W9XNJqQVm6VOtyIqK0SoQkNXfVaVzJ3KTjsBB88jOKxx4sLYW2NvCN9oMz6U2c/fct3L3JTNG1n534xW02TGecxbxjmrXi0lmYI9YbXf2HeL7ueRQULtkXgA9/GICKUMd0U09DzGtJahoaID8fv83C4YHDmoSbAKQbxWeqr6t1wnOceHH4dBMK22lmsYnqlJ3r0aEpJMRXiM7vkc7C3LnXWVhZtgKA+oEWjSuRxAMpFkoSk2eewZ0iEibnsli4LG8Zq4tW8/juJ3AurYG9sotsJniCPiEWmrXp4IkFxYUiVKO1Uy7OtaC+XezqVhUvm+LM+LNy0fkAbO6X14tY82bL29i9sGLBBGODx1FVGlokD7Yy6BIpkVqJhSa9CVNAwY0MSYoVL219EoBL01dN3C2WYJTMF4mXzS5pcTETOnrE/7f87HG6jUtLRfBUW9voxw8dgjffhEsvBccUdikXXkjVMQGzBn3sfTDtefMw+6HV3c4r9a9w6mAmOYPAlVcCUD5PCOGNrbIbdVaExkwPuw8TVIMU2jXqLAwJfX09bROe49T5cAQMEx4f6SwMSrEwKhyXhLyrcxcF9gIyrXPPl788swJFhYbAEa1LkcQBKRZKEo+ODli3DtcisdOZao79rmssufnkmxn0DfLEySbo6hJ/JNPCgy/pOguLioWPZWu/vKHTgrqOPeQMQNr8xBMLqypOIdULm4MTL/Qls2fIN8T6oTrOOAiGhZHt7ufYcrD7ddTTw6Ar1FlosMayzEmxB/S4lclTLyUz56WtT2EMwIdWX6d1KRGTlZqHza/Q7O+e+mTJGDpcInQsv7B67MGyMvF3aBR5hKefBlWF66+f+g0uvJBKcenA7CfmSagASl4euQPwlmcfrmEXl292werVUCQ63zKqluLwQFPn/pjXkrQ4nXDkyEi4CaBdZ6ElHYC+nkMTnuPU+3EEJxaq0yyhzkJk53pUOEYsDKpB9nTtmZMjyCBstoqHLTSYBsR1T5LUSLFQkng8+yyoKu75Yld3LncWAly/5HpSjCn8LifUri27C6fNED6sPsAw8S7oXKM4XSQrtg1N7CkjmTm/2/I7PvXsp1AnWMjUu5sTLgk5jE6n52RnClut/QTVoNblJC0ftH7AMAHOOQDU1ET0HEVRqPKn0WAfZvCwEHNtxtinmU6EPWjArZvYxF4yc4Z8Q7zu3sGZzZB6weValxMxiqJQ6kuhxTgIfvnZmC4dQ2JDN3/eorEHQ4nINDePfvzxxyElBS67bOo3WLaMSp9Y15ridXnPzSV3AHyIUJXLdw2PjCADKPPnU9ELTW65eTljjgs3AShK1UgsTMkCoM85/vpSVVWcxiAOdeJwrlSTaNToNwTGjN1LZsAxn48DfQcY9A3OyXCTMJVqBvXpQdRuuSmV7EixUJJ4PPMMmEzUZ4pk0LnYon0sqeZUrll0DRuCB2l1IH0LZ4CHAJbg3BgBi5Rih/BDag32aVxJcnLfB/fx5+1/5p2Wd8Ycc3qddAZdVHeTkGIhwMpgHi6TSn1XrdalJC1vNb8FwNmefHGjHyFVlgJa0qB3/zYAbKbInxttUlQDbn1A7u7HgLea38KjBLikN+uoSDRHKDVk05IG6vGilmRKOnwirjg3e5zf+XidhS0t8N57cMUVkV1HdLoRzy9zIE7rmpwccgbEjyV+O0sPA1dfffR4TQ3lfdAS6MUXkMLQjKgPhT0c01moWcBJqkjy7nOOP8k05B8ioAOHMvG0jlFvxBY04DQD/f2xKPPEorERbDbIy5vT4SZhqiyFOC3QXbdd61IkMUaKhZLEorsb3ngD58Xn8krza5xbdu6c7ywEOKvkLAA2FiI7C2eARwlgCSbX5cphdmAPGGg1ecArxzyiSfdgN3u6hCj/0JaHxhwfSULu1x+9+UswVqYIG4bNe/6lcSXJy1sH3sI2DKdkTy+4ojK9AlWBPc2bAbCZtfuOsitm3CbkNSQGvLjprwBcWvIhjSuZPiWpxQwZoWvvJq1LmXN0qG4yvTrMhnE8ksfrLHxS+FpGNIIcouqUCwEwxWtdY7eT6xHvdfm+IMr8+bBgwdHjhYWUuwwEFZWDTtldOCOO7Sx0hjoLtRpDdoh0676B8bu+nENik9qhn9xCw6GYpVgYLRoaRLiJoiSFWFiZIYJaGhrld0yyk1x335K5z//9HwQCPHdBCcOBYT62+GNaVxQVVhcJw/ENZUbZWTgDhFio17qMqKIoCkWKQ3Sbtk6cWCeZPu8efBcAvaLnqd1P0TPUM+p4XbdICa02F4A+MT9XK/NF58nmxnc1riQ58fg9vH/wPc44CMYF0xsFCofi7HCKz1GKhr66dsXMgBFwuzWrIVl5qf5lSvpg4VnXaF3KtCnNFX57LfWbNa5k7tFh8JDvm6DjKj0d0tJGi4WPPy4eu/jiiN+j4gKxtjXHa2JCUchTRdfj5dsGR40gA6DTUWHKBaCptyk+NSUbYbGwqmpkDFmzzsKMAgD6jlv7hHH2dgDg0E/eCetQrPRLsXD2+P3imhEKNwknIS/KGcfqYI5QWSDWTeGwQEnyIsVCSWLxzDNgMPCk4yB6Rc+HF3x46ufMARblLCLFmML6KosUC2eARxfEoiamqDMbik3ZtKUCB+VOfjR5p1mMHn/ltK/gDXh5dMejo47XdYru3qqscQzsE4SailXYvbC5S454xIINbRvwBoeFX+GxHTYRUFktNn92GkJpyBYNxUK9VXQWulya1ZCM1HXXUR/o5NJ6UM47T+typk1p6XIAmlt3a1zJHMPvp8MaIJ9JRJTS0qNjyPX1sHmzGOk1j9OJOAGppdXkDhsxjde9GCM+113Kt9+ECxoZPYIcojxd+IQ3HZEhJzOioUEkYWdl0e5qx26yjyQKx5v0TCFS9nnHF/lGxELT5N9daYYU2VkYDVpahGB4TBJyaVrpnA7wrKo4BYCGngaNK5HEGikWShJq1x3sAAAgAElEQVSH/n745z/pP38tr7S8zrnl55KTkqN1VVFBr9OzsnAlmzI9BA61Q5/0qZsOHl0QC0koFtqF54ezWS7Oo8m6g+vIsGTwX2f/F2nmNB7a8tCooJP6lq0AVJWerFWJU6KrqOTkDtjiaZIhJzHgzQNvAnD2AWDhwmk9t6pUdH3uyBGfKZtNmxtCALvByrABhp29mtWQjLxU9yIAl/orIDtb42qmT0nFSQA09zRqXMncwnPoIH1WyDekT3xSaanY4AsE4IknxGPTGEEO8x+X/Ijbr/rBDCudPvNTSvjum2DIK4BVq8YcLy8UnUKNTVvjVlNS0dAgxCBFoc3VpllXIUB6zjwA+obH30Ry9ncC4JhCrHIY7fRbkGLhbDlmRP39g++z4/AOTpt3mrY1zZLK+acC0DDYpnElklgjxUJJ4vD88+Dz8dxFZWIEeVFyjCCHWVO0BpfOR2020rdwGgTVIF69ilVNniTkMMVZYie/rVV2m0aLQd8gm9o3cUbJGaSYUrhp2U3s6tzF+rb1I+fUddWSPQDp85drWOkUlJay4hA48dLYK2/4o4mqqjy//3lsQQOr2pl2Z2GRowhzQKEnFIJss6VFv8gIsRuFX+JA//hG9pKZ8fyWxzH54bylV2hdyowozRDfLc1DhzSuZG5x+KBYm+VbJtmoLisT6bCHDokR5Kws+ND0fS3vXPtVbj/tzhlWOgNyxZgxV18NurG3f2VVolOoqUOuT6eN1ysE5MpKuga62Hdkn6Z+dJasPMx+6AuMb0/hDAWfOCyTiOJAmtmB0wyqbHCYHY1iDRcsL+POl+/EoDPwnbO/o2lJs8VhzyJ7SEc9Mg052ZFioSRxeOYZUBSeTG8TI8gLk2MEOcyIb2ERUiycBl6/MO63KEaNK4k+RQUiibe1S7bxR4sNbRvwB/2snbcWgFtW3ALAQ5uPBp3UD7ZS3UPCJiEDkJ7Oyj6hRm1ul75j0eSdlnfY2L6Rmw5lY0rLhJzpdbDrFB0V/qMdGTbb5DdcsSQllMTslmJh1DjkOsRrXeu5qAFSPnSJ1uXMiMLUQvSqQgtOIWxJIqKjXXT55zsKJj4pHHLy4ouwaxd89KNgnAPrkzwRejHeCDKAdcESClzQ1C8TtKdNU5NIpK+s5MW6FwmqQa6suVK7eqxW0j3QFxwa97DTdQQAhy1j0pdxWNJQFbkZNWtCnYWP6HaysX0jX1z9ReZnJ/D6M0KqvCk0WAa1LkMSY6RYKEkM3G546SX6zj2NVw6+yXnl55Ftm3ujP5MxSiyUvoUR4/F7gOQUC4uLREdTW5/0LIwWYb/CM0vPBGB5/nJWFa7i8d2P4/Q6cXqddDJAdTeJLRYCK00lAGw+JMXCaPLTd3+KgsJX3/CKrkJl+iEDVab8kZ9tdu3EQntojMztGt/IXjJ9Ht3xKEFUPr3LAGvXal3OjDDoDBSrqTSnqaPDOCST0tElwj3yM+ZNfFJZmfj7nnvE39ddF9uiosW//Rvcdx+cf/74x6urKe+FRr8UhqbNMWOm/9j/D3SKjkurL9WuHkUh3aenT/GMe9g5KGwrHCmZk75Mmk0c7w+NLUtmSEMDLovCN3b+gmxbNt8++9taVxQVKnVZHLapuPvk5yOZkWKhJDF46SXweHjukgp8QV/SpCAfyzzHPPJS8thQrEixcBoktViYJsSg1qHDGleSPLzT8g5mvZmVBStHHrtlxS0M+gb5686/Ut9TD0DVcIpItkxg5mctIGUYNrdt0rqUpGFX5y5eqHuBj1ReRk1977T9CsNUhsIAAGz2ybszYondIkag3QNSLIwGqqry8NY/kjkIlxWcDSmTp4UmMiWWXJrTgLo6rUuZM3T0iI27/NyKiU8KdxbW10N+Ppx1VhwqiwJlZfClL407ggxAVhblgya69B7cwzJdfVqExEJP+Txern+Z0+edrrnnerrfQJ9u/K7iEbHQnjXpa4SPO91HolvciUZDAz+61EHHQAf/fe5/kz7F+PdcodJWBEDD3vc0rkQSS6RYKEkMnnkGgCczDyVVCvKxKIrC6qLVbM9T8dTKhMJI8fjEGIVFZ9K4kuhT7CgGoDUgwwmigT/o5/3W91lTvAbzMSmT1y+5nhRjCg9teYi6bjFmVp1SolWZEaMvr+CkDthyaPOogBbJzLnnPdENdHdGyItumn6FYaoKj/pR2VIn786IJfaQX+LAgPSUigabD21md/debtgF5o/OkY6xCShNL6PHBu79u7QuZc7Q4RYej/mFNROfFBYLAT72MdAnSfiaolBhEALXgb4D2tYy1wiJhW/aOhnwDXBFjfZep+mqmT6jf9xjTo8ILHE4Jhc0HQ4x4dUvN6NmjqrS2F3Pz5Y4WZa3jJtX3Kx1RVGjKrMagIYDcvolmZFioUR7PB544QV6157Cq21vc37F+WTZJt/tmqusLlqNXwfbhltgYEDrcuYEHq/Y4U5GsTDblo0pqKPV7IVB6fsxW7Z3bMc97ObMkjNHPZ5qTuX6Jdez+dBmntryKABVuTMTieJKWRkr26Fv2ClDTqLAwf6DPLbzMc4pO4fVh0LLnxmKhZVVq0d+ttm1FAtFh4J7UIqF0eDhbQ8D8OntyoTebnOF0gLRNdvStE3jSuYOHR7RQZVfsmjik7KzwRZKN5orI8gRUu4Qm2iNbVJgnhYNDWAy8VzP+wBcOV9Dv8IQ6YoFt1HFHxwrGDq9TgAc6XmTvkaaQxwPdyJKZsCRI3zt9AGGdSr3X3w/el2SbC4AlUVi07RBhiIlNVIslGjPSy+B283/XSpGkK9ddK3WFcWMsG/h+iJg3z5ti5kjDA2IHVCr3qJxJdFHURSKSKXVgUjSk8yKdS3rAFhbMtZnLBx08kzTCwBUla8cc07CUV7OilCYqfQtnD33fXAf/qCfu0+/+2jI1AzHkKtKTx75WdM05NCYmNvj1KyGZMHr9/LYzr+wqEth5fxzph18k2iUFi8GoLlDrjUipcPfhz4IWRlFE5+kKLBoEVRUwKmnxq+4OFCeL66HTfXS+mJaNDSglpfx3P5/UJ1Zzfws7f2Q03XCQqF/HKHP6ROb8I6M/DHHjsVhFZtR/d7+KFd34vD6hif42yK4RreEc8rO0bqcqFJZtQqAermZndRIsVCiPY88AorCU7lHMOgMXL1gbu/mT8aqQnFhlSEnkeMZcgFgOWasNJkoNmXTlooUC6PAOy3voFN0nD7v9DHHVhetZmnuUgCyByB94cljzkk4yspYGRILtxzaom0tc5zeoV5+u/m3LM1dysXDJfD3v4PFcjSsYJqUppehD4qfLQbtNjLsoRFot9elWQ3Jwgt1L9Dj6eVT21SUa+e+b3JJehkAzf0t2hYyh+hggDyPAZ0yxe3R3/4Gb745sf/fHKWiQmyiNbVLq5yICQSgsZGtS3Noc7Vx5fwrUWYQmhVt0g12APq628YccwYGMQbAnDb5FFdayBPX2dkKwWD0i0xy/EE/X9rxU8x+uKfy37QuJ+rkVp9Mmgc2+WSIVjKTXN9ykrlHTw88/zy9F57Jq+3JPYIMkGHNoMZeJsXCaXBULEy+zkKA4tQijqSA52CT1qXMaVRVZV3LOpblLcNhdow5rijKSHdhdQ8Jn4QMQFkZC46ANaiXnYWz5MGNDzLgG+Bu/xqUVaugpUWkmc7Qb8yoN1JqK8CmmDW9MUxJDXUWSrFw1jy8/WF0Kty0A/jw3PdNLk0X3not/iMwPKxxNXODDqOX/EAEa41588SfJKNo4WqMAWjsk51CEdPWBsPDPFchwkQSYQQZIN0shL6+I61jjjmDgzi8oDjGrpWOJbyWcg70wBtvRL/IJOcPW//AzuGDfPU9KF84dhN7rqOkpnJNs40tlh52HpJ2F8mKFAsl2vLEE+Dz8ewV1fiDfj62aO7v5k/F6tLTqc+Cnlp5YY2EwUEx/pCsYmFRprihazsoxePZUN9Tz+GBw2P8Co/lpmU3kek3Cr+6GXaUxRW7HUNmNic5bWxs2ziSDC6ZHkO+IR5Yfz8lPhvXfel3wnPsrbfg9ttn9bpr51/A/LxJvM3igD1NjMq6fdIDdzZ0DnTyYt2LXNioo3DF2ZA3uZfXXKAkTfjPNTuAJrkZNRXq8DAdtiD5pGpdimboaxZQ0g9N3sNalzJ3CIWbPJfaTqY1c9zJBi0IJ+729baPOeZUPaR5mDLtPS0kOPZbgN//PtolJjVBNcg9791DVsDMN9chbAuSkM9WfhSAP/7fdzWuRBIrpFgo0ZZHHgGbjSftLRh0Bq5acJXWFcWc1fOEx83G7h0aVzI3+HvzywAsNhZqXElsKC4QHW6tnfUaVzK3mcyvMEyGNYP6pwv5acv8uZNgWVbGNfsU+r39PLrjUa2rmZP8+R/fp3Owi6/8axDjlVfDtm1wxhmzft2HrniI9z/3fhQqnDnhgJOBwJCmdcx1Htv5GP6gn09vCcJHP6p1OVHBZrSRrbPTnA7U1WldTsLjbGvAY4R8U4bWpWiH3U75oIkmvQtVVbWuZm7Q0ECrA7YGWrms+jIMOoPWFQGQniK6zvv6xgq/ToZx+HVTjtGPdBZWFInR+x6Zihwpr9S/Qn1PPbe05GBPzYI07byNY8npN3+P+UfgkdYXGQ7IDvZkRIqFEu2oq4P336fno5fxr5Y3uKDiAjKt2qVKxotwyMkG5ZBIgpZMSLurnUea/8EZLbDaUqV1OTEhLBa29UlfqdnwTss7wORiIX/+Mxm1LZgWLolTVVGgvJxb3nCSZk7jnvfuIahK36DpEPjnK9z71o/IHISbP36vuOHJjM73jElvwqyxl6rdJHyp3H4pFs6Gh7c/TFrAyFW1wEc+onU5UaPUVkhLGlIsjICOFtHdn2+d28E2s6VCl8WAIUjXQKfWpcwNGhr4R434MVFGkOEYsdA19vfo1Plw+KcWNcNiYf+iSvB64S9/iW6RScwvN/wSnaLjtveGobJS63JihlJayme9CzliHOYf62T3aTIixUKJdjwqumTuPVOHP+jnxqU3alxQfFievxyjqmNDoQr792tdTkLzwPoH8Kl+vvYuIowgCSl2CN+j1iE59jMb1rWsoyKjgsLUcTpQg0H45jfhU5+CoiL4znfiXt+MKSvD4YXbKq9nf/d+nqt9TuuK5g4+H0/87DPUZ8IXlt9Myhe/KpJMk4gRsVCVG08zZXvHdrZ1bOO6nSqWU9dCYfJ0sZdmV9GWCr46mYg8FR2HhKCa75gkCfkEoNwu1iRNTTJUKyIaGnhuvtg8uqjyIq2rGSE9TVgp9Lm7xxxzGgI4VOOUr5FqFiP5zsIscDjEKLLsOJ2S/d37ean+Ja6uuoKSus6kFgsBPnHFt9AH4Q+v/UzrUiQxQIqFEm1QVXjkERprcvhZx99ZlreMG5bcoHVVccFisLDcUsb6YlB3y8S5iXB5Xfx606+Zb53HFftJWrGwKHRj0hrolWlzM6TD3UFdT934foVuN1xzDfz4x7BmDWzcCIu09ZmbFuXlANxhOweT3sQ9792jcUFzB9dvfsnXFh8iQ7Vwx9U/0rqcmGAz2gBwI8d/ZsrD2x8G4NOb/EkzghymNK+GoA7aWuRaYyo6jhwAID+zRNtCNKY8R7TJNdWu17iSuYHrwH5er4Bzys4ZEdcSgfT0fAD6BkePDnv9Xrx6FQdTd8XrFB2pplT6/W648UbYvh22SBF5Kn614VcA3FF4tXggycXCgsuu59I2Gy8rDbQdkeFIyYYUCyXa8O670NTEXR9LZzgwzC8u+gV63RzxEIsCawpX0ZUCzfs+0LqUhOWhLQ/R7+3nLtv56FSSVizMt+ejUxVaLcNw//1alzMnebflXWCcEeSDB+HMM+HZZ+GGG0SaX36+BhXOglAQS0G7i08u+yTvHXxv5L9XMgn9/Xz/n9+i3QE/vODHZNuyta4oJugUHTa/DnfQA0NyFHm6+AI+Ht3xKNXDqZzaSlKNIAOUhBORG7ZAo7yJm4yOXpEam5+X3Df2U1FRehIAja07Na5kDqCqvBrcz7AerqxJnBFkgPRM0SHd5+0b9bhr2AWAQ7FG9DolaSV80PoBDdeHuiZl0MmkuLwu/rjtjyzNXcpZ/SH/0yQNNxlBp+OzVdcS1MGfH/+m1tVIoowUCyXa8MgjvF4OfzfUcc3Cazi3/FytK4orqxeeD8CGVrlzOx6+gI/7PriPPEs2N33vWWEMfN55WpcVEww6AwWpBbTlWODrX4etW7Uuac4R9isc1Vn49tuwerUIs/je94TXjjWyxXFCEeospKmJu06/CwWFn773U21rmgPs/dFXuW/5ECtNpdxy2uxSjxMdu8GKW/HDbbfJEbFp8szeZ+ga7OJT670op54K8+ZpXVJUKU0TYmGzeQg+9jHhOyYZl46BDgDyi+ZrXIm2lC8Sab5N3TJ0bUq6u3luntikuWL+FRoXM5r07GIA+oadox53evoBcOgjWw/94uJf4B52c8P+HzK8fAk89pjcmJqEh7c/jGvYxRdXfxElvEGT5J2FAJd99sfkDsAfDj4nw5GSDCkWSuKPx4P/qSf40tUWzHoz91xw4o3VrS4XHVAbhqTp+Hg8vutxWp2t3LHFiOVInxB6kuwm7liK0+bRWhwaX7nhBhgY0LagOca6lnXk2HKoyaqBhga49lo4+2zo74cnn4T//M+561VXKm72OXCA+dnzuWrBVTxX+xx7u/ZqW1cCozY2cvvhPxDQwYMf/2vSd63b0/NwZzvg4YfhwQe1LmfO8EHrB3zuuc+RobfzmQ3D4rqRZJSGOgubrzoHNm+Gr35V24ISmA6v8HbLL5lDNhUxIGvBCuxeaBo6pHUpCY+/rpYXauAkpYCStMQaX7dk5WH2Q3Owl9ebXufxXY/zwPoH+OnbwpLDYUiJ6HXOrzifr5/xdTa2b+Rb1+eKddUzz8Sy9DlLUA3yPxv+hwxLBh8vuwKeeEIcqK7WtrA4YMzN55O+xdTbPLzzxp+0LkcSRaRYKIk/zz/PQ5X97EzzcNfpd1GeUa51RXGnJqsGR8DABmsP+Hxal5NQqKrKPe/dQ0rQwG3PHRJhFJddpnVZMaU8o5x2Txe//d5VUFsLd96pdUlzBpfXxdaOrawtWINy112wcCE8/TRcdZXoKpzrAoDVCnl50NQEwN2n3w3Ave/dq2VVCc2TP7qJ18tUbi64jNUlp2ldTsyxm+0MFOVAVRV86Uvwzjtal5Tw7O7czWWPie+VF5rPoNCF8DZNMsICRvOKSjj3XPjVr8QGimQMHYF+bD6w26OTlj5XUcxmKgZMNCl9U598gvP+nlfotsGVWadrXcpYHA6yBuEdcwcf+vOHuOGZG7jz5Tv5zfbfo6iwRB95kNP3zv0epxafyj3e13llvgF+97sYFj53+Vfjv6jtruXmhR/HdvEVsH493HUXFBRoXVpc+Mzl3wLgD6/+WONKJNFEioWSuNPz2O/5z/Og0JbPN9Z+Q+tyNEGn6FhFEZvzwb9fphQey6sNr7Kzcyc3b/CTcf7l8K1vaV1SzPnuOd+lPL2cW71Pc+fnS/D/8ffw1FNal5XwDPmG+MEb3yWoBjnzj6/Dz38OS5fCm28Kn8KaGq1LjA7l5bB3L2zaxGnzTmNtyVoe2fEI7a52rStLOFzv/IuvON4n02/kh5/4k9blxIVUUyptA4d44cEvg9ksQjra2rQuK2E50HeACx+9EJfXxd8+/ASnPfk+rFp1tIs3iciyZmEz2mhxHRTjg3l5cPPNUCenGo6nQzdIvteIMle70KNIOek023z4/TI4aSJe2/o3bq7/OQBXLvywxtWMg17Pr1+z8p97c/mfqjt46kO/4a1Pvcney1+m5ydwlXlZxC9l1Bt57COP4TA7+OS1ejo2vwX1ckz9eH654ZfoFB3//t+vwKZN8F//BT89cWxjFp13Haf2pPCUfj/Orlaty5FECU3FQkVRqhVFeU9RlP2KomxUFGWxlvVI4kBXF9/1ip24n1x0D3aTXeuKNGN15lIGTbBn2z+1LiWhuOfF/4c+CF8+VAqPPAK65N/TqMmqYcMtGzi79GweKGzh0s8Y6b39Zmhu1rq0hCQQDPCnN39BzQ/y+Mn6n1HZA9e1ponPy8aNYgQ5mfjkJ8HpFILGtddyd8mN+II+7v9ABuKMQlX5/u8+KUJNTv1/SRtqcjxfPvXLKChc/t4XuPS7NdQGOkWXnPSnG0PnQCcXPnIhh1yH+POpP+Gi374m/m0lWQpyGEVRKE8vZ13LOv5j9wO0//EBYXNx7bXSd+w4OkzD5AfmoK9tDCi3FRLQQeu+DVqXknAcOtzAjd9bzvnPXcNB/QD31pWzcm1iTjBcoVbzvSc6+cJND/DRM2/lrKWXs+DGO0j3AKnTS24uzyjnoSseotPg5ZMfhuAfZNDJsTT0NPDC/he4osVK2cY6uPdeMRl1Im0+KAqfrbiGQRM88ee7ta5GEiW0vgv/DfBbVVVrgJ8Af9K2HEms2fOXX/CrU1TWWCq5cemNWpejKaurhKCxoeFtjStJHLbseIXXejdz3T49pY8+D+npWpcUN7Jt2bz6iVe5ZcUt/HOej1M/5mT/LR8Bv1/r0hIGVVV5+eX/4eRvZfOZt77M0JCL+zdksaf8Xgq31sNNNyWnuHzbbbB9O1x5JTz9NJdd+AUW+tL59cb/xel1Tv38E4Q9j/yc+0oPcYovl5svSf6O5DDXLLqG2ttruWnZTbzk3sqS23Xclbae/jtu1bq0hMLpdXLxH86jrqeOX20r4voLvwK/+IXww73pJq3Lixm/uPgXFKUW8aN1P6Js0018+tvL2NGxXYysSwAIDA7QaVXJ1zm0LiUhqMiqAqBp3/saV5I4BLwefvnz61lwfzX/v717D5Ksvgo4/j3d07uzs2R3AVliWJDwcK1gcH2QCIUxRQySMqgEZSsSojxSWmogrKQqUBg1mggVCRaIukQCwRQWJbGIIYKkioeWgFAJIVAIAQmwyzuYfc7uPLqPf/Sd3Z6hn7Psds/M91M1dXt+v9u3fzv37Lm/e/r27X/K73LqhhEeX30Vf3Tj07BoUb+H19zdd8M3vgFXXlmfR7zrXTA6CpUKrFnT8+bOOOYMPvbT5/HNI+GvHr7G+WmDa+78C5Lk43dvh/XrF+z9Ydd+9HJGJuBLz93qF67NE9Gvb6yJiJXA08ABmTkZ9ev+XwJOzMyW1zavWrUqN26cn5e2fuGzH+TmV+/quF7Q3bsU0cWu7fb9jm7W6+b1nl+0gw3LkgfOvId3HzXPrv7p0UsvP83b1h/NEZtLrN6xlFokNSCBWkAtcvdjkox6Xzud9lO7fTRSLbF8osyKiTLLJ0qsmCizbLJMOevbjdwdeVFsK5jet6t/V1/MWG/6GCdKMBHJRCmZiORfDvoB9xwyzsNvv5w1H12Y70plJlc/eDUX3v4Jlu1Mzvv+CoZLi6jUgkotWFQLKhnT9mXj37TZ41IG5YTyjGU16vFVjSx+oAa7Hlcjd/VvL9fYXKmypVJj81CVzZUa24aqHWPyzbS9VOXxFeMMT8AnXjyMT33wL1n+a2uhPL+/wGKa+++Hiy/m+s33cs6vw+qti1kx0d2/P3vYW73u10W1YKRaYkm1xEg1WFItMVwrUXozj0MdtnXv8h/y6I/U+O/fvJPjjnl/l1udX+7bcB/n334+33rpW6zcBidueguVLO3KG5ViOaXxTzrtcTS2Z9N13rheq201f37r12v1uGE7M4Kmm+0+tt8oj+4/zmfugj/+7go47TRYuxZOOql+8jyP1bLG15/8Olfcf8Wub49/3zNwTPUAaqUocv/ufF9rOBY0zkGmTO3S3XOCmNY+vW/mc6KLdZpspxjAzOd0M57m29v9+1jUWH/kD/n9sWO55nOPsNDddvOfc+oTn+bU54Z5x87erkB7syWws1xjtJyMlmtsH6oxWq4x1s3BpYXZXOv1YmUnT62octjWElcf8jF+9Q+uGtwiYSe12qzfWB2dGOW4zx3O96qv8d7XRiDeuJ2Z+XlaX5ttt5qjtNsewHC1xJJi3jFSPF5ci7bny+3mE+1erlXf3/3YaxyyFR47/ivEmWe2H/A89zuXvIMvL/4fzn1mBYuzPO0csNTkvHCqrRfd/h+eyh87ysmOhuV4jy/4yRMu4uSP/Elvg5xDIuKFzFzVtK+PxcKfBW7KzNUNbQ8Cn8rMuxra1gHrpn5fvnz5IZs2zc+b7v7pZ07i+m2tb0zeaU91syc7JdxOJ5Mdx9Bx+3DWfifw+Uu9mg7gfRe/jfuGXqYElLI+wS3l1ON6kWf34/bJcU/2TQKjQ8nWSq3Xf8Kb7pTyam6/1Ps43vnEbay96UNsKg/eF+BEwrKJEm+ZKO3Ty9Mj4aQ8nD87/WoOPf6UffjKAyaTsTtu45R/P4v/XbTn35y9px+SSWC8lLsmYV3WLveKdct+mSsuvKN/AxgAtaxxw39dw6dv/yQvVsY6HpcXilINLtp8DJf90mXEySfP3RP9PfTQCw9xxT2f5Zan/pVqr2do89wXj1rHeWde0e9h9N2GFx7niGt/ksk9KMjtLeUaLJ0s1YtBsxjebPPhUAa/veTnufT8W1i6/8Gz28g88djj9/Ab13+AVxZ1vqdlp320pxejZMBYKRkdyr4f66477OOcc/ZV/R3EAHjwoVs54bbTqA7oB32GJ+sXXvQSLn975Pn81jlX7rUx9ducLhbONJ+vLJT6rVqrsmVsC5vHNrNp5ya2jG2hWquSJFO5Yupxq2WndaB+9VxEUClVqJQr05bvPPidC/pelo22j2/nle2vMF4dZ6I6UV/W6stGM/P4tCtwMqlljWpWqdaqu5a1rFGKEuVSub6MMuVSmXKUd7U3Pl5aWcqyxctYPrycpZWl3gReLU3WJtkxsYOdkztbXynQZu7R7k2rds8rl8qsXLqy+4EuENVadVrumKhOTPv/O+1Krz637+1tl0sL6CrkDjbt3MT28e27jgFTx2mIL6gAAAY1SURBVIHG40Jj29TfsfE43/h7s7Y3e512z9vTdYZKQ+aPBq+Pvs6mnYNxccaSyhJGKiOMVEZYVF6YRX61l5mMV8cZnRhlx+QOxiZb37N3tnOMds9bVF7EocsOdW5c2Da+jR0TO0jq5yAzzwUb22rZ24UqvdauhoeGWVJZwpKhJQwPDbuPmhjUYqEfQ5YkSZIkSZL2sXbFwr5dIJqZrwLfBqbuKn06sLFdoVCSJEmSJEnS3jPU59f/XeCGiLgE2AKc3efxSJIkSZIkSQtWX4uFmfkkcHw/xyBJkiRJkiSpbkC/p0aSJEmSJEnSvmaxUJIkSZIkSRJgsVCSJEmSJElSwWKhJEmSJEmSJMBioSRJkiRJkqSCxUJJkiRJkiRJgMVCSZIkSZIkSQWLhZIkSZIkSZIAiMzs9xh6EhFjwGv9HsdetB+wrd+D0Jxj3Gg2jBv1ypjRbBg36pUxo9kwbjQbxo16NZ9i5qDMXNysY84VC+e7iNiYmav6PQ7NLcaNZsO4Ua+MGc2GcaNeGTOaDeNGs2HcqFcLJWb8GLIkSZIkSZIkwGKhJEmSJEmSpILFwsHzhX4PQHOScaPZMG7UK2NGs2HcqFfGjGbDuNFsGDfq1YKIGe9ZKEmSJEmSJAnwykJJkiRJkiRJBYuFkiRJkiRJkgCLhZIkSZIkSZIKFgsHSEQcHRH3RcT3IuKhiDim32PSYImI4Yi4tYiRRyLimxFxVNF3T0R8PyK+U/xc2O/xajBExLMR8WRDbKwt2s05aioiDmyIl+8UMTIZEQeYazQlIq4q8ktGxJqG9pa5xbyjZnHTbn5T9Jt3Frg2+abpHKfoM98sYC1yTcv5TdFvrlngOpxvr4yIOyLiqYh4LCLe0/C8ln1zlcXCwbIeuDYzfxy4HLihv8PRgLoWWJ2ZPwV8DfiHhr4LM3NN8XNlf4anAbW2ITZuLtrMOWoqM19viJc11PPO7Zn5f8Uq5hoB3AKcCDw3o71dbjHvqFXctJvfgHlnoWsVN9B8jgPmm4XuDTHTxfwGzDVqfTy6DHggM48GzgZuiohKF31zksXCARERK4GfA75SNH0VOLTxXVUpM3dm5r/l7q8xfwA4vI9D0hxlzlGPzgWu6/cgNFgy8z8yc2NjW7vcYt4RNI8b5zfqpFnctGO+UZcx4/xG03Q4Hp0B/H2x3kPAi8AvdtE3J1ksHByHAi9l5iRAEZzPA4f1dVQadBdQf7djymUR8WhE3BwRR/RrUBpINxaxcV1EHIQ5R12KiBOA/YHbGprNNWqlXW4x76hbM+c3YN5RazPnOGC+UQct5jdgrtF0FwBfi4gDgUpmvtzQ9yxwWLu+fTbKvcBioTRHRcQlwFHAxUXTWZn5E8CxwH/yxgOfFq73ZOaxwM8APwC+3OfxaG45F7hx6oQLc42kvajJ/AbMO2rNOY5ma+b8Bsw1atDieLRgWCwcHBuAH42IIYCICOqV6Of7OioNpIi4CPgQ8IHMHAXIzA3FMjPzb4Ajinc5tMBl5vPFcgL4a+AXMOeoCxGxH/WPVXxpqs1cow7a5RbzjtpqNr8B845aazHHAfON2mg2vwFzjXabeTzKzNeByYh4a8NqhwPPt+vbV+PdGywWDojMfBX4NvCRoul0YGNmPt2/UWkQRcQ64MPA+zNzU9E2FBEHN6xzOvBKkbi0gEXE0ohY0dD0YeBhc466tBZ4JDOfAHONOmuXW8w7aqfZ/KZoN++oqVZzHPDcSh1Nm9+AuUa7tToeAf8M/F6xznHAIcC9XfTNSbH7vo3qt4hYTf1bug4EtgBnZ+ajfR2UBkpErKL+TukzwNaieQw4iXoyWgzUqH8MY11mPtKPcWpwFPda+SpQBoJ67FyQmc+ac9RJRNwHfDEzry9+X4q5RoWIWA/8CvBW4HVga2Ye1S63mHfULG6A99JkfpOZ7zbvCFrGzcm0mOMUzzHfLGCtjlFF37T5TdFmrlHL8+3ieHQw8I/A24Fx4A8z8+7ieS375iqLhZIkSZIkSZIAP4YsSZIkSZIkqWCxUJIkSZIkSRJgsVCSJEmSJElSwWKhJEmSJEmSJMBioSRJkiRJkqSCxUJJkiRJkiRJgMVCSZIkSZIkSQWLhZIkSZIkSZIA+H9X4Y+njLcZ2wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1600x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Model\n",
    "# file_path = \"Liquor_ckpt_V0_L1_17_Dro1_0_De1_1a.hdf5\"\n",
    "model1 = load_model(file_path)\n",
    "predicted = model1.predict(test_x)\n",
    "\n",
    "# Reconstruct original values\n",
    "test_y1 = (y_test * (max_quantity-min_quantity)) + min_quantity\n",
    "pred_Model_1 = (predicted * (max_quantity-min_quantity)) + min_quantity\n",
    "plt.figure(figsize=(20,5),dpi=80)\n",
    "plt.plot(test_y1[:200], 'r',label='Actual')\n",
    "plt.plot(pred_Model_1[:200], 'g', label='Predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score for model 1 :  0.9979031303179644\n"
     ]
    }
   ],
   "source": [
    "score = r2_score(test_y1, pred_Model_1)\n",
    "print('r2 score for model 1 : ',score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
